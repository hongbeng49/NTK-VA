{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK92jpzh1CjC"
      },
      "source": [
        "# Manual inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVFYc9r8tbGa"
      },
      "source": [
        "This script should be executed at the start of the project. It requires access to Google Drive for data retrieval, so ensure that your Google account is connected. Additionally, the script reinstalls a necessary package, which requires manual input during execution. Please follow the prompts carefully to complete the installation process before proceeding with further code execution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c87b9hWW1Fft"
      },
      "source": [
        "## Prelims"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results presented in this paper rely on the following package versions:\n",
        "\n",
        "JAX: 0.4.31; TensorFlow: 2.17.0; Neural Tangents: 0.6.6."
      ],
      "metadata": {
        "id": "DmVN-GFk23-u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "61nHgfr1E7kp",
        "outputId": "d82c2e2c-74e8-47f4-f909-2c132b2e22ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jax==0.4.31\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib==0.4.31\n",
            "  Downloading jaxlib-0.4.31-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.31) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.31) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.31) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.31) (1.13.1)\n",
            "Downloading jax-0.4.31-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.31-cp310-cp310-manylinux2014_x86_64.whl (88.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.26+cuda12.cudnn89\n",
            "    Uninstalling jaxlib-0.4.26+cuda12.cudnn89:\n",
            "      Successfully uninstalled jaxlib-0.4.26+cuda12.cudnn89\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.26\n",
            "    Uninstalling jax-0.4.26:\n",
            "      Successfully uninstalled jax-0.4.26\n",
            "Successfully installed jax-0.4.31 jaxlib-0.4.31\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jax",
                  "jaxlib"
                ]
              },
              "id": "4fedb835b7e24d95b31005cabea7bd1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.17.0 in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.0) (0.1.2)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for neural-tangents (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade pip\n",
        "!pip install --upgrade jax==0.4.31 jaxlib==0.4.31\n",
        "!pip install tensorflow==2.17.0\n",
        "!pip install -q git+https://www.github.com/google/neural-tangents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import tensorflow as tf\n",
        "import neural_tangents as nt\n",
        "\n",
        "print(\"JAX version:\", jax.__version__) # 0.4.31\n",
        "print(\"TensorFlow version:\", tf.__version__) # 2.17.0\n",
        "print(\"Neural Tangents version:\", nt.__version__) #0.6.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wu9_aixy0ve",
        "outputId": "ebd7297a-f32e-4e3d-8af6-680b2222d16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JAX version: 0.4.31\n",
            "TensorFlow version: 2.17.0\n",
            "Neural Tangents version: 0.6.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Google Drive to access the required files. If you plan to save the file locally, modifications to the code will be necessary."
      ],
      "metadata": {
        "id": "pyAlgr_Z3zgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMJ_m4CO2SB9",
        "outputId": "36ac4554-2159-4211-b59b-1d005bd7200d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep"
      ],
      "metadata": {
        "id": "iFh8p6KV6jJV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJWzXizHtl8k"
      },
      "source": [
        "This code loads the data and performs minor pre-processing. Before running the code, ensure you have the following two files: inforce.csv; fmv_seriatim.csv. These can be downloaded at https://www2.math.uconn.edu/~gan/software.html, under *Aggregate results*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GTb0wxh4bWG",
        "outputId": "19e26f1b-c71e-4d96-e5cc-7727da6b5e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/VA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-f7b2983322a6>:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  mydat['gender'] = np.where(mydat['gender']=='M', 1, 0)\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/VA/\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "datpolicies = pd.read_csv('inforce.csv')\n",
        "datvals = pd.read_csv('fmv_seriatim.csv')\n",
        "datcombined = datpolicies.join(datvals.set_index('recordID'), on = 'recordID')\n",
        "#use matDate, birthDate, issueTime and currentDate to define\n",
        "datcombined['matTime'] = (datcombined.matDate - datcombined.currentDate)/365.25 #time to maturity\n",
        "datcombined['Age'] =   (datcombined.currentDate - datcombined.birthDate)/365.25 #age\n",
        "datgreeks = pd.read_csv('Greek.csv')\n",
        "#datcombined['issueTime'] =(datcombined.currentDate - datcombined.issueDate)/365.25 #time since issue\n",
        "mydat = datcombined #190000 policies\n",
        "mydat_y = mydat[\"base:base\"].astype('float') #contract valuation: base (dependent)\n",
        "mydat_y = np.array(mydat_y).flatten()\n",
        "\n",
        "mydat = mydat.iloc[:, np.r_[2, 3, 11:13, 15:25, 72,73]] #gender, productType, gbAmt, GMWBBalance, FundValue1-10, 2 derived variables\n",
        "#all products: include withdrawal variables\n",
        "mydat['gender'] = np.where(mydat['gender']=='M', 1, 0)\n",
        "mave = np.absolute(mydat_y).mean()\n",
        "datcombined2 = datcombined.join(datgreeks.set_index('recordID'), on = 'recordID')\n",
        "\n",
        "#for Greeks\n",
        "datgreeks2 = datcombined2.iloc[:, -13:]\n",
        "list1 = ['base'] + [str(x)+'_'+str(y) for x in range(1, 6) for y in ['D', 'U']]+['base']*16\n",
        "list2 = ['base']*11 + [str(x)+'y_'+str(y) for x in [1, 2, 3, 4, 5, 7, 10, 30] for y in ['D', 'U']]\n",
        "colnames = [str(x)+':'+str(y) for (x, y) in zip(list2, list1)]\n",
        "datvals2 = datcombined2[colnames]\n",
        "\n",
        "#for numeric variables, perform minmax scaling\n",
        "for i in range(2, len(mydat.columns)):\n",
        "  currentcolumn = mydat.iloc[:, i]\n",
        "  mydat.iloc[:, i] = (currentcolumn - currentcolumn.min())/(currentcolumn.max()-currentcolumn.min())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpj5Pn6IVKnD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from random import seed, sample\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Model as KerasModel\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Activation, Reshape\n",
        "from keras.layers import Concatenate, Softmax\n",
        "#from keras.layers.embeddings import Embedding\n",
        "from keras.utils import to_categorical, set_random_seed\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.constraints import NonNeg\n",
        "from keras import initializers\n",
        "\n",
        "#create one hot encoding of categorical\n",
        "from keras.utils import to_categorical\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "productTypes = mydat['productType'].unique()\n",
        "for i in range(len(productTypes)):\n",
        "  mydat.loc[mydat['productType'] == productTypes[i], 'productType'] = i\n",
        "mydat_emb = deepcopy(mydat)\n",
        "#mydat_emb will be used for embedding encoding for NNs\n",
        "productType_one_hot = to_categorical(mydat['productType'])\n",
        "productType_one_hot = pd.DataFrame(productType_one_hot, columns = productTypes)\n",
        "mydat = mydat.loc[:, mydat.columns != 'productType']\n",
        "mydat = mydat.join(productType_one_hot)\n",
        "#mydat will be used for one-hot encoding for NNs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ9gTfzn3M3f"
      },
      "source": [
        "# Sampling methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIwmNJFy3P4w"
      },
      "source": [
        "At the beginning of each run, select only one of the following methods: stratified random sampling, hierarchical k-means, or conditional cLHS. Ensure that the random state is set based on the run number for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "mydat_array = mydat.values #certain methods require data to be stored in an array rather than a data frame\n",
        "random_state = 10\n",
        "n = 680\n",
        "ndat = len(mydat_y) #190,000\n",
        "\n",
        "random.seed(1)"
      ],
      "metadata": {
        "id": "rzmJfB99z2El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwIKXq2GuJNK"
      },
      "source": [
        "## Stratified random sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results presented in the paper are the average of replicates generated using the following five seeds: 10, 20, 30, 40, and 50. For single replicates, the results are based on seed 10."
      ],
      "metadata": {
        "id": "W7ERM2i36oHe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa_NdxR-qeCj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "mydat_train, mydat_val, mydat_train_emb, mydat_val_emb, mydat_train_y, mydat_val_y  = train_test_split(mydat_array, mydat_emb, mydat_y, test_size=(n/2)/ndat,train_size=n/ndat,\n",
        "                                                                                                       stratify = mydat_emb['productType'], random_state =random_state)\n",
        "def makenpfloat(vec):\n",
        "  return np.array(vec, dtype = np.float32)\n",
        "\n",
        "mydat_train = makenpfloat(mydat_train)\n",
        "mydat_val = makenpfloat(mydat_val)\n",
        "mydat_train_y = makenpfloat(mydat_train_y).flatten()\n",
        "mydat_val_y = makenpfloat(mydat_val_y).flatten()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is intended exclusively for Greeks."
      ],
      "metadata": {
        "id": "nYwhqTuu7Ajp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3twpC9cjHSOu"
      },
      "outputs": [],
      "source": [
        "mydat_greeks_train = datgreeks2.loc[mydat_train_emb.index]\n",
        "mydat_greeks_val = datgreeks2.loc[mydat_val_emb.index]\n",
        "mydat_vals_train = datvals2.loc[mydat_train_emb.index]\n",
        "mydat_vals_val = datvals2.loc[mydat_val_emb.index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JSaYLTPKgq0"
      },
      "source": [
        "## Hierarchical k-means\n",
        "\n",
        "This code implements hierarchical k-means as described in the Gan and Valdez book. In broader machine learning contexts, this method is commonly referred to as bisecting k-means. The results presented in the paper are the average of replicates generated using the following five seeds: 10, 20, 30, 40, and 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zYktCwKL_0o"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import BisectingKMeans\n",
        "from scipy.cluster.vq import vq\n",
        "mydat_array =mydat.values\n",
        "\n",
        "\n",
        "mykmeans = BisectingKMeans(n_clusters = n, random_state = random_state, init = 'k-means++', n_init = 1, bisecting_strategy = 'largest_cluster').fit(mydat_array)\n",
        "\n",
        "\n",
        "def getclosestinds(dat, kmeans):\n",
        "  all_data = [ i for i in range(dat.shape[0]) ]\n",
        "  m_clusters = kmeans.labels_.tolist()\n",
        "  centers = np.array(kmeans.cluster_centers_)\n",
        "  n_clusters = centers.shape[0]\n",
        "\n",
        "  closest_data = []\n",
        "  for i in range(n_clusters):\n",
        "      center_vec = centers[i].reshape(1,-1)\n",
        "      data_idx_within_i_cluster = [ idx for idx, clu_num in enumerate(m_clusters) if clu_num == i ]\n",
        "\n",
        "      one_cluster_dat = np.zeros( (  len(data_idx_within_i_cluster) , centers.shape[1] ) )\n",
        "      for row_num, data_idx in enumerate(data_idx_within_i_cluster):\n",
        "          one_row = dat[data_idx]\n",
        "          one_cluster_dat[row_num] = one_row\n",
        "\n",
        "      closest, _ = vq(center_vec, one_cluster_dat)\n",
        "      closest_idx_in_one_cluster_dat = closest[0]\n",
        "      closest_data_row_num = data_idx_within_i_cluster[closest_idx_in_one_cluster_dat]\n",
        "      data_id = all_data[closest_data_row_num]\n",
        "\n",
        "      closest_data.append(data_id)\n",
        "\n",
        "  closest_data = list(set(closest_data))\n",
        "\n",
        "  assert len(closest_data) == n_clusters\n",
        "\n",
        "  return closest_data\n",
        "\n",
        "mydat_train_inds = getclosestinds(mydat_array, mykmeans)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "mydat_train = mydat_array[mydat_train_inds]\n",
        "mydat_train_y = mydat_y[mydat_train_inds]\n",
        "mydat_train_emb = mydat_emb.iloc[mydat_train_inds]\n",
        "\n",
        "\n",
        "mydat_y_notrain = np.delete(mydat_y, mydat_train_inds)\n",
        "mydat_array_notrain = np.delete(mydat_array, mydat_train_inds, axis=0)\n",
        "mydat_emb_notrain = mydat_emb[~mydat_emb.index.isin(mydat_train_inds)]\n",
        "productType_notrain = mydat_emb_notrain['productType']\n",
        "mydat_val, _, mydat_val_emb, _, mydat_val_y, _ = train_test_split(mydat_array_notrain, mydat_emb_notrain, mydat_y_notrain, train_size=(n/2)/(ndat-n), stratify = productType_notrain, random_state =random_state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_DZtKBgA2cd"
      },
      "source": [
        "## Conditioned latin hypercube sampling (cLHS)\n",
        "\n",
        "This code performs cLHS. The results presented in the paper are the average of replicates generated using the following five seeds: 10, 20, 30, 40, and 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg2_4xTwA0gS",
        "outputId": "7e4a7c68-0e23-4721-f902-0fc99593ebe6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting clhs\n",
            "  Downloading clhs-1.0.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clhs) (4.66.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from clhs) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from clhs) (2.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->clhs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->clhs) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->clhs) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->clhs) (1.16.0)\n",
            "Building wheels for collected packages: clhs\n",
            "  Building wheel for clhs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clhs: filename=clhs-1.0.2-py3-none-any.whl size=9790 sha256=c59f3b0a00433b61d8cbcce0cac4fe6b25f7a879051a9263f103edaf4c0c8158\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/f2/ed/264b8cb2bbd5b97222e4f091b14bc1c6dd837c7abf30443201\n",
            "Successfully built clhs\n",
            "Installing collected packages: clhs\n",
            "Successfully installed clhs-1.0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "cLHS: 91%|█████████ |907/1000 [Elapsed time: 5.80972695350647, ETA: 0.7257527427932804, 128.14it/s] /usr/local/lib/python3.10/dist-packages/clhs/clhs.py:760: RuntimeWarning: overflow encountered in exp\n",
            "  anneal_fac = np.exp(-delta_obj / temp)\n",
            "cLHS: 93%|█████████▎|933/1000 [Elapsed time: 6.02297306060791, ETA: 0.5331041294133011, 125.68it/s] /usr/local/lib/python3.10/dist-packages/clhs/clhs.py:760: RuntimeWarning: overflow encountered in exp\n",
            "  anneal_fac = np.exp(-delta_obj / temp)\n",
            "cLHS: 97%|█████████▋|972/1000 [Elapsed time: 6.34163498878479, ETA: 0.2275191727144864, 123.07it/s]  /usr/local/lib/python3.10/dist-packages/clhs/clhs.py:760: RuntimeWarning: overflow encountered in exp\n",
            "  anneal_fac = np.exp(-delta_obj / temp)\n",
            "cLHS: 98%|█████████▊|985/1000 [Elapsed time: 6.441846132278442, ETA: 0.12004685102924097, 124.95it/s]/usr/local/lib/python3.10/dist-packages/clhs/clhs.py:760: RuntimeWarning: overflow encountered in exp\n",
            "  anneal_fac = np.exp(-delta_obj / temp)\n",
            "cLHS:100%|█████████▉|998/1000 [Elapsed time: 6.555081367492676, ETA: 0.01642358055967479, 121.78it/s]/usr/local/lib/python3.10/dist-packages/clhs/clhs.py:760: RuntimeWarning: overflow encountered in exp\n",
            "  anneal_fac = np.exp(-delta_obj / temp)\n",
            "cLHS:100%|██████████|1000/1000 [Elapsed time: 6.575577259063721, ETA: 0.0, 152.08it/s]               \n"
          ]
        }
      ],
      "source": [
        "!pip install clhs\n",
        "import clhs as cl\n",
        "\n",
        "\n",
        "sampled=cl.clhs(mydat_emb, n, max_iterations=1000, random_state = random_state)\n",
        "# then, validation indices are sampled from outside of training indices, using\n",
        "# stratified random sampling\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mydat_train_inds = sampled['sample_indices']\n",
        "mydat_train = mydat_array[mydat_train_inds]\n",
        "mydat_train_y = mydat_y[mydat_train_inds]\n",
        "mydat_train_emb = mydat_emb.iloc[mydat_train_inds]\n",
        "\n",
        "\n",
        "mydat_y_notrain = np.delete(mydat_y, mydat_train_inds)\n",
        "mydat_array_notrain = np.delete(mydat_array, mydat_train_inds, axis=0)\n",
        "mydat_emb_notrain = mydat_emb[~mydat_emb.index.isin(mydat_train_inds)]\n",
        "productType_notrain = mydat_emb_notrain['productType']\n",
        "mydat_val, _, mydat_val_emb, _, mydat_val_y, _ = train_test_split(mydat_array_notrain, mydat_emb_notrain, mydat_y_notrain, train_size=(n/2)/(190000-n), stratify = productType_notrain, random_state =random_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1Aq104Q4XRZ"
      },
      "source": [
        "#Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C90_xzTA5KdY"
      },
      "source": [
        "The paper explores three methods: bagging, bias-corrected bagging, and boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6u80n5FwkCc"
      },
      "source": [
        "## Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7YzBOHR4ZtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac55069-a04b-4889-ed80-6f9fc2f25c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2688.1189249238923\n",
            "30.583535498456747\n",
            "51.253657098781936\n",
            "30.233491811118913\n",
            "0.009126183053668769\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "\n",
        "reg = BaggingRegressor(n_estimators=300, random_state=0, oob_score=True).fit(mydat_train, mydat_train_y)\n",
        "\n",
        "mypreds_full = reg.predict(mydat_array)\n",
        "mypreds_val = reg.predict(mydat_val)\n",
        "mypreds = reg.predict(mydat_train)\n",
        "\n",
        "errs = mypreds_full.flatten() - mydat_y.flatten()\n",
        "\n",
        "print(np.mean(np.square(errs))/1000000)\n",
        "print(np.mean(np.absolute(errs))/1000)\n",
        "print(np.sqrt(np.mean(np.square(errs)))/mave*100)\n",
        "print(np.mean(np.absolute(errs))/mave*100)\n",
        "print(np.mean(errs)/mydat_y.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdxi8NALwoxj"
      },
      "source": [
        "##Bias-corrected bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoshUrNDtHaH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7489f2-980d-42c4-e8d2-1bc5d515bbaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2903.293524983555\n",
            "28.17172130689984\n",
            "53.26551036447196\n",
            "27.849282025626472\n",
            "-0.016365147761239162\n"
          ]
        }
      ],
      "source": [
        "oob_pred_train = reg.oob_prediction_\n",
        "bias_train = oob_pred_train - mydat_train_y\n",
        "regbias = BaggingRegressor(n_estimators=300, random_state=0).fit(mydat_train, bias_train)\n",
        "\n",
        "mypreds_full = reg.predict(mydat_array)-regbias.predict(mydat_array)\n",
        "mypreds_val = reg.predict(mydat_val)-regbias.predict(mydat_val)\n",
        "mypreds = reg.predict(mydat_train)-regbias.predict(mydat_train)\n",
        "\n",
        "errs = mypreds_full.flatten() - mydat_y.flatten()\n",
        "\n",
        "print(np.mean(np.square(errs))/1000000)\n",
        "print(np.mean(np.absolute(errs))/1000)\n",
        "print(np.sqrt(np.mean(np.square(errs)))/mave*100)\n",
        "print(np.mean(np.absolute(errs))/mave*100)\n",
        "print(np.mean(errs)/mydat_y.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz2xT-i9wtn8"
      },
      "source": [
        "##Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-0aCCvSK8C5",
        "outputId": "f4363df3-5ed4-4420-a59a-5a706b4a0835"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "nest = [500, 1000, 2000]\n",
        "maxdepth = [1, 3, 5]\n",
        "minsamplesleaf = [1, 5, 10]\n",
        "learningrate = [0.01, 0.1, 0.2]\n",
        "boost_mat = np.zeros((81, 5))\n",
        "\n",
        "\n",
        "ncount = 0\n",
        "for i in range(3):\n",
        "  currentnest = nest[i]\n",
        "  for j in range(3):\n",
        "    currentmaxdepth = maxdepth[j]\n",
        "    for k in range(3):\n",
        "      currentminsamplesleaf = minsamplesleaf[k]\n",
        "      for l in range(3):\n",
        "        print(ncount)\n",
        "        currentlearningrate = learningrate[l]\n",
        "        booster = GradientBoostingRegressor(n_estimators=currentnest,\n",
        "                                            learning_rate = currentlearningrate,\n",
        "                                            max_depth = currentmaxdepth,\n",
        "                                            min_samples_leaf=currentminsamplesleaf,\n",
        "                                            random_state=0).fit(mydat_train, mydat_train_y)\n",
        "        val_preds = booster.predict(mydat_val)\n",
        "        val_error = np.mean(np.square(val_preds.flatten() - mydat_val_y.flatten()))\n",
        "        boost_mat[ncount] = [currentnest, currentmaxdepth, currentminsamplesleaf, currentlearningrate, val_error]\n",
        "\n",
        "\n",
        "\n",
        "        ncount = ncount+1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzY0W_U3De8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd2a3f99-1bd3-4842-ed92-0ccc52e76e08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1845.4511999744511\n",
            "25.096442636260985\n",
            "42.46703132729037\n",
            "24.809201440098345\n",
            "0.020151650567711465\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "minind = np.argmin(boost_mat[:, 4])\n",
        "mynest = int(boost_mat[minind, 0])\n",
        "mymaxdepth = int(boost_mat[minind, 1])\n",
        "myminsamples = int(boost_mat[minind, 2])\n",
        "mylearning = boost_mat[minind, 3]\n",
        "booster = GradientBoostingRegressor(n_estimators=mynest, learning_rate = mylearning, max_depth = mymaxdepth, min_samples_leaf=myminsamples, random_state=0).fit(mydat_train, mydat_train_y)\n",
        "\n",
        "mypreds_full = booster.predict(mydat_array)\n",
        "mypreds_val = booster.predict(mydat_val)\n",
        "mypreds = booster.predict(mydat_train)\n",
        "\n",
        "errs = mypreds_full.flatten() - mydat_y.flatten()\n",
        "\n",
        "print(np.mean(np.square(errs))/1000000)\n",
        "print(np.mean(np.absolute(errs))/1000)\n",
        "print(np.sqrt(np.mean(np.square(errs)))/mave*100)\n",
        "print(np.mean(np.absolute(errs))/mave*100)\n",
        "print(np.mean(errs)/mydat_y.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlR_01EQymlm"
      },
      "source": [
        "# Finite NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iftaU3lV_wR4"
      },
      "source": [
        "## One-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "set_random_seed(1)\n",
        "\n",
        "\n",
        "k = 128\n",
        "inputdata = Input((34, ))\n",
        "mydense = Dense(k, activation = 'relu')(inputdata)\n",
        "mydense = Dense(int(k/2), activation = 'relu')(mydense)\n",
        "myval = Dense(1)(mydense)\n",
        "\n",
        "inputs = inputdata\n",
        "outputs = myval\n",
        "\n",
        "m = KerasModel(inputs = inputs, outputs=outputs)\n",
        "m.compile(optimizer=Adam(), loss='mse')\n",
        "#m.summary()\n",
        "callback = EarlyStopping(monitor='val_loss', patience=50)\n",
        "m.fit(x=mydat_train, y=mydat_train_y, epochs = 20000, validation_data = (mydat_val, mydat_val_y), callbacks = [callback], verbose = 0, batch_size = n)"
      ],
      "metadata": {
        "id": "3n1LHyrr01yg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66c9961-159c-4362-ad46-bffc01a56dec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79ef1fe56f20>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mypreds_full = m.predict(x=mydat_array)\n",
        "mypreds_val = m.predict(x=mydat_val)\n",
        "mypreds = m.predict(x=mydat_train)\n",
        "\n",
        "errs = mypreds_full.flatten() - mydat_y.flatten()\n",
        "\n",
        "print(np.mean(np.square(errs))/1000000)\n",
        "print(np.mean(np.absolute(errs))/1000)\n",
        "print(np.sqrt(np.mean(np.square(errs)))/mave*100)\n",
        "print(np.mean(np.absolute(errs))/mave*100)\n",
        "print(np.mean(errs)/mydat_y.mean())\n"
      ],
      "metadata": {
        "id": "lu_iIrFf090M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c6ac3a3-2b2b-4348-abc6-fa8a36ec6c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5938/5938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
            "694.2738266048668\n",
            "15.303218239298548\n",
            "26.047498181076143\n",
            "15.128065339108513\n",
            "0.026120714890647795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(19):\n",
        "  print(\"Product type:\" + str(i+1))\n",
        "  currenterrs = errs[10000*i:(10000*i+10000)]\n",
        "  currenty = mydat_y[10000*i:(10000*i+10000)]\n",
        "  currentmave = np.mean(np.abs(currenty))\n",
        "  currentmse = np.mean(np.square(currenterrs))\n",
        "  print(currentmse/1000000)\n",
        "  print(np.sqrt(currentmse)/currentmave)\n",
        "  print(np.mean(currenterrs)/np.mean(currenty))"
      ],
      "metadata": {
        "id": "TlevJtqo1ODr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfdf5c5-74dc-4bc3-f382-599cf4e9cade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product type:1\n",
            "183.47643966549677\n",
            "0.39129161773743193\n",
            "-0.0006935991460613565\n",
            "Product type:2\n",
            "1878.589386982343\n",
            "0.1371328665561452\n",
            "0.013118943047495061\n",
            "Product type:3\n",
            "188.66827908947027\n",
            "0.3830771982497577\n",
            "0.14106062344897866\n",
            "Product type:4\n",
            "641.3151089578128\n",
            "0.36024020928790657\n",
            "0.07953607974576944\n",
            "Product type:5\n",
            "221.77844110206675\n",
            "0.3178048978418479\n",
            "0.06633715109723426\n",
            "Product type:6\n",
            "463.2954285027623\n",
            "0.45531886897754154\n",
            "-0.015906215684053884\n",
            "Product type:7\n",
            "1388.5828105684916\n",
            "0.7568603651594896\n",
            "-0.10495115957853395\n",
            "Product type:8\n",
            "42.78755926014768\n",
            "1.245956612534637\n",
            "-0.0843358182144588\n",
            "Product type:9\n",
            "230.88142312797999\n",
            "0.31219606282831136\n",
            "0.059076928572447764\n",
            "Product type:10\n",
            "295.742945999504\n",
            "0.35115688110038684\n",
            "0.044727075148855745\n",
            "Product type:11\n",
            "489.4722456226593\n",
            "0.4385905701341235\n",
            "0.1087630761411987\n",
            "Product type:12\n",
            "412.06265777118335\n",
            "0.27608141629827326\n",
            "0.049636163616487136\n",
            "Product type:13\n",
            "58.142876523614525\n",
            "0.7497570228243251\n",
            "-0.03865545344881798\n",
            "Product type:14\n",
            "48.90567541993699\n",
            "0.6931262606902503\n",
            "-0.012948422917951743\n",
            "Product type:15\n",
            "1138.9611152619689\n",
            "0.546599796068813\n",
            "0.17843532211921273\n",
            "Product type:16\n",
            "1608.3407645737789\n",
            "0.11076912219148993\n",
            "-0.0029470388189073364\n",
            "Product type:17\n",
            "214.05623490422911\n",
            "0.27337387925016593\n",
            "-0.015614558928290197\n",
            "Product type:18\n",
            "3267.4000466033976\n",
            "0.11786974593559066\n",
            "0.028660097816756228\n",
            "Product type:19\n",
            "418.7432655556251\n",
            "0.18204404218602285\n",
            "-0.004577504081598014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m63Xjz9RNYN-"
      },
      "source": [
        "## With embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n",
        "K.clear_session()\n",
        "set_random_seed(1)\n",
        "\n",
        "\n",
        "\n",
        "input_product_type = Input(shape=(1,))\n",
        "output_product_type = Embedding(19, 5, name='product_type_embedding')(input_product_type)\n",
        "output_product_type = Reshape(target_shape=(5,))(output_product_type)\n",
        "input_mat = Input(shape = (15, ))\n",
        "output_mat = input_mat\n",
        "input_list = [input_mat, input_product_type]\n",
        "outputs =[output_mat, output_product_type]\n",
        "outputs = Concatenate()(outputs)\n",
        "k = 128\n",
        "\n",
        "outputs = Dense(k, activation = 'relu')(outputs)\n",
        "outputs = Dense(int(k/2), activation = 'relu')(outputs)\n",
        "outputs = Dense(1)(outputs)\n",
        "\n",
        "m2 = KerasModel(inputs = input_list, outputs=outputs)\n",
        "m2.compile(optimizer=Adam(), loss='mse')\n",
        "#m2.summary()\n",
        "callback = EarlyStopping(monitor='val_loss', patience=50)\n",
        "\n",
        "\n",
        "def split_features(dat_emb):\n",
        "  product_type = np.array(dat_emb['productType']).astype('float')\n",
        "  dat_emb = np.array(dat_emb.loc[:, dat_emb.columns != 'productType'].values)\n",
        "  return [dat_emb, product_type]\n",
        "\n",
        "m2.fit(x=split_features(mydat_train_emb), y=mydat_train_y, epochs = 20000, validation_data = (split_features(mydat_val_emb), mydat_val_y), callbacks = [callback], verbose = 2, batch_size = n)\n"
      ],
      "metadata": {
        "id": "N_fAYaCy11R0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5975a1b-8e17-4b3c-d5e1-f269c9b954f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 10779/20000\n",
            "1/1 - 0s - 72ms/step - loss: 1087942528.0000 - val_loss: 1258240512.0000\n",
            "Epoch 10780/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1087448960.0000 - val_loss: 1258270720.0000\n",
            "Epoch 10781/20000\n",
            "1/1 - 0s - 63ms/step - loss: 1086932480.0000 - val_loss: 1258088448.0000\n",
            "Epoch 10782/20000\n",
            "1/1 - 0s - 67ms/step - loss: 1086429184.0000 - val_loss: 1256362112.0000\n",
            "Epoch 10783/20000\n",
            "1/1 - 0s - 143ms/step - loss: 1085915520.0000 - val_loss: 1256419072.0000\n",
            "Epoch 10784/20000\n",
            "1/1 - 0s - 67ms/step - loss: 1085377792.0000 - val_loss: 1255677056.0000\n",
            "Epoch 10785/20000\n",
            "1/1 - 0s - 64ms/step - loss: 1084863616.0000 - val_loss: 1254000384.0000\n",
            "Epoch 10786/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1084389120.0000 - val_loss: 1255117824.0000\n",
            "Epoch 10787/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1083845504.0000 - val_loss: 1254210560.0000\n",
            "Epoch 10788/20000\n",
            "1/1 - 0s - 139ms/step - loss: 1083295872.0000 - val_loss: 1252024960.0000\n",
            "Epoch 10789/20000\n",
            "1/1 - 0s - 137ms/step - loss: 1082768768.0000 - val_loss: 1252326784.0000\n",
            "Epoch 10790/20000\n",
            "1/1 - 0s - 63ms/step - loss: 1082200448.0000 - val_loss: 1251720448.0000\n",
            "Epoch 10791/20000\n",
            "1/1 - 0s - 138ms/step - loss: 1081660544.0000 - val_loss: 1250279680.0000\n",
            "Epoch 10792/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1081111296.0000 - val_loss: 1250448896.0000\n",
            "Epoch 10793/20000\n",
            "1/1 - 0s - 74ms/step - loss: 1080556032.0000 - val_loss: 1249420160.0000\n",
            "Epoch 10794/20000\n",
            "1/1 - 0s - 66ms/step - loss: 1079989760.0000 - val_loss: 1247756032.0000\n",
            "Epoch 10795/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1079443200.0000 - val_loss: 1248414208.0000\n",
            "Epoch 10796/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1078839936.0000 - val_loss: 1247813248.0000\n",
            "Epoch 10797/20000\n",
            "1/1 - 0s - 58ms/step - loss: 1078247808.0000 - val_loss: 1247046144.0000\n",
            "Epoch 10798/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1077654656.0000 - val_loss: 1246178560.0000\n",
            "Epoch 10799/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1077052160.0000 - val_loss: 1246224768.0000\n",
            "Epoch 10800/20000\n",
            "1/1 - 0s - 144ms/step - loss: 1076437504.0000 - val_loss: 1244537600.0000\n",
            "Epoch 10801/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1075813248.0000 - val_loss: 1244173952.0000\n",
            "Epoch 10802/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1075166208.0000 - val_loss: 1243421312.0000\n",
            "Epoch 10803/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1074513792.0000 - val_loss: 1242485248.0000\n",
            "Epoch 10804/20000\n",
            "1/1 - 0s - 139ms/step - loss: 1073865088.0000 - val_loss: 1241947136.0000\n",
            "Epoch 10805/20000\n",
            "1/1 - 0s - 146ms/step - loss: 1073225856.0000 - val_loss: 1241235328.0000\n",
            "Epoch 10806/20000\n",
            "1/1 - 0s - 59ms/step - loss: 1072593856.0000 - val_loss: 1239424512.0000\n",
            "Epoch 10807/20000\n",
            "1/1 - 0s - 59ms/step - loss: 1071964032.0000 - val_loss: 1239569920.0000\n",
            "Epoch 10808/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1071316800.0000 - val_loss: 1238931072.0000\n",
            "Epoch 10809/20000\n",
            "1/1 - 0s - 58ms/step - loss: 1070682816.0000 - val_loss: 1237276032.0000\n",
            "Epoch 10810/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1070050880.0000 - val_loss: 1236683520.0000\n",
            "Epoch 10811/20000\n",
            "1/1 - 0s - 59ms/step - loss: 1069400256.0000 - val_loss: 1236679040.0000\n",
            "Epoch 10812/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1068774656.0000 - val_loss: 1234714112.0000\n",
            "Epoch 10813/20000\n",
            "1/1 - 0s - 63ms/step - loss: 1068142912.0000 - val_loss: 1234237824.0000\n",
            "Epoch 10814/20000\n",
            "1/1 - 0s - 58ms/step - loss: 1067494144.0000 - val_loss: 1233494912.0000\n",
            "Epoch 10815/20000\n",
            "1/1 - 0s - 55ms/step - loss: 1066845504.0000 - val_loss: 1232580096.0000\n",
            "Epoch 10816/20000\n",
            "1/1 - 0s - 58ms/step - loss: 1066195904.0000 - val_loss: 1231347328.0000\n",
            "Epoch 10817/20000\n",
            "1/1 - 0s - 64ms/step - loss: 1065569280.0000 - val_loss: 1231438592.0000\n",
            "Epoch 10818/20000\n",
            "1/1 - 0s - 139ms/step - loss: 1064959616.0000 - val_loss: 1230232576.0000\n",
            "Epoch 10819/20000\n",
            "1/1 - 0s - 71ms/step - loss: 1064351488.0000 - val_loss: 1229761536.0000\n",
            "Epoch 10820/20000\n",
            "1/1 - 0s - 71ms/step - loss: 1063717888.0000 - val_loss: 1229016448.0000\n",
            "Epoch 10821/20000\n",
            "1/1 - 0s - 132ms/step - loss: 1063075072.0000 - val_loss: 1227721344.0000\n",
            "Epoch 10822/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1062420800.0000 - val_loss: 1227803008.0000\n",
            "Epoch 10823/20000\n",
            "1/1 - 0s - 58ms/step - loss: 1061768192.0000 - val_loss: 1226891136.0000\n",
            "Epoch 10824/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1061100864.0000 - val_loss: 1225889920.0000\n",
            "Epoch 10825/20000\n",
            "1/1 - 0s - 59ms/step - loss: 1060462400.0000 - val_loss: 1224862592.0000\n",
            "Epoch 10826/20000\n",
            "1/1 - 0s - 65ms/step - loss: 1059835776.0000 - val_loss: 1224938496.0000\n",
            "Epoch 10827/20000\n",
            "1/1 - 0s - 59ms/step - loss: 1059187072.0000 - val_loss: 1224422784.0000\n",
            "Epoch 10828/20000\n",
            "1/1 - 0s - 58ms/step - loss: 1058534848.0000 - val_loss: 1222574720.0000\n",
            "Epoch 10829/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1057875840.0000 - val_loss: 1221613568.0000\n",
            "Epoch 10830/20000\n",
            "1/1 - 0s - 162ms/step - loss: 1057228992.0000 - val_loss: 1222289920.0000\n",
            "Epoch 10831/20000\n",
            "1/1 - 0s - 97ms/step - loss: 1056583424.0000 - val_loss: 1221435648.0000\n",
            "Epoch 10832/20000\n",
            "1/1 - 0s - 150ms/step - loss: 1055935808.0000 - val_loss: 1219931648.0000\n",
            "Epoch 10833/20000\n",
            "1/1 - 0s - 92ms/step - loss: 1055291520.0000 - val_loss: 1219298816.0000\n",
            "Epoch 10834/20000\n",
            "1/1 - 0s - 78ms/step - loss: 1054645376.0000 - val_loss: 1219308032.0000\n",
            "Epoch 10835/20000\n",
            "1/1 - 0s - 139ms/step - loss: 1054019328.0000 - val_loss: 1217758592.0000\n",
            "Epoch 10836/20000\n",
            "1/1 - 0s - 83ms/step - loss: 1053384000.0000 - val_loss: 1216500480.0000\n",
            "Epoch 10837/20000\n",
            "1/1 - 0s - 87ms/step - loss: 1052771456.0000 - val_loss: 1216663168.0000\n",
            "Epoch 10838/20000\n",
            "1/1 - 0s - 87ms/step - loss: 1052164224.0000 - val_loss: 1215628800.0000\n",
            "Epoch 10839/20000\n",
            "1/1 - 0s - 165ms/step - loss: 1051544576.0000 - val_loss: 1213633792.0000\n",
            "Epoch 10840/20000\n",
            "1/1 - 0s - 82ms/step - loss: 1050934016.0000 - val_loss: 1213933312.0000\n",
            "Epoch 10841/20000\n",
            "1/1 - 0s - 94ms/step - loss: 1050294016.0000 - val_loss: 1213526400.0000\n",
            "Epoch 10842/20000\n",
            "1/1 - 0s - 142ms/step - loss: 1049678336.0000 - val_loss: 1211999232.0000\n",
            "Epoch 10843/20000\n",
            "1/1 - 0s - 94ms/step - loss: 1049060032.0000 - val_loss: 1211624064.0000\n",
            "Epoch 10844/20000\n",
            "1/1 - 0s - 151ms/step - loss: 1048442880.0000 - val_loss: 1211373440.0000\n",
            "Epoch 10845/20000\n",
            "1/1 - 0s - 84ms/step - loss: 1047845056.0000 - val_loss: 1209975808.0000\n",
            "Epoch 10846/20000\n",
            "1/1 - 0s - 139ms/step - loss: 1047228096.0000 - val_loss: 1208965760.0000\n",
            "Epoch 10847/20000\n",
            "1/1 - 0s - 138ms/step - loss: 1046617984.0000 - val_loss: 1208980224.0000\n",
            "Epoch 10848/20000\n",
            "1/1 - 0s - 137ms/step - loss: 1046009088.0000 - val_loss: 1208941056.0000\n",
            "Epoch 10849/20000\n",
            "1/1 - 0s - 79ms/step - loss: 1045422400.0000 - val_loss: 1206904832.0000\n",
            "Epoch 10850/20000\n",
            "1/1 - 0s - 72ms/step - loss: 1044815744.0000 - val_loss: 1206023296.0000\n",
            "Epoch 10851/20000\n",
            "1/1 - 0s - 141ms/step - loss: 1044218624.0000 - val_loss: 1206999168.0000\n",
            "Epoch 10852/20000\n",
            "1/1 - 0s - 140ms/step - loss: 1043630336.0000 - val_loss: 1205301632.0000\n",
            "Epoch 10853/20000\n",
            "1/1 - 0s - 110ms/step - loss: 1043010048.0000 - val_loss: 1204049408.0000\n",
            "Epoch 10854/20000\n",
            "1/1 - 0s - 125ms/step - loss: 1042432128.0000 - val_loss: 1204128000.0000\n",
            "Epoch 10855/20000\n",
            "1/1 - 0s - 93ms/step - loss: 1041845632.0000 - val_loss: 1203863296.0000\n",
            "Epoch 10856/20000\n",
            "1/1 - 0s - 98ms/step - loss: 1041270592.0000 - val_loss: 1202696832.0000\n",
            "Epoch 10857/20000\n",
            "1/1 - 0s - 161ms/step - loss: 1040683328.0000 - val_loss: 1201081216.0000\n",
            "Epoch 10858/20000\n",
            "1/1 - 0s - 116ms/step - loss: 1040126208.0000 - val_loss: 1202062976.0000\n",
            "Epoch 10859/20000\n",
            "1/1 - 0s - 142ms/step - loss: 1039546112.0000 - val_loss: 1201387776.0000\n",
            "Epoch 10860/20000\n",
            "1/1 - 0s - 82ms/step - loss: 1038965504.0000 - val_loss: 1200050432.0000\n",
            "Epoch 10861/20000\n",
            "1/1 - 0s - 155ms/step - loss: 1038410432.0000 - val_loss: 1200288512.0000\n",
            "Epoch 10862/20000\n",
            "1/1 - 0s - 97ms/step - loss: 1037836992.0000 - val_loss: 1200011776.0000\n",
            "Epoch 10863/20000\n",
            "1/1 - 0s - 124ms/step - loss: 1037287424.0000 - val_loss: 1198178944.0000\n",
            "Epoch 10864/20000\n",
            "1/1 - 0s - 129ms/step - loss: 1036747904.0000 - val_loss: 1198483968.0000\n",
            "Epoch 10865/20000\n",
            "1/1 - 0s - 74ms/step - loss: 1036183808.0000 - val_loss: 1197761536.0000\n",
            "Epoch 10866/20000\n",
            "1/1 - 0s - 58ms/step - loss: 1035629184.0000 - val_loss: 1196540032.0000\n",
            "Epoch 10867/20000\n",
            "1/1 - 0s - 57ms/step - loss: 1035096000.0000 - val_loss: 1196032384.0000\n",
            "Epoch 10868/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1034559168.0000 - val_loss: 1196041600.0000\n",
            "Epoch 10869/20000\n",
            "1/1 - 0s - 139ms/step - loss: 1034013504.0000 - val_loss: 1195003904.0000\n",
            "Epoch 10870/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1033480768.0000 - val_loss: 1194463104.0000\n",
            "Epoch 10871/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1032940672.0000 - val_loss: 1194064384.0000\n",
            "Epoch 10872/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1032403264.0000 - val_loss: 1193894272.0000\n",
            "Epoch 10873/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1031855616.0000 - val_loss: 1192680832.0000\n",
            "Epoch 10874/20000\n",
            "1/1 - 0s - 138ms/step - loss: 1031319232.0000 - val_loss: 1192623488.0000\n",
            "Epoch 10875/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1030777600.0000 - val_loss: 1191977984.0000\n",
            "Epoch 10876/20000\n",
            "1/1 - 0s - 68ms/step - loss: 1030225536.0000 - val_loss: 1190797824.0000\n",
            "Epoch 10877/20000\n",
            "1/1 - 0s - 116ms/step - loss: 1029705472.0000 - val_loss: 1191481728.0000\n",
            "Epoch 10878/20000\n",
            "1/1 - 0s - 91ms/step - loss: 1029171008.0000 - val_loss: 1190542848.0000\n",
            "Epoch 10879/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1028633472.0000 - val_loss: 1189873024.0000\n",
            "Epoch 10880/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1028100800.0000 - val_loss: 1189387008.0000\n",
            "Epoch 10881/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1027573184.0000 - val_loss: 1188754432.0000\n",
            "Epoch 10882/20000\n",
            "1/1 - 0s - 63ms/step - loss: 1027060416.0000 - val_loss: 1188004224.0000\n",
            "Epoch 10883/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1026542592.0000 - val_loss: 1187753984.0000\n",
            "Epoch 10884/20000\n",
            "1/1 - 0s - 139ms/step - loss: 1026029376.0000 - val_loss: 1186755328.0000\n",
            "Epoch 10885/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1025532864.0000 - val_loss: 1187398656.0000\n",
            "Epoch 10886/20000\n",
            "1/1 - 0s - 63ms/step - loss: 1025022656.0000 - val_loss: 1186145280.0000\n",
            "Epoch 10887/20000\n",
            "1/1 - 0s - 63ms/step - loss: 1024506944.0000 - val_loss: 1186024704.0000\n",
            "Epoch 10888/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1024009664.0000 - val_loss: 1184583680.0000\n",
            "Epoch 10889/20000\n",
            "1/1 - 0s - 74ms/step - loss: 1023534336.0000 - val_loss: 1184940160.0000\n",
            "Epoch 10890/20000\n",
            "1/1 - 0s - 137ms/step - loss: 1023026624.0000 - val_loss: 1184409472.0000\n",
            "Epoch 10891/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1022538176.0000 - val_loss: 1183969664.0000\n",
            "Epoch 10892/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1022048640.0000 - val_loss: 1183260416.0000\n",
            "Epoch 10893/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1021577088.0000 - val_loss: 1183528832.0000\n",
            "Epoch 10894/20000\n",
            "1/1 - 0s - 64ms/step - loss: 1021112192.0000 - val_loss: 1182187648.0000\n",
            "Epoch 10895/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1020622400.0000 - val_loss: 1182068608.0000\n",
            "Epoch 10896/20000\n",
            "1/1 - 0s - 63ms/step - loss: 1020149184.0000 - val_loss: 1180825472.0000\n",
            "Epoch 10897/20000\n",
            "1/1 - 0s - 63ms/step - loss: 1019702208.0000 - val_loss: 1180567680.0000\n",
            "Epoch 10898/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1019239168.0000 - val_loss: 1180149760.0000\n",
            "Epoch 10899/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1018768320.0000 - val_loss: 1180434816.0000\n",
            "Epoch 10900/20000\n",
            "1/1 - 0s - 65ms/step - loss: 1018302912.0000 - val_loss: 1179774080.0000\n",
            "Epoch 10901/20000\n",
            "1/1 - 0s - 143ms/step - loss: 1017850240.0000 - val_loss: 1178951168.0000\n",
            "Epoch 10902/20000\n",
            "1/1 - 0s - 73ms/step - loss: 1017408064.0000 - val_loss: 1179515648.0000\n",
            "Epoch 10903/20000\n",
            "1/1 - 0s - 64ms/step - loss: 1016950080.0000 - val_loss: 1178170496.0000\n",
            "Epoch 10904/20000\n",
            "1/1 - 0s - 70ms/step - loss: 1016509056.0000 - val_loss: 1178057856.0000\n",
            "Epoch 10905/20000\n",
            "1/1 - 0s - 59ms/step - loss: 1016065920.0000 - val_loss: 1177909504.0000\n",
            "Epoch 10906/20000\n",
            "1/1 - 0s - 142ms/step - loss: 1015603328.0000 - val_loss: 1176750464.0000\n",
            "Epoch 10907/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1015137216.0000 - val_loss: 1176443520.0000\n",
            "Epoch 10908/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1014681536.0000 - val_loss: 1175890304.0000\n",
            "Epoch 10909/20000\n",
            "1/1 - 0s - 64ms/step - loss: 1014221248.0000 - val_loss: 1175648640.0000\n",
            "Epoch 10910/20000\n",
            "1/1 - 0s - 143ms/step - loss: 1013766336.0000 - val_loss: 1175266048.0000\n",
            "Epoch 10911/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1013313024.0000 - val_loss: 1175245568.0000\n",
            "Epoch 10912/20000\n",
            "1/1 - 0s - 63ms/step - loss: 1012855680.0000 - val_loss: 1173924736.0000\n",
            "Epoch 10913/20000\n",
            "1/1 - 0s - 140ms/step - loss: 1012392512.0000 - val_loss: 1173368704.0000\n",
            "Epoch 10914/20000\n",
            "1/1 - 0s - 133ms/step - loss: 1011936064.0000 - val_loss: 1174486912.0000\n",
            "Epoch 10915/20000\n",
            "1/1 - 0s - 139ms/step - loss: 1011487232.0000 - val_loss: 1173797248.0000\n",
            "Epoch 10916/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1011033536.0000 - val_loss: 1172166016.0000\n",
            "Epoch 10917/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1010591744.0000 - val_loss: 1172868736.0000\n",
            "Epoch 10918/20000\n",
            "1/1 - 0s - 139ms/step - loss: 1010139456.0000 - val_loss: 1171729408.0000\n",
            "Epoch 10919/20000\n",
            "1/1 - 0s - 61ms/step - loss: 1009710272.0000 - val_loss: 1171731968.0000\n",
            "Epoch 10920/20000\n",
            "1/1 - 0s - 141ms/step - loss: 1009269632.0000 - val_loss: 1172063104.0000\n",
            "Epoch 10921/20000\n",
            "1/1 - 0s - 59ms/step - loss: 1008837696.0000 - val_loss: 1170801280.0000\n",
            "Epoch 10922/20000\n",
            "1/1 - 0s - 65ms/step - loss: 1008392576.0000 - val_loss: 1170050432.0000\n",
            "Epoch 10923/20000\n",
            "1/1 - 0s - 75ms/step - loss: 1007965056.0000 - val_loss: 1170491904.0000\n",
            "Epoch 10924/20000\n",
            "1/1 - 0s - 137ms/step - loss: 1007541184.0000 - val_loss: 1170041344.0000\n",
            "Epoch 10925/20000\n",
            "1/1 - 0s - 59ms/step - loss: 1007107712.0000 - val_loss: 1169086976.0000\n",
            "Epoch 10926/20000\n",
            "1/1 - 0s - 138ms/step - loss: 1006678656.0000 - val_loss: 1168752384.0000\n",
            "Epoch 10927/20000\n",
            "1/1 - 0s - 63ms/step - loss: 1006240512.0000 - val_loss: 1168061056.0000\n",
            "Epoch 10928/20000\n",
            "1/1 - 0s - 65ms/step - loss: 1005813376.0000 - val_loss: 1168074880.0000\n",
            "Epoch 10929/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1005382784.0000 - val_loss: 1168558464.0000\n",
            "Epoch 10930/20000\n",
            "1/1 - 0s - 59ms/step - loss: 1004960064.0000 - val_loss: 1168305152.0000\n",
            "Epoch 10931/20000\n",
            "1/1 - 0s - 58ms/step - loss: 1004542848.0000 - val_loss: 1167567232.0000\n",
            "Epoch 10932/20000\n",
            "1/1 - 0s - 141ms/step - loss: 1004113984.0000 - val_loss: 1167554432.0000\n",
            "Epoch 10933/20000\n",
            "1/1 - 0s - 145ms/step - loss: 1003696000.0000 - val_loss: 1165408512.0000\n",
            "Epoch 10934/20000\n",
            "1/1 - 0s - 132ms/step - loss: 1003292928.0000 - val_loss: 1165510784.0000\n",
            "Epoch 10935/20000\n",
            "1/1 - 0s - 58ms/step - loss: 1002856768.0000 - val_loss: 1166434688.0000\n",
            "Epoch 10936/20000\n",
            "1/1 - 0s - 62ms/step - loss: 1002450048.0000 - val_loss: 1164950528.0000\n",
            "Epoch 10937/20000\n",
            "1/1 - 0s - 138ms/step - loss: 1002037952.0000 - val_loss: 1164531328.0000\n",
            "Epoch 10938/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1001626944.0000 - val_loss: 1165499648.0000\n",
            "Epoch 10939/20000\n",
            "1/1 - 0s - 67ms/step - loss: 1001202752.0000 - val_loss: 1164763776.0000\n",
            "Epoch 10940/20000\n",
            "1/1 - 0s - 134ms/step - loss: 1000788288.0000 - val_loss: 1163691008.0000\n",
            "Epoch 10941/20000\n",
            "1/1 - 0s - 60ms/step - loss: 1000394688.0000 - val_loss: 1164115328.0000\n",
            "Epoch 10942/20000\n",
            "1/1 - 0s - 138ms/step - loss: 999974144.0000 - val_loss: 1163892608.0000\n",
            "Epoch 10943/20000\n",
            "1/1 - 0s - 84ms/step - loss: 999556416.0000 - val_loss: 1163758464.0000\n",
            "Epoch 10944/20000\n",
            "1/1 - 0s - 70ms/step - loss: 999144512.0000 - val_loss: 1161796992.0000\n",
            "Epoch 10945/20000\n",
            "1/1 - 0s - 63ms/step - loss: 998746496.0000 - val_loss: 1161238272.0000\n",
            "Epoch 10946/20000\n",
            "1/1 - 0s - 60ms/step - loss: 998338880.0000 - val_loss: 1161822464.0000\n",
            "Epoch 10947/20000\n",
            "1/1 - 0s - 63ms/step - loss: 997941632.0000 - val_loss: 1161432192.0000\n",
            "Epoch 10948/20000\n",
            "1/1 - 0s - 65ms/step - loss: 997545600.0000 - val_loss: 1160480128.0000\n",
            "Epoch 10949/20000\n",
            "1/1 - 0s - 67ms/step - loss: 997142016.0000 - val_loss: 1159808384.0000\n",
            "Epoch 10950/20000\n",
            "1/1 - 0s - 140ms/step - loss: 996755264.0000 - val_loss: 1160291712.0000\n",
            "Epoch 10951/20000\n",
            "1/1 - 0s - 59ms/step - loss: 996353856.0000 - val_loss: 1159776128.0000\n",
            "Epoch 10952/20000\n",
            "1/1 - 0s - 62ms/step - loss: 995952704.0000 - val_loss: 1158945024.0000\n",
            "Epoch 10953/20000\n",
            "1/1 - 0s - 63ms/step - loss: 995558080.0000 - val_loss: 1159837696.0000\n",
            "Epoch 10954/20000\n",
            "1/1 - 0s - 59ms/step - loss: 995176128.0000 - val_loss: 1158560896.0000\n",
            "Epoch 10955/20000\n",
            "1/1 - 0s - 63ms/step - loss: 994793408.0000 - val_loss: 1158285440.0000\n",
            "Epoch 10956/20000\n",
            "1/1 - 0s - 63ms/step - loss: 994412032.0000 - val_loss: 1159887232.0000\n",
            "Epoch 10957/20000\n",
            "1/1 - 0s - 70ms/step - loss: 994058944.0000 - val_loss: 1158045824.0000\n",
            "Epoch 10958/20000\n",
            "1/1 - 0s - 132ms/step - loss: 993636800.0000 - val_loss: 1155727104.0000\n",
            "Epoch 10959/20000\n",
            "1/1 - 0s - 64ms/step - loss: 993347392.0000 - val_loss: 1158424960.0000\n",
            "Epoch 10960/20000\n",
            "1/1 - 0s - 59ms/step - loss: 992904448.0000 - val_loss: 1158155904.0000\n",
            "Epoch 10961/20000\n",
            "1/1 - 0s - 62ms/step - loss: 992524800.0000 - val_loss: 1155119872.0000\n",
            "Epoch 10962/20000\n",
            "1/1 - 0s - 80ms/step - loss: 992138816.0000 - val_loss: 1155570304.0000\n",
            "Epoch 10963/20000\n",
            "1/1 - 0s - 139ms/step - loss: 991711360.0000 - val_loss: 1156851840.0000\n",
            "Epoch 10964/20000\n",
            "1/1 - 0s - 59ms/step - loss: 991347840.0000 - val_loss: 1154985216.0000\n",
            "Epoch 10965/20000\n",
            "1/1 - 0s - 60ms/step - loss: 990943040.0000 - val_loss: 1153757568.0000\n",
            "Epoch 10966/20000\n",
            "1/1 - 0s - 140ms/step - loss: 990579648.0000 - val_loss: 1156802560.0000\n",
            "Epoch 10967/20000\n",
            "1/1 - 0s - 147ms/step - loss: 990220416.0000 - val_loss: 1155369984.0000\n",
            "Epoch 10968/20000\n",
            "1/1 - 0s - 72ms/step - loss: 989777216.0000 - val_loss: 1151924608.0000\n",
            "Epoch 10969/20000\n",
            "1/1 - 0s - 63ms/step - loss: 989499712.0000 - val_loss: 1155342976.0000\n",
            "Epoch 10970/20000\n",
            "1/1 - 0s - 70ms/step - loss: 989031360.0000 - val_loss: 1155492096.0000\n",
            "Epoch 10971/20000\n",
            "1/1 - 0s - 136ms/step - loss: 988660224.0000 - val_loss: 1152024320.0000\n",
            "Epoch 10972/20000\n",
            "1/1 - 0s - 70ms/step - loss: 988258688.0000 - val_loss: 1152431744.0000\n",
            "Epoch 10973/20000\n",
            "1/1 - 0s - 61ms/step - loss: 987833088.0000 - val_loss: 1154244736.0000\n",
            "Epoch 10974/20000\n",
            "1/1 - 0s - 67ms/step - loss: 987488832.0000 - val_loss: 1152813952.0000\n",
            "Epoch 10975/20000\n",
            "1/1 - 0s - 62ms/step - loss: 987071616.0000 - val_loss: 1150838784.0000\n",
            "Epoch 10976/20000\n",
            "1/1 - 0s - 63ms/step - loss: 986766272.0000 - val_loss: 1154123136.0000\n",
            "Epoch 10977/20000\n",
            "1/1 - 0s - 61ms/step - loss: 986395840.0000 - val_loss: 1152662912.0000\n",
            "Epoch 10978/20000\n",
            "1/1 - 0s - 59ms/step - loss: 985977856.0000 - val_loss: 1149436672.0000\n",
            "Epoch 10979/20000\n",
            "1/1 - 0s - 146ms/step - loss: 985713408.0000 - val_loss: 1152767232.0000\n",
            "Epoch 10980/20000\n",
            "1/1 - 0s - 147ms/step - loss: 985272448.0000 - val_loss: 1152286464.0000\n",
            "Epoch 10981/20000\n",
            "1/1 - 0s - 141ms/step - loss: 984908096.0000 - val_loss: 1148877696.0000\n",
            "Epoch 10982/20000\n",
            "1/1 - 0s - 117ms/step - loss: 984555136.0000 - val_loss: 1149715456.0000\n",
            "Epoch 10983/20000\n",
            "1/1 - 0s - 115ms/step - loss: 984150464.0000 - val_loss: 1150917248.0000\n",
            "Epoch 10984/20000\n",
            "1/1 - 0s - 139ms/step - loss: 983817344.0000 - val_loss: 1149560832.0000\n",
            "Epoch 10985/20000\n",
            "1/1 - 0s - 139ms/step - loss: 983454592.0000 - val_loss: 1148246016.0000\n",
            "Epoch 10986/20000\n",
            "1/1 - 0s - 143ms/step - loss: 983119808.0000 - val_loss: 1149133696.0000\n",
            "Epoch 10987/20000\n",
            "1/1 - 0s - 152ms/step - loss: 982735360.0000 - val_loss: 1149862912.0000\n",
            "Epoch 10988/20000\n",
            "1/1 - 0s - 87ms/step - loss: 982385600.0000 - val_loss: 1147934592.0000\n",
            "Epoch 10989/20000\n",
            "1/1 - 0s - 139ms/step - loss: 982057536.0000 - val_loss: 1148837376.0000\n",
            "Epoch 10990/20000\n",
            "1/1 - 0s - 145ms/step - loss: 981688704.0000 - val_loss: 1149011328.0000\n",
            "Epoch 10991/20000\n",
            "1/1 - 0s - 125ms/step - loss: 981344128.0000 - val_loss: 1146416640.0000\n",
            "Epoch 10992/20000\n",
            "1/1 - 0s - 138ms/step - loss: 981024640.0000 - val_loss: 1148036480.0000\n",
            "Epoch 10993/20000\n",
            "1/1 - 0s - 95ms/step - loss: 980655232.0000 - val_loss: 1147679616.0000\n",
            "Epoch 10994/20000\n",
            "1/1 - 0s - 160ms/step - loss: 980301184.0000 - val_loss: 1145854336.0000\n",
            "Epoch 10995/20000\n",
            "1/1 - 0s - 116ms/step - loss: 979953344.0000 - val_loss: 1147062144.0000\n",
            "Epoch 10996/20000\n",
            "1/1 - 0s - 108ms/step - loss: 979588928.0000 - val_loss: 1146356608.0000\n",
            "Epoch 10997/20000\n",
            "1/1 - 0s - 101ms/step - loss: 979216960.0000 - val_loss: 1145769600.0000\n",
            "Epoch 10998/20000\n",
            "1/1 - 0s - 134ms/step - loss: 978886272.0000 - val_loss: 1146693120.0000\n",
            "Epoch 10999/20000\n",
            "1/1 - 0s - 105ms/step - loss: 978571904.0000 - val_loss: 1144763648.0000\n",
            "Epoch 11000/20000\n",
            "1/1 - 0s - 153ms/step - loss: 978225920.0000 - val_loss: 1144364032.0000\n",
            "Epoch 11001/20000\n",
            "1/1 - 0s - 151ms/step - loss: 977877504.0000 - val_loss: 1145390976.0000\n",
            "Epoch 11002/20000\n",
            "1/1 - 0s - 107ms/step - loss: 977554240.0000 - val_loss: 1143662720.0000\n",
            "Epoch 11003/20000\n",
            "1/1 - 0s - 151ms/step - loss: 977208960.0000 - val_loss: 1143074816.0000\n",
            "Epoch 11004/20000\n",
            "1/1 - 0s - 96ms/step - loss: 976881472.0000 - val_loss: 1144613632.0000\n",
            "Epoch 11005/20000\n",
            "1/1 - 0s - 128ms/step - loss: 976514176.0000 - val_loss: 1144065792.0000\n",
            "Epoch 11006/20000\n",
            "1/1 - 0s - 96ms/step - loss: 976161216.0000 - val_loss: 1143721856.0000\n",
            "Epoch 11007/20000\n",
            "1/1 - 0s - 125ms/step - loss: 975821696.0000 - val_loss: 1143934080.0000\n",
            "Epoch 11008/20000\n",
            "1/1 - 0s - 106ms/step - loss: 975491968.0000 - val_loss: 1142844288.0000\n",
            "Epoch 11009/20000\n",
            "1/1 - 0s - 136ms/step - loss: 975160256.0000 - val_loss: 1143533696.0000\n",
            "Epoch 11010/20000\n",
            "1/1 - 0s - 126ms/step - loss: 974829568.0000 - val_loss: 1142405376.0000\n",
            "Epoch 11011/20000\n",
            "1/1 - 0s - 128ms/step - loss: 974492544.0000 - val_loss: 1142893440.0000\n",
            "Epoch 11012/20000\n",
            "1/1 - 0s - 66ms/step - loss: 974147712.0000 - val_loss: 1142540032.0000\n",
            "Epoch 11013/20000\n",
            "1/1 - 0s - 71ms/step - loss: 973838528.0000 - val_loss: 1141244544.0000\n",
            "Epoch 11014/20000\n",
            "1/1 - 0s - 135ms/step - loss: 973526080.0000 - val_loss: 1141672320.0000\n",
            "Epoch 11015/20000\n",
            "1/1 - 0s - 68ms/step - loss: 973183680.0000 - val_loss: 1141767936.0000\n",
            "Epoch 11016/20000\n",
            "1/1 - 0s - 65ms/step - loss: 972858240.0000 - val_loss: 1140334464.0000\n",
            "Epoch 11017/20000\n",
            "1/1 - 0s - 143ms/step - loss: 972545152.0000 - val_loss: 1141460736.0000\n",
            "Epoch 11018/20000\n",
            "1/1 - 0s - 62ms/step - loss: 972195840.0000 - val_loss: 1140634752.0000\n",
            "Epoch 11019/20000\n",
            "1/1 - 0s - 61ms/step - loss: 971860416.0000 - val_loss: 1139999872.0000\n",
            "Epoch 11020/20000\n",
            "1/1 - 0s - 75ms/step - loss: 971540928.0000 - val_loss: 1141819136.0000\n",
            "Epoch 11021/20000\n",
            "1/1 - 0s - 73ms/step - loss: 971227712.0000 - val_loss: 1139781760.0000\n",
            "Epoch 11022/20000\n",
            "1/1 - 0s - 131ms/step - loss: 970901376.0000 - val_loss: 1139458304.0000\n",
            "Epoch 11023/20000\n",
            "1/1 - 0s - 61ms/step - loss: 970579264.0000 - val_loss: 1141185408.0000\n",
            "Epoch 11024/20000\n",
            "1/1 - 0s - 60ms/step - loss: 970270528.0000 - val_loss: 1138729600.0000\n",
            "Epoch 11025/20000\n",
            "1/1 - 0s - 59ms/step - loss: 969921600.0000 - val_loss: 1139134464.0000\n",
            "Epoch 11026/20000\n",
            "1/1 - 0s - 143ms/step - loss: 969573312.0000 - val_loss: 1139211648.0000\n",
            "Epoch 11027/20000\n",
            "1/1 - 0s - 61ms/step - loss: 969239936.0000 - val_loss: 1137649792.0000\n",
            "Epoch 11028/20000\n",
            "1/1 - 0s - 61ms/step - loss: 968951488.0000 - val_loss: 1139619200.0000\n",
            "Epoch 11029/20000\n",
            "1/1 - 0s - 138ms/step - loss: 968638976.0000 - val_loss: 1138879232.0000\n",
            "Epoch 11030/20000\n",
            "1/1 - 0s - 63ms/step - loss: 968295104.0000 - val_loss: 1137239808.0000\n",
            "Epoch 11031/20000\n",
            "1/1 - 0s - 67ms/step - loss: 967977984.0000 - val_loss: 1137832704.0000\n",
            "Epoch 11032/20000\n",
            "1/1 - 0s - 77ms/step - loss: 967632768.0000 - val_loss: 1137732864.0000\n",
            "Epoch 11033/20000\n",
            "1/1 - 0s - 126ms/step - loss: 967327232.0000 - val_loss: 1136619008.0000\n",
            "Epoch 11034/20000\n",
            "1/1 - 0s - 62ms/step - loss: 967036032.0000 - val_loss: 1138761600.0000\n",
            "Epoch 11035/20000\n",
            "1/1 - 0s - 62ms/step - loss: 966739648.0000 - val_loss: 1136822400.0000\n",
            "Epoch 11036/20000\n",
            "1/1 - 0s - 64ms/step - loss: 966400960.0000 - val_loss: 1136990720.0000\n",
            "Epoch 11037/20000\n",
            "1/1 - 0s - 60ms/step - loss: 966078208.0000 - val_loss: 1137759360.0000\n",
            "Epoch 11038/20000\n",
            "1/1 - 0s - 61ms/step - loss: 965764800.0000 - val_loss: 1136176384.0000\n",
            "Epoch 11039/20000\n",
            "1/1 - 0s - 65ms/step - loss: 965453440.0000 - val_loss: 1136382592.0000\n",
            "Epoch 11040/20000\n",
            "1/1 - 0s - 60ms/step - loss: 965135168.0000 - val_loss: 1136321408.0000\n",
            "Epoch 11041/20000\n",
            "1/1 - 0s - 57ms/step - loss: 964824192.0000 - val_loss: 1135015040.0000\n",
            "Epoch 11042/20000\n",
            "1/1 - 0s - 71ms/step - loss: 964515648.0000 - val_loss: 1135352320.0000\n",
            "Epoch 11043/20000\n",
            "1/1 - 0s - 63ms/step - loss: 964179904.0000 - val_loss: 1135127808.0000\n",
            "Epoch 11044/20000\n",
            "1/1 - 0s - 60ms/step - loss: 963857792.0000 - val_loss: 1134486528.0000\n",
            "Epoch 11045/20000\n",
            "1/1 - 0s - 78ms/step - loss: 963552896.0000 - val_loss: 1135306880.0000\n",
            "Epoch 11046/20000\n",
            "1/1 - 0s - 132ms/step - loss: 963244352.0000 - val_loss: 1135062528.0000\n",
            "Epoch 11047/20000\n",
            "1/1 - 0s - 61ms/step - loss: 962926976.0000 - val_loss: 1133292928.0000\n",
            "Epoch 11048/20000\n",
            "1/1 - 0s - 59ms/step - loss: 962609024.0000 - val_loss: 1134555904.0000\n",
            "Epoch 11049/20000\n",
            "1/1 - 0s - 71ms/step - loss: 962317248.0000 - val_loss: 1133264256.0000\n",
            "Epoch 11050/20000\n",
            "1/1 - 0s - 58ms/step - loss: 961976704.0000 - val_loss: 1131888384.0000\n",
            "Epoch 11051/20000\n",
            "1/1 - 0s - 61ms/step - loss: 961703104.0000 - val_loss: 1133934080.0000\n",
            "Epoch 11052/20000\n",
            "1/1 - 0s - 138ms/step - loss: 961382656.0000 - val_loss: 1132953728.0000\n",
            "Epoch 11053/20000\n",
            "1/1 - 0s - 60ms/step - loss: 961042560.0000 - val_loss: 1131054976.0000\n",
            "Epoch 11054/20000\n",
            "1/1 - 0s - 60ms/step - loss: 960790528.0000 - val_loss: 1133954048.0000\n",
            "Epoch 11055/20000\n",
            "1/1 - 0s - 141ms/step - loss: 960463616.0000 - val_loss: 1132438656.0000\n",
            "Epoch 11056/20000\n",
            "1/1 - 0s - 63ms/step - loss: 960098624.0000 - val_loss: 1130934272.0000\n",
            "Epoch 11057/20000\n",
            "1/1 - 0s - 137ms/step - loss: 959801536.0000 - val_loss: 1132355584.0000\n",
            "Epoch 11058/20000\n",
            "1/1 - 0s - 137ms/step - loss: 959479040.0000 - val_loss: 1131791488.0000\n",
            "Epoch 11059/20000\n",
            "1/1 - 0s - 59ms/step - loss: 959156736.0000 - val_loss: 1129894656.0000\n",
            "Epoch 11060/20000\n",
            "1/1 - 0s - 55ms/step - loss: 958875136.0000 - val_loss: 1131716608.0000\n",
            "Epoch 11061/20000\n",
            "1/1 - 0s - 60ms/step - loss: 958542528.0000 - val_loss: 1131080192.0000\n",
            "Epoch 11062/20000\n",
            "1/1 - 0s - 138ms/step - loss: 958206784.0000 - val_loss: 1129210496.0000\n",
            "Epoch 11063/20000\n",
            "1/1 - 0s - 58ms/step - loss: 957909824.0000 - val_loss: 1130331008.0000\n",
            "Epoch 11064/20000\n",
            "1/1 - 0s - 59ms/step - loss: 957583104.0000 - val_loss: 1130218240.0000\n",
            "Epoch 11065/20000\n",
            "1/1 - 0s - 60ms/step - loss: 957266048.0000 - val_loss: 1128162048.0000\n",
            "Epoch 11066/20000\n",
            "1/1 - 0s - 140ms/step - loss: 956970944.0000 - val_loss: 1129216640.0000\n",
            "Epoch 11067/20000\n",
            "1/1 - 0s - 67ms/step - loss: 956619520.0000 - val_loss: 1129948544.0000\n",
            "Epoch 11068/20000\n",
            "1/1 - 0s - 97ms/step - loss: 956319808.0000 - val_loss: 1127831296.0000\n",
            "Epoch 11069/20000\n",
            "1/1 - 0s - 113ms/step - loss: 956017856.0000 - val_loss: 1129232640.0000\n",
            "Epoch 11070/20000\n",
            "1/1 - 0s - 60ms/step - loss: 955690496.0000 - val_loss: 1128794112.0000\n",
            "Epoch 11071/20000\n",
            "1/1 - 0s - 61ms/step - loss: 955371392.0000 - val_loss: 1127104512.0000\n",
            "Epoch 11072/20000\n",
            "1/1 - 0s - 61ms/step - loss: 955055232.0000 - val_loss: 1128260608.0000\n",
            "Epoch 11073/20000\n",
            "1/1 - 0s - 63ms/step - loss: 954718912.0000 - val_loss: 1128164736.0000\n",
            "Epoch 11074/20000\n",
            "1/1 - 0s - 59ms/step - loss: 954403584.0000 - val_loss: 1126202368.0000\n",
            "Epoch 11075/20000\n",
            "1/1 - 0s - 60ms/step - loss: 954114176.0000 - val_loss: 1127726080.0000\n",
            "Epoch 11076/20000\n",
            "1/1 - 0s - 58ms/step - loss: 953771072.0000 - val_loss: 1127173504.0000\n",
            "Epoch 11077/20000\n",
            "1/1 - 0s - 145ms/step - loss: 953431424.0000 - val_loss: 1125633280.0000\n",
            "Epoch 11078/20000\n",
            "1/1 - 0s - 139ms/step - loss: 953134080.0000 - val_loss: 1127113728.0000\n",
            "Epoch 11079/20000\n",
            "1/1 - 0s - 137ms/step - loss: 952788288.0000 - val_loss: 1127030144.0000\n",
            "Epoch 11080/20000\n",
            "1/1 - 0s - 60ms/step - loss: 952460800.0000 - val_loss: 1124574080.0000\n",
            "Epoch 11081/20000\n",
            "1/1 - 0s - 59ms/step - loss: 952147200.0000 - val_loss: 1125625216.0000\n",
            "Epoch 11082/20000\n",
            "1/1 - 0s - 65ms/step - loss: 951751168.0000 - val_loss: 1126789888.0000\n",
            "Epoch 11083/20000\n",
            "1/1 - 0s - 133ms/step - loss: 951460736.0000 - val_loss: 1124238720.0000\n",
            "Epoch 11084/20000\n",
            "1/1 - 0s - 59ms/step - loss: 951098496.0000 - val_loss: 1123873408.0000\n",
            "Epoch 11085/20000\n",
            "1/1 - 0s - 60ms/step - loss: 950759872.0000 - val_loss: 1126226048.0000\n",
            "Epoch 11086/20000\n",
            "1/1 - 0s - 60ms/step - loss: 950422656.0000 - val_loss: 1124609920.0000\n",
            "Epoch 11087/20000\n",
            "1/1 - 0s - 58ms/step - loss: 950032512.0000 - val_loss: 1123671424.0000\n",
            "Epoch 11088/20000\n",
            "1/1 - 0s - 61ms/step - loss: 949696704.0000 - val_loss: 1125590016.0000\n",
            "Epoch 11089/20000\n",
            "1/1 - 0s - 146ms/step - loss: 949373376.0000 - val_loss: 1124279040.0000\n",
            "Epoch 11090/20000\n",
            "1/1 - 0s - 63ms/step - loss: 949013824.0000 - val_loss: 1122621696.0000\n",
            "Epoch 11091/20000\n",
            "1/1 - 0s - 75ms/step - loss: 948719360.0000 - val_loss: 1124400896.0000\n",
            "Epoch 11092/20000\n",
            "1/1 - 0s - 69ms/step - loss: 948403072.0000 - val_loss: 1123323776.0000\n",
            "Epoch 11093/20000\n",
            "1/1 - 0s - 62ms/step - loss: 948050496.0000 - val_loss: 1121304064.0000\n",
            "Epoch 11094/20000\n",
            "1/1 - 0s - 64ms/step - loss: 947715712.0000 - val_loss: 1121982336.0000\n",
            "Epoch 11095/20000\n",
            "1/1 - 0s - 65ms/step - loss: 947356928.0000 - val_loss: 1122690816.0000\n",
            "Epoch 11096/20000\n",
            "1/1 - 0s - 134ms/step - loss: 947038976.0000 - val_loss: 1121114752.0000\n",
            "Epoch 11097/20000\n",
            "1/1 - 0s - 60ms/step - loss: 946720832.0000 - val_loss: 1121451008.0000\n",
            "Epoch 11098/20000\n",
            "1/1 - 0s - 61ms/step - loss: 946352576.0000 - val_loss: 1121213696.0000\n",
            "Epoch 11099/20000\n",
            "1/1 - 0s - 65ms/step - loss: 945986496.0000 - val_loss: 1120662272.0000\n",
            "Epoch 11100/20000\n",
            "1/1 - 0s - 61ms/step - loss: 945639744.0000 - val_loss: 1121177344.0000\n",
            "Epoch 11101/20000\n",
            "1/1 - 0s - 61ms/step - loss: 945297408.0000 - val_loss: 1121476608.0000\n",
            "Epoch 11102/20000\n",
            "1/1 - 0s - 62ms/step - loss: 944966976.0000 - val_loss: 1120017408.0000\n",
            "Epoch 11103/20000\n",
            "1/1 - 0s - 149ms/step - loss: 944635200.0000 - val_loss: 1120161792.0000\n",
            "Epoch 11104/20000\n",
            "1/1 - 0s - 85ms/step - loss: 944285312.0000 - val_loss: 1119952512.0000\n",
            "Epoch 11105/20000\n",
            "1/1 - 0s - 134ms/step - loss: 943947776.0000 - val_loss: 1117663232.0000\n",
            "Epoch 11106/20000\n",
            "1/1 - 0s - 58ms/step - loss: 943624704.0000 - val_loss: 1119065600.0000\n",
            "Epoch 11107/20000\n",
            "1/1 - 0s - 139ms/step - loss: 943261888.0000 - val_loss: 1118433024.0000\n",
            "Epoch 11108/20000\n",
            "1/1 - 0s - 63ms/step - loss: 942910464.0000 - val_loss: 1117148032.0000\n",
            "Epoch 11109/20000\n",
            "1/1 - 0s - 63ms/step - loss: 942574400.0000 - val_loss: 1117948032.0000\n",
            "Epoch 11110/20000\n",
            "1/1 - 0s - 59ms/step - loss: 942210496.0000 - val_loss: 1116967040.0000\n",
            "Epoch 11111/20000\n",
            "1/1 - 0s - 139ms/step - loss: 941873664.0000 - val_loss: 1117232512.0000\n",
            "Epoch 11112/20000\n",
            "1/1 - 0s - 62ms/step - loss: 941520448.0000 - val_loss: 1116710528.0000\n",
            "Epoch 11113/20000\n",
            "1/1 - 0s - 92ms/step - loss: 941175616.0000 - val_loss: 1116923008.0000\n",
            "Epoch 11114/20000\n",
            "1/1 - 0s - 76ms/step - loss: 940829824.0000 - val_loss: 1116108672.0000\n",
            "Epoch 11115/20000\n",
            "1/1 - 0s - 65ms/step - loss: 940486208.0000 - val_loss: 1116027520.0000\n",
            "Epoch 11116/20000\n",
            "1/1 - 0s - 59ms/step - loss: 940133952.0000 - val_loss: 1115310208.0000\n",
            "Epoch 11117/20000\n",
            "1/1 - 0s - 59ms/step - loss: 939774272.0000 - val_loss: 1115437312.0000\n",
            "Epoch 11118/20000\n",
            "1/1 - 0s - 61ms/step - loss: 939431104.0000 - val_loss: 1114045056.0000\n",
            "Epoch 11119/20000\n",
            "1/1 - 0s - 139ms/step - loss: 939084160.0000 - val_loss: 1114928640.0000\n",
            "Epoch 11120/20000\n",
            "1/1 - 0s - 66ms/step - loss: 938733888.0000 - val_loss: 1113131392.0000\n",
            "Epoch 11121/20000\n",
            "1/1 - 0s - 141ms/step - loss: 938382784.0000 - val_loss: 1114035712.0000\n",
            "Epoch 11122/20000\n",
            "1/1 - 0s - 60ms/step - loss: 938027584.0000 - val_loss: 1112533120.0000\n",
            "Epoch 11123/20000\n",
            "1/1 - 0s - 136ms/step - loss: 937680384.0000 - val_loss: 1112897408.0000\n",
            "Epoch 11124/20000\n",
            "1/1 - 0s - 143ms/step - loss: 937315712.0000 - val_loss: 1113008896.0000\n",
            "Epoch 11125/20000\n",
            "1/1 - 0s - 140ms/step - loss: 936958336.0000 - val_loss: 1111745408.0000\n",
            "Epoch 11126/20000\n",
            "1/1 - 0s - 74ms/step - loss: 936598080.0000 - val_loss: 1111970944.0000\n",
            "Epoch 11127/20000\n",
            "1/1 - 0s - 145ms/step - loss: 936235072.0000 - val_loss: 1110507392.0000\n",
            "Epoch 11128/20000\n",
            "1/1 - 0s - 136ms/step - loss: 935865344.0000 - val_loss: 1109436288.0000\n",
            "Epoch 11129/20000\n",
            "1/1 - 0s - 132ms/step - loss: 935514176.0000 - val_loss: 1110557696.0000\n",
            "Epoch 11130/20000\n",
            "1/1 - 0s - 138ms/step - loss: 935165184.0000 - val_loss: 1109306496.0000\n",
            "Epoch 11131/20000\n",
            "1/1 - 0s - 147ms/step - loss: 934783808.0000 - val_loss: 1108032512.0000\n",
            "Epoch 11132/20000\n",
            "1/1 - 0s - 131ms/step - loss: 934450752.0000 - val_loss: 1109655424.0000\n",
            "Epoch 11133/20000\n",
            "1/1 - 0s - 84ms/step - loss: 934074752.0000 - val_loss: 1108076160.0000\n",
            "Epoch 11134/20000\n",
            "1/1 - 0s - 82ms/step - loss: 933680384.0000 - val_loss: 1107402240.0000\n",
            "Epoch 11135/20000\n",
            "1/1 - 0s - 171ms/step - loss: 933304704.0000 - val_loss: 1107632768.0000\n",
            "Epoch 11136/20000\n",
            "1/1 - 0s - 102ms/step - loss: 932956416.0000 - val_loss: 1106728064.0000\n",
            "Epoch 11137/20000\n",
            "1/1 - 0s - 121ms/step - loss: 932590656.0000 - val_loss: 1105677440.0000\n",
            "Epoch 11138/20000\n",
            "1/1 - 0s - 90ms/step - loss: 932213248.0000 - val_loss: 1105819264.0000\n",
            "Epoch 11139/20000\n",
            "1/1 - 0s - 115ms/step - loss: 931845184.0000 - val_loss: 1105356160.0000\n",
            "Epoch 11140/20000\n",
            "1/1 - 0s - 164ms/step - loss: 931478784.0000 - val_loss: 1104123776.0000\n",
            "Epoch 11141/20000\n",
            "1/1 - 0s - 78ms/step - loss: 931102144.0000 - val_loss: 1104174208.0000\n",
            "Epoch 11142/20000\n",
            "1/1 - 0s - 122ms/step - loss: 930726400.0000 - val_loss: 1103335424.0000\n",
            "Epoch 11143/20000\n",
            "1/1 - 0s - 89ms/step - loss: 930354624.0000 - val_loss: 1101712256.0000\n",
            "Epoch 11144/20000\n",
            "1/1 - 0s - 97ms/step - loss: 930000832.0000 - val_loss: 1102459264.0000\n",
            "Epoch 11145/20000\n",
            "1/1 - 0s - 140ms/step - loss: 929611584.0000 - val_loss: 1102092032.0000\n",
            "Epoch 11146/20000\n",
            "1/1 - 0s - 140ms/step - loss: 929215360.0000 - val_loss: 1100477568.0000\n",
            "Epoch 11147/20000\n",
            "1/1 - 0s - 92ms/step - loss: 928855872.0000 - val_loss: 1100995968.0000\n",
            "Epoch 11148/20000\n",
            "1/1 - 0s - 110ms/step - loss: 928466432.0000 - val_loss: 1101675136.0000\n",
            "Epoch 11149/20000\n",
            "1/1 - 0s - 164ms/step - loss: 928101184.0000 - val_loss: 1099730944.0000\n",
            "Epoch 11150/20000\n",
            "1/1 - 0s - 104ms/step - loss: 927688576.0000 - val_loss: 1098314496.0000\n",
            "Epoch 11151/20000\n",
            "1/1 - 0s - 131ms/step - loss: 927302400.0000 - val_loss: 1098924416.0000\n",
            "Epoch 11152/20000\n",
            "1/1 - 0s - 130ms/step - loss: 926906112.0000 - val_loss: 1098187776.0000\n",
            "Epoch 11153/20000\n",
            "1/1 - 0s - 143ms/step - loss: 926514816.0000 - val_loss: 1097293312.0000\n",
            "Epoch 11154/20000\n",
            "1/1 - 0s - 150ms/step - loss: 926115136.0000 - val_loss: 1097053568.0000\n",
            "Epoch 11155/20000\n",
            "1/1 - 0s - 144ms/step - loss: 925690304.0000 - val_loss: 1096280832.0000\n",
            "Epoch 11156/20000\n",
            "1/1 - 0s - 121ms/step - loss: 925261504.0000 - val_loss: 1095534592.0000\n",
            "Epoch 11157/20000\n",
            "1/1 - 0s - 114ms/step - loss: 924845184.0000 - val_loss: 1095372160.0000\n",
            "Epoch 11158/20000\n",
            "1/1 - 0s - 106ms/step - loss: 924412736.0000 - val_loss: 1095060480.0000\n",
            "Epoch 11159/20000\n",
            "1/1 - 0s - 110ms/step - loss: 923983744.0000 - val_loss: 1094192128.0000\n",
            "Epoch 11160/20000\n",
            "1/1 - 0s - 63ms/step - loss: 923554496.0000 - val_loss: 1094441984.0000\n",
            "Epoch 11161/20000\n",
            "1/1 - 0s - 62ms/step - loss: 923104512.0000 - val_loss: 1092619520.0000\n",
            "Epoch 11162/20000\n",
            "1/1 - 0s - 67ms/step - loss: 922634880.0000 - val_loss: 1091955200.0000\n",
            "Epoch 11163/20000\n",
            "1/1 - 0s - 64ms/step - loss: 922177280.0000 - val_loss: 1091177856.0000\n",
            "Epoch 11164/20000\n",
            "1/1 - 0s - 62ms/step - loss: 921736000.0000 - val_loss: 1090525056.0000\n",
            "Epoch 11165/20000\n",
            "1/1 - 0s - 64ms/step - loss: 921283200.0000 - val_loss: 1090377600.0000\n",
            "Epoch 11166/20000\n",
            "1/1 - 0s - 70ms/step - loss: 920798720.0000 - val_loss: 1090127872.0000\n",
            "Epoch 11167/20000\n",
            "1/1 - 0s - 76ms/step - loss: 920310848.0000 - val_loss: 1089597440.0000\n",
            "Epoch 11168/20000\n",
            "1/1 - 0s - 69ms/step - loss: 919819904.0000 - val_loss: 1089708672.0000\n",
            "Epoch 11169/20000\n",
            "1/1 - 0s - 136ms/step - loss: 919337280.0000 - val_loss: 1088210816.0000\n",
            "Epoch 11170/20000\n",
            "1/1 - 0s - 62ms/step - loss: 918822592.0000 - val_loss: 1087112064.0000\n",
            "Epoch 11171/20000\n",
            "1/1 - 0s - 139ms/step - loss: 918331392.0000 - val_loss: 1088278528.0000\n",
            "Epoch 11172/20000\n",
            "1/1 - 0s - 62ms/step - loss: 917821760.0000 - val_loss: 1087152256.0000\n",
            "Epoch 11173/20000\n",
            "1/1 - 0s - 62ms/step - loss: 917284352.0000 - val_loss: 1084806784.0000\n",
            "Epoch 11174/20000\n",
            "1/1 - 0s - 63ms/step - loss: 916786176.0000 - val_loss: 1085283968.0000\n",
            "Epoch 11175/20000\n",
            "1/1 - 0s - 64ms/step - loss: 916241728.0000 - val_loss: 1084261504.0000\n",
            "Epoch 11176/20000\n",
            "1/1 - 0s - 63ms/step - loss: 915696064.0000 - val_loss: 1083600000.0000\n",
            "Epoch 11177/20000\n",
            "1/1 - 0s - 63ms/step - loss: 915157440.0000 - val_loss: 1083483776.0000\n",
            "Epoch 11178/20000\n",
            "1/1 - 0s - 146ms/step - loss: 914620608.0000 - val_loss: 1082892928.0000\n",
            "Epoch 11179/20000\n",
            "1/1 - 0s - 73ms/step - loss: 914085504.0000 - val_loss: 1083594624.0000\n",
            "Epoch 11180/20000\n",
            "1/1 - 0s - 62ms/step - loss: 913542272.0000 - val_loss: 1081353600.0000\n",
            "Epoch 11181/20000\n",
            "1/1 - 0s - 76ms/step - loss: 912997632.0000 - val_loss: 1081138560.0000\n",
            "Epoch 11182/20000\n",
            "1/1 - 0s - 137ms/step - loss: 912420736.0000 - val_loss: 1080542592.0000\n",
            "Epoch 11183/20000\n",
            "1/1 - 0s - 66ms/step - loss: 911827648.0000 - val_loss: 1078949504.0000\n",
            "Epoch 11184/20000\n",
            "1/1 - 0s - 138ms/step - loss: 911214976.0000 - val_loss: 1078096640.0000\n",
            "Epoch 11185/20000\n",
            "1/1 - 0s - 137ms/step - loss: 910609600.0000 - val_loss: 1078724608.0000\n",
            "Epoch 11186/20000\n",
            "1/1 - 0s - 65ms/step - loss: 910005312.0000 - val_loss: 1077596288.0000\n",
            "Epoch 11187/20000\n",
            "1/1 - 0s - 63ms/step - loss: 909386624.0000 - val_loss: 1076815104.0000\n",
            "Epoch 11188/20000\n",
            "1/1 - 0s - 66ms/step - loss: 908746560.0000 - val_loss: 1076550656.0000\n",
            "Epoch 11189/20000\n",
            "1/1 - 0s - 78ms/step - loss: 908108672.0000 - val_loss: 1075519744.0000\n",
            "Epoch 11190/20000\n",
            "1/1 - 0s - 67ms/step - loss: 907468096.0000 - val_loss: 1074959232.0000\n",
            "Epoch 11191/20000\n",
            "1/1 - 0s - 66ms/step - loss: 906819008.0000 - val_loss: 1074876800.0000\n",
            "Epoch 11192/20000\n",
            "1/1 - 0s - 74ms/step - loss: 906168576.0000 - val_loss: 1072481024.0000\n",
            "Epoch 11193/20000\n",
            "1/1 - 0s - 121ms/step - loss: 905529920.0000 - val_loss: 1073009472.0000\n",
            "Epoch 11194/20000\n",
            "1/1 - 0s - 143ms/step - loss: 904846976.0000 - val_loss: 1072719360.0000\n",
            "Epoch 11195/20000\n",
            "1/1 - 0s - 62ms/step - loss: 904188032.0000 - val_loss: 1069950848.0000\n",
            "Epoch 11196/20000\n",
            "1/1 - 0s - 60ms/step - loss: 903530304.0000 - val_loss: 1070087040.0000\n",
            "Epoch 11197/20000\n",
            "1/1 - 0s - 59ms/step - loss: 902866624.0000 - val_loss: 1070518592.0000\n",
            "Epoch 11198/20000\n",
            "1/1 - 0s - 60ms/step - loss: 902232768.0000 - val_loss: 1068377216.0000\n",
            "Epoch 11199/20000\n",
            "1/1 - 0s - 59ms/step - loss: 901544640.0000 - val_loss: 1068019648.0000\n",
            "Epoch 11200/20000\n",
            "1/1 - 0s - 61ms/step - loss: 900871168.0000 - val_loss: 1069023936.0000\n",
            "Epoch 11201/20000\n",
            "1/1 - 0s - 147ms/step - loss: 900223232.0000 - val_loss: 1067107584.0000\n",
            "Epoch 11202/20000\n",
            "1/1 - 0s - 128ms/step - loss: 899559488.0000 - val_loss: 1066848320.0000\n",
            "Epoch 11203/20000\n",
            "1/1 - 0s - 75ms/step - loss: 898922816.0000 - val_loss: 1066238912.0000\n",
            "Epoch 11204/20000\n",
            "1/1 - 0s - 129ms/step - loss: 898280960.0000 - val_loss: 1064865152.0000\n",
            "Epoch 11205/20000\n",
            "1/1 - 0s - 64ms/step - loss: 897641792.0000 - val_loss: 1065236864.0000\n",
            "Epoch 11206/20000\n",
            "1/1 - 0s - 66ms/step - loss: 896997568.0000 - val_loss: 1064797120.0000\n",
            "Epoch 11207/20000\n",
            "1/1 - 0s - 63ms/step - loss: 896367296.0000 - val_loss: 1063424448.0000\n",
            "Epoch 11208/20000\n",
            "1/1 - 0s - 61ms/step - loss: 895746432.0000 - val_loss: 1063518912.0000\n",
            "Epoch 11209/20000\n",
            "1/1 - 0s - 60ms/step - loss: 895122368.0000 - val_loss: 1062783232.0000\n",
            "Epoch 11210/20000\n",
            "1/1 - 0s - 60ms/step - loss: 894523008.0000 - val_loss: 1062424064.0000\n",
            "Epoch 11211/20000\n",
            "1/1 - 0s - 146ms/step - loss: 893929152.0000 - val_loss: 1062518016.0000\n",
            "Epoch 11212/20000\n",
            "1/1 - 0s - 131ms/step - loss: 893351296.0000 - val_loss: 1060647936.0000\n",
            "Epoch 11213/20000\n",
            "1/1 - 0s - 64ms/step - loss: 892754304.0000 - val_loss: 1060653888.0000\n",
            "Epoch 11214/20000\n",
            "1/1 - 0s - 61ms/step - loss: 892170176.0000 - val_loss: 1061018880.0000\n",
            "Epoch 11215/20000\n",
            "1/1 - 0s - 75ms/step - loss: 891613248.0000 - val_loss: 1058751360.0000\n",
            "Epoch 11216/20000\n",
            "1/1 - 0s - 127ms/step - loss: 891073344.0000 - val_loss: 1059273408.0000\n",
            "Epoch 11217/20000\n",
            "1/1 - 0s - 62ms/step - loss: 890505024.0000 - val_loss: 1060022720.0000\n",
            "Epoch 11218/20000\n",
            "1/1 - 0s - 67ms/step - loss: 889976256.0000 - val_loss: 1057832448.0000\n",
            "Epoch 11219/20000\n",
            "1/1 - 0s - 60ms/step - loss: 889411392.0000 - val_loss: 1056936192.0000\n",
            "Epoch 11220/20000\n",
            "1/1 - 0s - 61ms/step - loss: 888900224.0000 - val_loss: 1058257792.0000\n",
            "Epoch 11221/20000\n",
            "1/1 - 0s - 63ms/step - loss: 888380288.0000 - val_loss: 1058253760.0000\n",
            "Epoch 11222/20000\n",
            "1/1 - 0s - 60ms/step - loss: 887857664.0000 - val_loss: 1056665664.0000\n",
            "Epoch 11223/20000\n",
            "1/1 - 0s - 65ms/step - loss: 887316096.0000 - val_loss: 1055615424.0000\n",
            "Epoch 11224/20000\n",
            "1/1 - 0s - 73ms/step - loss: 886770304.0000 - val_loss: 1056180288.0000\n",
            "Epoch 11225/20000\n",
            "1/1 - 0s - 75ms/step - loss: 886229952.0000 - val_loss: 1055012608.0000\n",
            "Epoch 11226/20000\n",
            "1/1 - 0s - 130ms/step - loss: 885702528.0000 - val_loss: 1053425088.0000\n",
            "Epoch 11227/20000\n",
            "1/1 - 0s - 65ms/step - loss: 885188416.0000 - val_loss: 1054338624.0000\n",
            "Epoch 11228/20000\n",
            "1/1 - 0s - 138ms/step - loss: 884669376.0000 - val_loss: 1054012864.0000\n",
            "Epoch 11229/20000\n",
            "1/1 - 0s - 63ms/step - loss: 884162368.0000 - val_loss: 1052266624.0000\n",
            "Epoch 11230/20000\n",
            "1/1 - 0s - 65ms/step - loss: 883658688.0000 - val_loss: 1052354944.0000\n",
            "Epoch 11231/20000\n",
            "1/1 - 0s - 67ms/step - loss: 883137984.0000 - val_loss: 1052361408.0000\n",
            "Epoch 11232/20000\n",
            "1/1 - 0s - 134ms/step - loss: 882626048.0000 - val_loss: 1051760576.0000\n",
            "Epoch 11233/20000\n",
            "1/1 - 0s - 137ms/step - loss: 882116672.0000 - val_loss: 1049545664.0000\n",
            "Epoch 11234/20000\n",
            "1/1 - 0s - 137ms/step - loss: 881636160.0000 - val_loss: 1049832640.0000\n",
            "Epoch 11235/20000\n",
            "1/1 - 0s - 59ms/step - loss: 881129088.0000 - val_loss: 1051143936.0000\n",
            "Epoch 11236/20000\n",
            "1/1 - 0s - 67ms/step - loss: 880668352.0000 - val_loss: 1048948992.0000\n",
            "Epoch 11237/20000\n",
            "1/1 - 0s - 62ms/step - loss: 880165568.0000 - val_loss: 1048635264.0000\n",
            "Epoch 11238/20000\n",
            "1/1 - 0s - 63ms/step - loss: 879669568.0000 - val_loss: 1048820608.0000\n",
            "Epoch 11239/20000\n",
            "1/1 - 0s - 137ms/step - loss: 879168256.0000 - val_loss: 1047494464.0000\n",
            "Epoch 11240/20000\n",
            "1/1 - 0s - 63ms/step - loss: 878651008.0000 - val_loss: 1047054208.0000\n",
            "Epoch 11241/20000\n",
            "1/1 - 0s - 66ms/step - loss: 878141312.0000 - val_loss: 1047775808.0000\n",
            "Epoch 11242/20000\n",
            "1/1 - 0s - 62ms/step - loss: 877644224.0000 - val_loss: 1045554880.0000\n",
            "Epoch 11243/20000\n",
            "1/1 - 0s - 60ms/step - loss: 877122240.0000 - val_loss: 1045510272.0000\n",
            "Epoch 11244/20000\n",
            "1/1 - 0s - 65ms/step - loss: 876604608.0000 - val_loss: 1047111104.0000\n",
            "Epoch 11245/20000\n",
            "1/1 - 0s - 144ms/step - loss: 876142720.0000 - val_loss: 1044766720.0000\n",
            "Epoch 11246/20000\n",
            "1/1 - 0s - 136ms/step - loss: 875595840.0000 - val_loss: 1042951488.0000\n",
            "Epoch 11247/20000\n",
            "1/1 - 0s - 64ms/step - loss: 875112064.0000 - val_loss: 1044633536.0000\n",
            "Epoch 11248/20000\n",
            "1/1 - 0s - 135ms/step - loss: 874613568.0000 - val_loss: 1043544064.0000\n",
            "Epoch 11249/20000\n",
            "1/1 - 0s - 69ms/step - loss: 874124736.0000 - val_loss: 1042728576.0000\n",
            "Epoch 11250/20000\n",
            "1/1 - 0s - 70ms/step - loss: 873653504.0000 - val_loss: 1042799296.0000\n",
            "Epoch 11251/20000\n",
            "1/1 - 0s - 60ms/step - loss: 873176000.0000 - val_loss: 1041447616.0000\n",
            "Epoch 11252/20000\n",
            "1/1 - 0s - 60ms/step - loss: 872704768.0000 - val_loss: 1042166400.0000\n",
            "Epoch 11253/20000\n",
            "1/1 - 0s - 59ms/step - loss: 872226432.0000 - val_loss: 1041347072.0000\n",
            "Epoch 11254/20000\n",
            "1/1 - 0s - 140ms/step - loss: 871751296.0000 - val_loss: 1040091968.0000\n",
            "Epoch 11255/20000\n",
            "1/1 - 0s - 62ms/step - loss: 871311936.0000 - val_loss: 1041104320.0000\n",
            "Epoch 11256/20000\n",
            "1/1 - 0s - 64ms/step - loss: 870866048.0000 - val_loss: 1040238400.0000\n",
            "Epoch 11257/20000\n",
            "1/1 - 0s - 72ms/step - loss: 870420352.0000 - val_loss: 1039477504.0000\n",
            "Epoch 11258/20000\n",
            "1/1 - 0s - 131ms/step - loss: 869972160.0000 - val_loss: 1039975744.0000\n",
            "Epoch 11259/20000\n",
            "1/1 - 0s - 63ms/step - loss: 869525760.0000 - val_loss: 1039168384.0000\n",
            "Epoch 11260/20000\n",
            "1/1 - 0s - 63ms/step - loss: 869079424.0000 - val_loss: 1038719552.0000\n",
            "Epoch 11261/20000\n",
            "1/1 - 0s - 69ms/step - loss: 868635136.0000 - val_loss: 1038320000.0000\n",
            "Epoch 11262/20000\n",
            "1/1 - 0s - 80ms/step - loss: 868200512.0000 - val_loss: 1037288384.0000\n",
            "Epoch 11263/20000\n",
            "1/1 - 0s - 65ms/step - loss: 867762944.0000 - val_loss: 1037250304.0000\n",
            "Epoch 11264/20000\n",
            "1/1 - 0s - 64ms/step - loss: 867304960.0000 - val_loss: 1036750976.0000\n",
            "Epoch 11265/20000\n",
            "1/1 - 0s - 66ms/step - loss: 866880128.0000 - val_loss: 1036338112.0000\n",
            "Epoch 11266/20000\n",
            "1/1 - 0s - 62ms/step - loss: 866452928.0000 - val_loss: 1036226496.0000\n",
            "Epoch 11267/20000\n",
            "1/1 - 0s - 62ms/step - loss: 866016896.0000 - val_loss: 1035866048.0000\n",
            "Epoch 11268/20000\n",
            "1/1 - 0s - 61ms/step - loss: 865582016.0000 - val_loss: 1034614976.0000\n",
            "Epoch 11269/20000\n",
            "1/1 - 0s - 59ms/step - loss: 865135616.0000 - val_loss: 1033976000.0000\n",
            "Epoch 11270/20000\n",
            "1/1 - 0s - 77ms/step - loss: 864696064.0000 - val_loss: 1034695488.0000\n",
            "Epoch 11271/20000\n",
            "1/1 - 0s - 74ms/step - loss: 864251840.0000 - val_loss: 1034061056.0000\n",
            "Epoch 11272/20000\n",
            "1/1 - 0s - 66ms/step - loss: 863801216.0000 - val_loss: 1032476672.0000\n",
            "Epoch 11273/20000\n",
            "1/1 - 0s - 139ms/step - loss: 863372928.0000 - val_loss: 1034000768.0000\n",
            "Epoch 11274/20000\n",
            "1/1 - 0s - 64ms/step - loss: 862941440.0000 - val_loss: 1032363328.0000\n",
            "Epoch 11275/20000\n",
            "1/1 - 0s - 69ms/step - loss: 862466304.0000 - val_loss: 1030630016.0000\n",
            "Epoch 11276/20000\n",
            "1/1 - 0s - 169ms/step - loss: 862004352.0000 - val_loss: 1031185856.0000\n",
            "Epoch 11277/20000\n",
            "1/1 - 0s - 97ms/step - loss: 861509440.0000 - val_loss: 1031363520.0000\n",
            "Epoch 11278/20000\n",
            "1/1 - 0s - 138ms/step - loss: 861055808.0000 - val_loss: 1030544832.0000\n",
            "Epoch 11279/20000\n",
            "1/1 - 0s - 163ms/step - loss: 860589440.0000 - val_loss: 1029429440.0000\n",
            "Epoch 11280/20000\n",
            "1/1 - 0s - 114ms/step - loss: 860128512.0000 - val_loss: 1029333504.0000\n",
            "Epoch 11281/20000\n",
            "1/1 - 0s - 138ms/step - loss: 859648320.0000 - val_loss: 1028550400.0000\n",
            "Epoch 11282/20000\n",
            "1/1 - 0s - 98ms/step - loss: 859186240.0000 - val_loss: 1028298944.0000\n",
            "Epoch 11283/20000\n",
            "1/1 - 0s - 89ms/step - loss: 858737792.0000 - val_loss: 1027293952.0000\n",
            "Epoch 11284/20000\n",
            "1/1 - 0s - 144ms/step - loss: 858279872.0000 - val_loss: 1026381440.0000\n",
            "Epoch 11285/20000\n",
            "1/1 - 0s - 100ms/step - loss: 857822080.0000 - val_loss: 1026081856.0000\n",
            "Epoch 11286/20000\n",
            "1/1 - 0s - 135ms/step - loss: 857354944.0000 - val_loss: 1025575360.0000\n",
            "Epoch 11287/20000\n",
            "1/1 - 0s - 141ms/step - loss: 856900352.0000 - val_loss: 1024368960.0000\n",
            "Epoch 11288/20000\n",
            "1/1 - 0s - 137ms/step - loss: 856448128.0000 - val_loss: 1024902272.0000\n",
            "Epoch 11289/20000\n",
            "1/1 - 0s - 83ms/step - loss: 855984768.0000 - val_loss: 1025050688.0000\n",
            "Epoch 11290/20000\n",
            "1/1 - 0s - 87ms/step - loss: 855534336.0000 - val_loss: 1023793280.0000\n",
            "Epoch 11291/20000\n",
            "1/1 - 0s - 88ms/step - loss: 855060544.0000 - val_loss: 1022638848.0000\n",
            "Epoch 11292/20000\n",
            "1/1 - 0s - 171ms/step - loss: 854601984.0000 - val_loss: 1023080064.0000\n",
            "Epoch 11293/20000\n",
            "1/1 - 0s - 106ms/step - loss: 854154560.0000 - val_loss: 1021539584.0000\n",
            "Epoch 11294/20000\n",
            "1/1 - 0s - 141ms/step - loss: 853700480.0000 - val_loss: 1021702016.0000\n",
            "Epoch 11295/20000\n",
            "1/1 - 0s - 115ms/step - loss: 853235712.0000 - val_loss: 1021101568.0000\n",
            "Epoch 11296/20000\n",
            "1/1 - 0s - 124ms/step - loss: 852774080.0000 - val_loss: 1020367744.0000\n",
            "Epoch 11297/20000\n",
            "1/1 - 0s - 99ms/step - loss: 852312832.0000 - val_loss: 1020135104.0000\n",
            "Epoch 11298/20000\n",
            "1/1 - 0s - 127ms/step - loss: 851838656.0000 - val_loss: 1019117248.0000\n",
            "Epoch 11299/20000\n",
            "1/1 - 0s - 149ms/step - loss: 851358784.0000 - val_loss: 1018344512.0000\n",
            "Epoch 11300/20000\n",
            "1/1 - 0s - 103ms/step - loss: 850885568.0000 - val_loss: 1019188864.0000\n",
            "Epoch 11301/20000\n",
            "1/1 - 0s - 147ms/step - loss: 850434624.0000 - val_loss: 1017139456.0000\n",
            "Epoch 11302/20000\n",
            "1/1 - 0s - 80ms/step - loss: 849963008.0000 - val_loss: 1016555712.0000\n",
            "Epoch 11303/20000\n",
            "1/1 - 0s - 140ms/step - loss: 849494144.0000 - val_loss: 1017496512.0000\n",
            "Epoch 11304/20000\n",
            "1/1 - 0s - 141ms/step - loss: 849016768.0000 - val_loss: 1016158912.0000\n",
            "Epoch 11305/20000\n",
            "1/1 - 0s - 82ms/step - loss: 848524864.0000 - val_loss: 1014917312.0000\n",
            "Epoch 11306/20000\n",
            "1/1 - 0s - 127ms/step - loss: 848051648.0000 - val_loss: 1014574272.0000\n",
            "Epoch 11307/20000\n",
            "1/1 - 0s - 84ms/step - loss: 847569664.0000 - val_loss: 1014024256.0000\n",
            "Epoch 11308/20000\n",
            "1/1 - 0s - 140ms/step - loss: 847096384.0000 - val_loss: 1013164224.0000\n",
            "Epoch 11309/20000\n",
            "1/1 - 0s - 115ms/step - loss: 846621888.0000 - val_loss: 1012578624.0000\n",
            "Epoch 11310/20000\n",
            "1/1 - 0s - 140ms/step - loss: 846152640.0000 - val_loss: 1012886272.0000\n",
            "Epoch 11311/20000\n",
            "1/1 - 0s - 66ms/step - loss: 845677504.0000 - val_loss: 1012456576.0000\n",
            "Epoch 11312/20000\n",
            "1/1 - 0s - 134ms/step - loss: 845227072.0000 - val_loss: 1011150592.0000\n",
            "Epoch 11313/20000\n",
            "1/1 - 0s - 63ms/step - loss: 844760960.0000 - val_loss: 1010327040.0000\n",
            "Epoch 11314/20000\n",
            "1/1 - 0s - 140ms/step - loss: 844295744.0000 - val_loss: 1008925632.0000\n",
            "Epoch 11315/20000\n",
            "1/1 - 0s - 63ms/step - loss: 843834432.0000 - val_loss: 1008544128.0000\n",
            "Epoch 11316/20000\n",
            "1/1 - 0s - 140ms/step - loss: 843350208.0000 - val_loss: 1008874112.0000\n",
            "Epoch 11317/20000\n",
            "1/1 - 0s - 139ms/step - loss: 842872000.0000 - val_loss: 1007753792.0000\n",
            "Epoch 11318/20000\n",
            "1/1 - 0s - 72ms/step - loss: 842408064.0000 - val_loss: 1008588800.0000\n",
            "Epoch 11319/20000\n",
            "1/1 - 0s - 59ms/step - loss: 841936640.0000 - val_loss: 1007726528.0000\n",
            "Epoch 11320/20000\n",
            "1/1 - 0s - 61ms/step - loss: 841464384.0000 - val_loss: 1006466048.0000\n",
            "Epoch 11321/20000\n",
            "1/1 - 0s - 59ms/step - loss: 840993600.0000 - val_loss: 1006543744.0000\n",
            "Epoch 11322/20000\n",
            "1/1 - 0s - 78ms/step - loss: 840537344.0000 - val_loss: 1005194624.0000\n",
            "Epoch 11323/20000\n",
            "1/1 - 0s - 129ms/step - loss: 840067264.0000 - val_loss: 1004459456.0000\n",
            "Epoch 11324/20000\n",
            "1/1 - 0s - 60ms/step - loss: 839592000.0000 - val_loss: 1003896960.0000\n",
            "Epoch 11325/20000\n",
            "1/1 - 0s - 65ms/step - loss: 839108992.0000 - val_loss: 1003146048.0000\n",
            "Epoch 11326/20000\n",
            "1/1 - 0s - 61ms/step - loss: 838624704.0000 - val_loss: 1003475264.0000\n",
            "Epoch 11327/20000\n",
            "1/1 - 0s - 63ms/step - loss: 838138944.0000 - val_loss: 1001965184.0000\n",
            "Epoch 11328/20000\n",
            "1/1 - 0s - 66ms/step - loss: 837651840.0000 - val_loss: 1002326464.0000\n",
            "Epoch 11329/20000\n",
            "1/1 - 0s - 63ms/step - loss: 837169024.0000 - val_loss: 1001494528.0000\n",
            "Epoch 11330/20000\n",
            "1/1 - 0s - 62ms/step - loss: 836673024.0000 - val_loss: 999606080.0000\n",
            "Epoch 11331/20000\n",
            "1/1 - 0s - 60ms/step - loss: 836182208.0000 - val_loss: 999091712.0000\n",
            "Epoch 11332/20000\n",
            "1/1 - 0s - 140ms/step - loss: 835684416.0000 - val_loss: 999343168.0000\n",
            "Epoch 11333/20000\n",
            "1/1 - 0s - 67ms/step - loss: 835205312.0000 - val_loss: 997165696.0000\n",
            "Epoch 11334/20000\n",
            "1/1 - 0s - 140ms/step - loss: 834736576.0000 - val_loss: 998034624.0000\n",
            "Epoch 11335/20000\n",
            "1/1 - 0s - 136ms/step - loss: 834249792.0000 - val_loss: 996464960.0000\n",
            "Epoch 11336/20000\n",
            "1/1 - 0s - 61ms/step - loss: 833745600.0000 - val_loss: 995155968.0000\n",
            "Epoch 11337/20000\n",
            "1/1 - 0s - 62ms/step - loss: 833273984.0000 - val_loss: 996356224.0000\n",
            "Epoch 11338/20000\n",
            "1/1 - 0s - 60ms/step - loss: 832789056.0000 - val_loss: 995537024.0000\n",
            "Epoch 11339/20000\n",
            "1/1 - 0s - 59ms/step - loss: 832287360.0000 - val_loss: 993800448.0000\n",
            "Epoch 11340/20000\n",
            "1/1 - 0s - 65ms/step - loss: 831825344.0000 - val_loss: 995266816.0000\n",
            "Epoch 11341/20000\n",
            "1/1 - 0s - 61ms/step - loss: 831316096.0000 - val_loss: 994237376.0000\n",
            "Epoch 11342/20000\n",
            "1/1 - 0s - 140ms/step - loss: 830818496.0000 - val_loss: 992148992.0000\n",
            "Epoch 11343/20000\n",
            "1/1 - 0s - 72ms/step - loss: 830354432.0000 - val_loss: 992448000.0000\n",
            "Epoch 11344/20000\n",
            "1/1 - 0s - 136ms/step - loss: 829836672.0000 - val_loss: 992349312.0000\n",
            "Epoch 11345/20000\n",
            "1/1 - 0s - 91ms/step - loss: 829364160.0000 - val_loss: 989869824.0000\n",
            "Epoch 11346/20000\n",
            "1/1 - 0s - 116ms/step - loss: 828868608.0000 - val_loss: 989235648.0000\n",
            "Epoch 11347/20000\n",
            "1/1 - 0s - 61ms/step - loss: 828382720.0000 - val_loss: 990149376.0000\n",
            "Epoch 11348/20000\n",
            "1/1 - 0s - 61ms/step - loss: 827887296.0000 - val_loss: 989390912.0000\n",
            "Epoch 11349/20000\n",
            "1/1 - 0s - 69ms/step - loss: 827396416.0000 - val_loss: 988307584.0000\n",
            "Epoch 11350/20000\n",
            "1/1 - 0s - 60ms/step - loss: 826918208.0000 - val_loss: 988679872.0000\n",
            "Epoch 11351/20000\n",
            "1/1 - 0s - 62ms/step - loss: 826428544.0000 - val_loss: 987088320.0000\n",
            "Epoch 11352/20000\n",
            "1/1 - 0s - 59ms/step - loss: 825945088.0000 - val_loss: 986165952.0000\n",
            "Epoch 11353/20000\n",
            "1/1 - 0s - 60ms/step - loss: 825467840.0000 - val_loss: 986344832.0000\n",
            "Epoch 11354/20000\n",
            "1/1 - 0s - 61ms/step - loss: 824983360.0000 - val_loss: 985343040.0000\n",
            "Epoch 11355/20000\n",
            "1/1 - 0s - 57ms/step - loss: 824493184.0000 - val_loss: 983991808.0000\n",
            "Epoch 11356/20000\n",
            "1/1 - 0s - 71ms/step - loss: 824014016.0000 - val_loss: 984079424.0000\n",
            "Epoch 11357/20000\n",
            "1/1 - 0s - 141ms/step - loss: 823539648.0000 - val_loss: 983596736.0000\n",
            "Epoch 11358/20000\n",
            "1/1 - 0s - 79ms/step - loss: 823066624.0000 - val_loss: 981724160.0000\n",
            "Epoch 11359/20000\n",
            "1/1 - 0s - 72ms/step - loss: 822608256.0000 - val_loss: 982666432.0000\n",
            "Epoch 11360/20000\n",
            "1/1 - 0s - 61ms/step - loss: 822116736.0000 - val_loss: 982074880.0000\n",
            "Epoch 11361/20000\n",
            "1/1 - 0s - 61ms/step - loss: 821644096.0000 - val_loss: 981144384.0000\n",
            "Epoch 11362/20000\n",
            "1/1 - 0s - 67ms/step - loss: 821185472.0000 - val_loss: 982623488.0000\n",
            "Epoch 11363/20000\n",
            "1/1 - 0s - 61ms/step - loss: 820748416.0000 - val_loss: 979748736.0000\n",
            "Epoch 11364/20000\n",
            "1/1 - 0s - 61ms/step - loss: 820238656.0000 - val_loss: 979526784.0000\n",
            "Epoch 11365/20000\n",
            "1/1 - 0s - 70ms/step - loss: 819756864.0000 - val_loss: 978682624.0000\n",
            "Epoch 11366/20000\n",
            "1/1 - 0s - 61ms/step - loss: 819294912.0000 - val_loss: 977999488.0000\n",
            "Epoch 11367/20000\n",
            "1/1 - 0s - 135ms/step - loss: 818831680.0000 - val_loss: 978132032.0000\n",
            "Epoch 11368/20000\n",
            "1/1 - 0s - 84ms/step - loss: 818363648.0000 - val_loss: 976954816.0000\n",
            "Epoch 11369/20000\n",
            "1/1 - 0s - 61ms/step - loss: 817897280.0000 - val_loss: 977526272.0000\n",
            "Epoch 11370/20000\n",
            "1/1 - 0s - 73ms/step - loss: 817425600.0000 - val_loss: 976426048.0000\n",
            "Epoch 11371/20000\n",
            "1/1 - 0s - 72ms/step - loss: 816939392.0000 - val_loss: 975214144.0000\n",
            "Epoch 11372/20000\n",
            "1/1 - 0s - 69ms/step - loss: 816469632.0000 - val_loss: 976227328.0000\n",
            "Epoch 11373/20000\n",
            "1/1 - 0s - 61ms/step - loss: 816000576.0000 - val_loss: 974838144.0000\n",
            "Epoch 11374/20000\n",
            "1/1 - 0s - 62ms/step - loss: 815540032.0000 - val_loss: 974912384.0000\n",
            "Epoch 11375/20000\n",
            "1/1 - 0s - 67ms/step - loss: 815053504.0000 - val_loss: 974437888.0000\n",
            "Epoch 11376/20000\n",
            "1/1 - 0s - 137ms/step - loss: 814569408.0000 - val_loss: 973642304.0000\n",
            "Epoch 11377/20000\n",
            "1/1 - 0s - 135ms/step - loss: 814097344.0000 - val_loss: 973024640.0000\n",
            "Epoch 11378/20000\n",
            "1/1 - 0s - 140ms/step - loss: 813631552.0000 - val_loss: 972420480.0000\n",
            "Epoch 11379/20000\n",
            "1/1 - 0s - 73ms/step - loss: 813163264.0000 - val_loss: 971666816.0000\n",
            "Epoch 11380/20000\n",
            "1/1 - 0s - 67ms/step - loss: 812689024.0000 - val_loss: 972208896.0000\n",
            "Epoch 11381/20000\n",
            "1/1 - 0s - 75ms/step - loss: 812231424.0000 - val_loss: 969961984.0000\n",
            "Epoch 11382/20000\n",
            "1/1 - 0s - 69ms/step - loss: 811763136.0000 - val_loss: 970641280.0000\n",
            "Epoch 11383/20000\n",
            "1/1 - 0s - 64ms/step - loss: 811276672.0000 - val_loss: 970260352.0000\n",
            "Epoch 11384/20000\n",
            "1/1 - 0s - 59ms/step - loss: 810795200.0000 - val_loss: 968507200.0000\n",
            "Epoch 11385/20000\n",
            "1/1 - 0s - 61ms/step - loss: 810336640.0000 - val_loss: 969811840.0000\n",
            "Epoch 11386/20000\n",
            "1/1 - 0s - 63ms/step - loss: 809877888.0000 - val_loss: 967809408.0000\n",
            "Epoch 11387/20000\n",
            "1/1 - 0s - 66ms/step - loss: 809395136.0000 - val_loss: 968079552.0000\n",
            "Epoch 11388/20000\n",
            "1/1 - 0s - 60ms/step - loss: 808914624.0000 - val_loss: 966609920.0000\n",
            "Epoch 11389/20000\n",
            "1/1 - 0s - 141ms/step - loss: 808458368.0000 - val_loss: 967611072.0000\n",
            "Epoch 11390/20000\n",
            "1/1 - 0s - 65ms/step - loss: 807992640.0000 - val_loss: 966043712.0000\n",
            "Epoch 11391/20000\n",
            "1/1 - 0s - 62ms/step - loss: 807502144.0000 - val_loss: 966039744.0000\n",
            "Epoch 11392/20000\n",
            "1/1 - 0s - 61ms/step - loss: 807045696.0000 - val_loss: 965764224.0000\n",
            "Epoch 11393/20000\n",
            "1/1 - 0s - 74ms/step - loss: 806581120.0000 - val_loss: 965145664.0000\n",
            "Epoch 11394/20000\n",
            "1/1 - 0s - 70ms/step - loss: 806124480.0000 - val_loss: 964450624.0000\n",
            "Epoch 11395/20000\n",
            "1/1 - 0s - 139ms/step - loss: 805683712.0000 - val_loss: 965097664.0000\n",
            "Epoch 11396/20000\n",
            "1/1 - 0s - 60ms/step - loss: 805228160.0000 - val_loss: 963657920.0000\n",
            "Epoch 11397/20000\n",
            "1/1 - 0s - 64ms/step - loss: 804744320.0000 - val_loss: 962381312.0000\n",
            "Epoch 11398/20000\n",
            "1/1 - 0s - 66ms/step - loss: 804285248.0000 - val_loss: 963608192.0000\n",
            "Epoch 11399/20000\n",
            "1/1 - 0s - 63ms/step - loss: 803854336.0000 - val_loss: 962145408.0000\n",
            "Epoch 11400/20000\n",
            "1/1 - 0s - 135ms/step - loss: 803382784.0000 - val_loss: 961376320.0000\n",
            "Epoch 11401/20000\n",
            "1/1 - 0s - 142ms/step - loss: 802951680.0000 - val_loss: 963339392.0000\n",
            "Epoch 11402/20000\n",
            "1/1 - 0s - 139ms/step - loss: 802558272.0000 - val_loss: 960172736.0000\n",
            "Epoch 11403/20000\n",
            "1/1 - 0s - 61ms/step - loss: 802060096.0000 - val_loss: 959274880.0000\n",
            "Epoch 11404/20000\n",
            "1/1 - 0s - 144ms/step - loss: 801610752.0000 - val_loss: 960585856.0000\n",
            "Epoch 11405/20000\n",
            "1/1 - 0s - 65ms/step - loss: 801198400.0000 - val_loss: 958884416.0000\n",
            "Epoch 11406/20000\n",
            "1/1 - 0s - 61ms/step - loss: 800744000.0000 - val_loss: 957456960.0000\n",
            "Epoch 11407/20000\n",
            "1/1 - 0s - 61ms/step - loss: 800341696.0000 - val_loss: 958835648.0000\n",
            "Epoch 11408/20000\n",
            "1/1 - 0s - 65ms/step - loss: 799878144.0000 - val_loss: 958203008.0000\n",
            "Epoch 11409/20000\n",
            "1/1 - 0s - 64ms/step - loss: 799431360.0000 - val_loss: 957226048.0000\n",
            "Epoch 11410/20000\n",
            "1/1 - 0s - 61ms/step - loss: 798997440.0000 - val_loss: 957848064.0000\n",
            "Epoch 11411/20000\n",
            "1/1 - 0s - 141ms/step - loss: 798573632.0000 - val_loss: 956488640.0000\n",
            "Epoch 11412/20000\n",
            "1/1 - 0s - 59ms/step - loss: 798129152.0000 - val_loss: 955081600.0000\n",
            "Epoch 11413/20000\n",
            "1/1 - 0s - 64ms/step - loss: 797699584.0000 - val_loss: 955078464.0000\n",
            "Epoch 11414/20000\n",
            "1/1 - 0s - 61ms/step - loss: 797254528.0000 - val_loss: 955080128.0000\n",
            "Epoch 11415/20000\n",
            "1/1 - 0s - 64ms/step - loss: 796829504.0000 - val_loss: 953870016.0000\n",
            "Epoch 11416/20000\n",
            "1/1 - 0s - 60ms/step - loss: 796428672.0000 - val_loss: 955474880.0000\n",
            "Epoch 11417/20000\n",
            "1/1 - 0s - 83ms/step - loss: 795998400.0000 - val_loss: 953481152.0000\n",
            "Epoch 11418/20000\n",
            "1/1 - 0s - 132ms/step - loss: 795557696.0000 - val_loss: 953746112.0000\n",
            "Epoch 11419/20000\n",
            "1/1 - 0s - 63ms/step - loss: 795107904.0000 - val_loss: 952307776.0000\n",
            "Epoch 11420/20000\n",
            "1/1 - 0s - 61ms/step - loss: 794677184.0000 - val_loss: 951641600.0000\n",
            "Epoch 11421/20000\n",
            "1/1 - 0s - 62ms/step - loss: 794254656.0000 - val_loss: 952659456.0000\n",
            "Epoch 11422/20000\n",
            "1/1 - 0s - 138ms/step - loss: 793846272.0000 - val_loss: 951200576.0000\n",
            "Epoch 11423/20000\n",
            "1/1 - 0s - 63ms/step - loss: 793395904.0000 - val_loss: 949908672.0000\n",
            "Epoch 11424/20000\n",
            "1/1 - 0s - 136ms/step - loss: 792966336.0000 - val_loss: 950848640.0000\n",
            "Epoch 11425/20000\n",
            "1/1 - 0s - 168ms/step - loss: 792530496.0000 - val_loss: 949475904.0000\n",
            "Epoch 11426/20000\n",
            "1/1 - 0s - 136ms/step - loss: 792094656.0000 - val_loss: 950018368.0000\n",
            "Epoch 11427/20000\n",
            "1/1 - 0s - 117ms/step - loss: 791668544.0000 - val_loss: 948886080.0000\n",
            "Epoch 11428/20000\n",
            "1/1 - 0s - 95ms/step - loss: 791229120.0000 - val_loss: 947677568.0000\n",
            "Epoch 11429/20000\n",
            "1/1 - 0s - 139ms/step - loss: 790798656.0000 - val_loss: 948098880.0000\n",
            "Epoch 11430/20000\n",
            "1/1 - 0s - 107ms/step - loss: 790339712.0000 - val_loss: 946978496.0000\n",
            "Epoch 11431/20000\n",
            "1/1 - 0s - 134ms/step - loss: 789900480.0000 - val_loss: 946028544.0000\n",
            "Epoch 11432/20000\n",
            "1/1 - 0s - 141ms/step - loss: 789463744.0000 - val_loss: 946878208.0000\n",
            "Epoch 11433/20000\n",
            "1/1 - 0s - 118ms/step - loss: 789030144.0000 - val_loss: 945659136.0000\n",
            "Epoch 11434/20000\n",
            "1/1 - 0s - 109ms/step - loss: 788586944.0000 - val_loss: 945583488.0000\n",
            "Epoch 11435/20000\n",
            "1/1 - 0s - 99ms/step - loss: 788142784.0000 - val_loss: 944169920.0000\n",
            "Epoch 11436/20000\n",
            "1/1 - 0s - 86ms/step - loss: 787701568.0000 - val_loss: 944372416.0000\n",
            "Epoch 11437/20000\n",
            "1/1 - 0s - 85ms/step - loss: 787264384.0000 - val_loss: 943508800.0000\n",
            "Epoch 11438/20000\n",
            "1/1 - 0s - 94ms/step - loss: 786816896.0000 - val_loss: 942597056.0000\n",
            "Epoch 11439/20000\n",
            "1/1 - 0s - 145ms/step - loss: 786382464.0000 - val_loss: 943205504.0000\n",
            "Epoch 11440/20000\n",
            "1/1 - 0s - 90ms/step - loss: 785944896.0000 - val_loss: 942369152.0000\n",
            "Epoch 11441/20000\n",
            "1/1 - 0s - 138ms/step - loss: 785499648.0000 - val_loss: 941234176.0000\n",
            "Epoch 11442/20000\n",
            "1/1 - 0s - 90ms/step - loss: 785073216.0000 - val_loss: 941186304.0000\n",
            "Epoch 11443/20000\n",
            "1/1 - 0s - 161ms/step - loss: 784627328.0000 - val_loss: 940954240.0000\n",
            "Epoch 11444/20000\n",
            "1/1 - 0s - 106ms/step - loss: 784189824.0000 - val_loss: 940858688.0000\n",
            "Epoch 11445/20000\n",
            "1/1 - 0s - 132ms/step - loss: 783750656.0000 - val_loss: 939079424.0000\n",
            "Epoch 11446/20000\n",
            "1/1 - 0s - 109ms/step - loss: 783310720.0000 - val_loss: 939000320.0000\n",
            "Epoch 11447/20000\n",
            "1/1 - 0s - 155ms/step - loss: 782873600.0000 - val_loss: 938590400.0000\n",
            "Epoch 11448/20000\n",
            "1/1 - 0s - 84ms/step - loss: 782432640.0000 - val_loss: 937791808.0000\n",
            "Epoch 11449/20000\n",
            "1/1 - 0s - 148ms/step - loss: 781987328.0000 - val_loss: 937438528.0000\n",
            "Epoch 11450/20000\n",
            "1/1 - 0s - 128ms/step - loss: 781542656.0000 - val_loss: 937158720.0000\n",
            "Epoch 11451/20000\n",
            "1/1 - 0s - 147ms/step - loss: 781089024.0000 - val_loss: 937093952.0000\n",
            "Epoch 11452/20000\n",
            "1/1 - 0s - 134ms/step - loss: 780646016.0000 - val_loss: 935927104.0000\n",
            "Epoch 11453/20000\n",
            "1/1 - 0s - 114ms/step - loss: 780201344.0000 - val_loss: 935753088.0000\n",
            "Epoch 11454/20000\n",
            "1/1 - 0s - 139ms/step - loss: 779743360.0000 - val_loss: 934667968.0000\n",
            "Epoch 11455/20000\n",
            "1/1 - 0s - 96ms/step - loss: 779301312.0000 - val_loss: 934683520.0000\n",
            "Epoch 11456/20000\n",
            "1/1 - 0s - 119ms/step - loss: 778859968.0000 - val_loss: 933703040.0000\n",
            "Epoch 11457/20000\n",
            "1/1 - 0s - 113ms/step - loss: 778410816.0000 - val_loss: 933989696.0000\n",
            "Epoch 11458/20000\n",
            "1/1 - 0s - 93ms/step - loss: 777948096.0000 - val_loss: 933906432.0000\n",
            "Epoch 11459/20000\n",
            "1/1 - 0s - 157ms/step - loss: 777496640.0000 - val_loss: 932674304.0000\n",
            "Epoch 11460/20000\n",
            "1/1 - 0s - 63ms/step - loss: 777040768.0000 - val_loss: 933048576.0000\n",
            "Epoch 11461/20000\n",
            "1/1 - 0s - 81ms/step - loss: 776600320.0000 - val_loss: 931066304.0000\n",
            "Epoch 11462/20000\n",
            "1/1 - 0s - 70ms/step - loss: 776161152.0000 - val_loss: 931345088.0000\n",
            "Epoch 11463/20000\n",
            "1/1 - 0s - 135ms/step - loss: 775713856.0000 - val_loss: 931292416.0000\n",
            "Epoch 11464/20000\n",
            "1/1 - 0s - 65ms/step - loss: 775269376.0000 - val_loss: 930148224.0000\n",
            "Epoch 11465/20000\n",
            "1/1 - 0s - 65ms/step - loss: 774803008.0000 - val_loss: 929820160.0000\n",
            "Epoch 11466/20000\n",
            "1/1 - 0s - 63ms/step - loss: 774362176.0000 - val_loss: 930787584.0000\n",
            "Epoch 11467/20000\n",
            "1/1 - 0s - 137ms/step - loss: 773932736.0000 - val_loss: 929483968.0000\n",
            "Epoch 11468/20000\n",
            "1/1 - 0s - 64ms/step - loss: 773482112.0000 - val_loss: 928380672.0000\n",
            "Epoch 11469/20000\n",
            "1/1 - 0s - 72ms/step - loss: 773036992.0000 - val_loss: 929496704.0000\n",
            "Epoch 11470/20000\n",
            "1/1 - 0s - 130ms/step - loss: 772569920.0000 - val_loss: 927762112.0000\n",
            "Epoch 11471/20000\n",
            "1/1 - 0s - 64ms/step - loss: 772099904.0000 - val_loss: 927141632.0000\n",
            "Epoch 11472/20000\n",
            "1/1 - 0s - 61ms/step - loss: 771644224.0000 - val_loss: 927860544.0000\n",
            "Epoch 11473/20000\n",
            "1/1 - 0s - 139ms/step - loss: 771175552.0000 - val_loss: 926880000.0000\n",
            "Epoch 11474/20000\n",
            "1/1 - 0s - 61ms/step - loss: 770708480.0000 - val_loss: 926078208.0000\n",
            "Epoch 11475/20000\n",
            "1/1 - 0s - 67ms/step - loss: 770246144.0000 - val_loss: 925581888.0000\n",
            "Epoch 11476/20000\n",
            "1/1 - 0s - 63ms/step - loss: 769785152.0000 - val_loss: 925184896.0000\n",
            "Epoch 11477/20000\n",
            "1/1 - 0s - 60ms/step - loss: 769326592.0000 - val_loss: 925464320.0000\n",
            "Epoch 11478/20000\n",
            "1/1 - 0s - 62ms/step - loss: 768856384.0000 - val_loss: 925165440.0000\n",
            "Epoch 11479/20000\n",
            "1/1 - 0s - 60ms/step - loss: 768394624.0000 - val_loss: 923824448.0000\n",
            "Epoch 11480/20000\n",
            "1/1 - 0s - 60ms/step - loss: 767942912.0000 - val_loss: 924630080.0000\n",
            "Epoch 11481/20000\n",
            "1/1 - 0s - 61ms/step - loss: 767470144.0000 - val_loss: 923457344.0000\n",
            "Epoch 11482/20000\n",
            "1/1 - 0s - 145ms/step - loss: 766977920.0000 - val_loss: 921377152.0000\n",
            "Epoch 11483/20000\n",
            "1/1 - 0s - 133ms/step - loss: 766573952.0000 - val_loss: 923907264.0000\n",
            "Epoch 11484/20000\n",
            "1/1 - 0s - 63ms/step - loss: 766078272.0000 - val_loss: 923553344.0000\n",
            "Epoch 11485/20000\n",
            "1/1 - 0s - 139ms/step - loss: 765602432.0000 - val_loss: 920351040.0000\n",
            "Epoch 11486/20000\n",
            "1/1 - 0s - 66ms/step - loss: 765150656.0000 - val_loss: 921732928.0000\n",
            "Epoch 11487/20000\n",
            "1/1 - 0s - 63ms/step - loss: 764624512.0000 - val_loss: 921699968.0000\n",
            "Epoch 11488/20000\n",
            "1/1 - 0s - 62ms/step - loss: 764157248.0000 - val_loss: 919548544.0000\n",
            "Epoch 11489/20000\n",
            "1/1 - 0s - 143ms/step - loss: 763688384.0000 - val_loss: 920467072.0000\n",
            "Epoch 11490/20000\n",
            "1/1 - 0s - 62ms/step - loss: 763214528.0000 - val_loss: 919741568.0000\n",
            "Epoch 11491/20000\n",
            "1/1 - 0s - 60ms/step - loss: 762739776.0000 - val_loss: 918540992.0000\n",
            "Epoch 11492/20000\n",
            "1/1 - 0s - 146ms/step - loss: 762265920.0000 - val_loss: 919243520.0000\n",
            "Epoch 11493/20000\n",
            "1/1 - 0s - 64ms/step - loss: 761815040.0000 - val_loss: 917686144.0000\n",
            "Epoch 11494/20000\n",
            "1/1 - 0s - 61ms/step - loss: 761397056.0000 - val_loss: 918413312.0000\n",
            "Epoch 11495/20000\n",
            "1/1 - 0s - 60ms/step - loss: 760982272.0000 - val_loss: 918180736.0000\n",
            "Epoch 11496/20000\n",
            "1/1 - 0s - 71ms/step - loss: 760574656.0000 - val_loss: 915955200.0000\n",
            "Epoch 11497/20000\n",
            "1/1 - 0s - 75ms/step - loss: 760194368.0000 - val_loss: 918740480.0000\n",
            "Epoch 11498/20000\n",
            "1/1 - 0s - 63ms/step - loss: 759845824.0000 - val_loss: 915951552.0000\n",
            "Epoch 11499/20000\n",
            "1/1 - 0s - 65ms/step - loss: 759363776.0000 - val_loss: 913578752.0000\n",
            "Epoch 11500/20000\n",
            "1/1 - 0s - 65ms/step - loss: 759021632.0000 - val_loss: 915985792.0000\n",
            "Epoch 11501/20000\n",
            "1/1 - 0s - 65ms/step - loss: 758546752.0000 - val_loss: 916483968.0000\n",
            "Epoch 11502/20000\n",
            "1/1 - 0s - 65ms/step - loss: 758156928.0000 - val_loss: 912650304.0000\n",
            "Epoch 11503/20000\n",
            "1/1 - 0s - 139ms/step - loss: 757740032.0000 - val_loss: 913769728.0000\n",
            "Epoch 11504/20000\n",
            "1/1 - 0s - 70ms/step - loss: 757230464.0000 - val_loss: 915957248.0000\n",
            "Epoch 11505/20000\n",
            "1/1 - 0s - 132ms/step - loss: 756892608.0000 - val_loss: 912320896.0000\n",
            "Epoch 11506/20000\n",
            "1/1 - 0s - 64ms/step - loss: 756380160.0000 - val_loss: 910360384.0000\n",
            "Epoch 11507/20000\n",
            "1/1 - 0s - 58ms/step - loss: 756027072.0000 - val_loss: 913718144.0000\n",
            "Epoch 11508/20000\n",
            "1/1 - 0s - 59ms/step - loss: 755543808.0000 - val_loss: 913761216.0000\n",
            "Epoch 11509/20000\n",
            "1/1 - 0s - 62ms/step - loss: 755127488.0000 - val_loss: 909992320.0000\n",
            "Epoch 11510/20000\n",
            "1/1 - 0s - 79ms/step - loss: 754740544.0000 - val_loss: 911584576.0000\n",
            "Epoch 11511/20000\n",
            "1/1 - 0s - 68ms/step - loss: 754268544.0000 - val_loss: 912268544.0000\n",
            "Epoch 11512/20000\n",
            "1/1 - 0s - 128ms/step - loss: 753896192.0000 - val_loss: 909806976.0000\n",
            "Epoch 11513/20000\n",
            "1/1 - 0s - 64ms/step - loss: 753524864.0000 - val_loss: 909732224.0000\n",
            "Epoch 11514/20000\n",
            "1/1 - 0s - 138ms/step - loss: 753138432.0000 - val_loss: 910964928.0000\n",
            "Epoch 11515/20000\n",
            "1/1 - 0s - 61ms/step - loss: 752766784.0000 - val_loss: 909635712.0000\n",
            "Epoch 11516/20000\n",
            "1/1 - 0s - 76ms/step - loss: 752397120.0000 - val_loss: 907981504.0000\n",
            "Epoch 11517/20000\n",
            "1/1 - 0s - 77ms/step - loss: 752042304.0000 - val_loss: 908745856.0000\n",
            "Epoch 11518/20000\n",
            "1/1 - 0s - 129ms/step - loss: 751653696.0000 - val_loss: 908495040.0000\n",
            "Epoch 11519/20000\n",
            "1/1 - 0s - 62ms/step - loss: 751275776.0000 - val_loss: 907854720.0000\n",
            "Epoch 11520/20000\n",
            "1/1 - 0s - 151ms/step - loss: 750911168.0000 - val_loss: 908497984.0000\n",
            "Epoch 11521/20000\n",
            "1/1 - 0s - 66ms/step - loss: 750541376.0000 - val_loss: 907925312.0000\n",
            "Epoch 11522/20000\n",
            "1/1 - 0s - 63ms/step - loss: 750174400.0000 - val_loss: 905621568.0000\n",
            "Epoch 11523/20000\n",
            "1/1 - 0s - 59ms/step - loss: 749807040.0000 - val_loss: 906325184.0000\n",
            "Epoch 11524/20000\n",
            "1/1 - 0s - 64ms/step - loss: 749391552.0000 - val_loss: 907210688.0000\n",
            "Epoch 11525/20000\n",
            "1/1 - 0s - 142ms/step - loss: 749036224.0000 - val_loss: 904376576.0000\n",
            "Epoch 11526/20000\n",
            "1/1 - 0s - 60ms/step - loss: 748675456.0000 - val_loss: 905868160.0000\n",
            "Epoch 11527/20000\n",
            "1/1 - 0s - 143ms/step - loss: 748277056.0000 - val_loss: 905739328.0000\n",
            "Epoch 11528/20000\n",
            "1/1 - 0s - 64ms/step - loss: 747902400.0000 - val_loss: 903921088.0000\n",
            "Epoch 11529/20000\n",
            "1/1 - 0s - 61ms/step - loss: 747518848.0000 - val_loss: 903540608.0000\n",
            "Epoch 11530/20000\n",
            "1/1 - 0s - 140ms/step - loss: 747134464.0000 - val_loss: 904820096.0000\n",
            "Epoch 11531/20000\n",
            "1/1 - 0s - 61ms/step - loss: 746763136.0000 - val_loss: 903081728.0000\n",
            "Epoch 11532/20000\n",
            "1/1 - 0s - 73ms/step - loss: 746374464.0000 - val_loss: 902690368.0000\n",
            "Epoch 11533/20000\n",
            "1/1 - 0s - 69ms/step - loss: 746002816.0000 - val_loss: 902466688.0000\n",
            "Epoch 11534/20000\n",
            "1/1 - 0s - 133ms/step - loss: 745625024.0000 - val_loss: 901608640.0000\n",
            "Epoch 11535/20000\n",
            "1/1 - 0s - 61ms/step - loss: 745234560.0000 - val_loss: 901460288.0000\n",
            "Epoch 11536/20000\n",
            "1/1 - 0s - 63ms/step - loss: 744852032.0000 - val_loss: 901690176.0000\n",
            "Epoch 11537/20000\n",
            "1/1 - 0s - 147ms/step - loss: 744480448.0000 - val_loss: 902283456.0000\n",
            "Epoch 11538/20000\n",
            "1/1 - 0s - 131ms/step - loss: 744118656.0000 - val_loss: 900083968.0000\n",
            "Epoch 11539/20000\n",
            "1/1 - 0s - 61ms/step - loss: 743739328.0000 - val_loss: 899997312.0000\n",
            "Epoch 11540/20000\n",
            "1/1 - 0s - 140ms/step - loss: 743350592.0000 - val_loss: 900424384.0000\n",
            "Epoch 11541/20000\n",
            "1/1 - 0s - 62ms/step - loss: 742985472.0000 - val_loss: 898513856.0000\n",
            "Epoch 11542/20000\n",
            "1/1 - 0s - 60ms/step - loss: 742604992.0000 - val_loss: 898778368.0000\n",
            "Epoch 11543/20000\n",
            "1/1 - 0s - 139ms/step - loss: 742211648.0000 - val_loss: 899353536.0000\n",
            "Epoch 11544/20000\n",
            "1/1 - 0s - 139ms/step - loss: 741828928.0000 - val_loss: 898426752.0000\n",
            "Epoch 11545/20000\n",
            "1/1 - 0s - 61ms/step - loss: 741424960.0000 - val_loss: 897595584.0000\n",
            "Epoch 11546/20000\n",
            "1/1 - 0s - 59ms/step - loss: 741048704.0000 - val_loss: 898457984.0000\n",
            "Epoch 11547/20000\n",
            "1/1 - 0s - 64ms/step - loss: 740656512.0000 - val_loss: 898060736.0000\n",
            "Epoch 11548/20000\n",
            "1/1 - 0s - 75ms/step - loss: 740265024.0000 - val_loss: 896421568.0000\n",
            "Epoch 11549/20000\n",
            "1/1 - 0s - 133ms/step - loss: 739871744.0000 - val_loss: 897196736.0000\n",
            "Epoch 11550/20000\n",
            "1/1 - 0s - 59ms/step - loss: 739497664.0000 - val_loss: 895616064.0000\n",
            "Epoch 11551/20000\n",
            "1/1 - 0s - 58ms/step - loss: 739095744.0000 - val_loss: 894907584.0000\n",
            "Epoch 11552/20000\n",
            "1/1 - 0s - 62ms/step - loss: 738717120.0000 - val_loss: 896298880.0000\n",
            "Epoch 11553/20000\n",
            "1/1 - 0s - 64ms/step - loss: 738332352.0000 - val_loss: 895203520.0000\n",
            "Epoch 11554/20000\n",
            "1/1 - 0s - 62ms/step - loss: 737927552.0000 - val_loss: 894282496.0000\n",
            "Epoch 11555/20000\n",
            "1/1 - 0s - 138ms/step - loss: 737537920.0000 - val_loss: 894242368.0000\n",
            "Epoch 11556/20000\n",
            "1/1 - 0s - 140ms/step - loss: 737150848.0000 - val_loss: 893708928.0000\n",
            "Epoch 11557/20000\n",
            "1/1 - 0s - 61ms/step - loss: 736765120.0000 - val_loss: 893855424.0000\n",
            "Epoch 11558/20000\n",
            "1/1 - 0s - 63ms/step - loss: 736369984.0000 - val_loss: 893070144.0000\n",
            "Epoch 11559/20000\n",
            "1/1 - 0s - 72ms/step - loss: 735976512.0000 - val_loss: 892812800.0000\n",
            "Epoch 11560/20000\n",
            "1/1 - 0s - 75ms/step - loss: 735583040.0000 - val_loss: 892191296.0000\n",
            "Epoch 11561/20000\n",
            "1/1 - 0s - 62ms/step - loss: 735182208.0000 - val_loss: 891673920.0000\n",
            "Epoch 11562/20000\n",
            "1/1 - 0s - 64ms/step - loss: 734787584.0000 - val_loss: 890876224.0000\n",
            "Epoch 11563/20000\n",
            "1/1 - 0s - 139ms/step - loss: 734394752.0000 - val_loss: 891220800.0000\n",
            "Epoch 11564/20000\n",
            "1/1 - 0s - 61ms/step - loss: 733993472.0000 - val_loss: 891231296.0000\n",
            "Epoch 11565/20000\n",
            "1/1 - 0s - 62ms/step - loss: 733604288.0000 - val_loss: 890237248.0000\n",
            "Epoch 11566/20000\n",
            "1/1 - 0s - 79ms/step - loss: 733206720.0000 - val_loss: 889468672.0000\n",
            "Epoch 11567/20000\n",
            "1/1 - 0s - 118ms/step - loss: 732811456.0000 - val_loss: 889917376.0000\n",
            "Epoch 11568/20000\n",
            "1/1 - 0s - 62ms/step - loss: 732432384.0000 - val_loss: 888885696.0000\n",
            "Epoch 11569/20000\n",
            "1/1 - 0s - 62ms/step - loss: 732039360.0000 - val_loss: 887962944.0000\n",
            "Epoch 11570/20000\n",
            "1/1 - 0s - 59ms/step - loss: 731648704.0000 - val_loss: 888166592.0000\n",
            "Epoch 11571/20000\n",
            "1/1 - 0s - 74ms/step - loss: 731267520.0000 - val_loss: 887753472.0000\n",
            "Epoch 11572/20000\n",
            "1/1 - 0s - 71ms/step - loss: 730889344.0000 - val_loss: 887209984.0000\n",
            "Epoch 11573/20000\n",
            "1/1 - 0s - 74ms/step - loss: 730515200.0000 - val_loss: 887785472.0000\n",
            "Epoch 11574/20000\n",
            "1/1 - 0s - 85ms/step - loss: 730148160.0000 - val_loss: 886699776.0000\n",
            "Epoch 11575/20000\n",
            "1/1 - 0s - 95ms/step - loss: 729764480.0000 - val_loss: 886101760.0000\n",
            "Epoch 11576/20000\n",
            "1/1 - 0s - 79ms/step - loss: 729390144.0000 - val_loss: 886771008.0000\n",
            "Epoch 11577/20000\n",
            "1/1 - 0s - 140ms/step - loss: 729022336.0000 - val_loss: 885891200.0000\n",
            "Epoch 11578/20000\n",
            "1/1 - 0s - 112ms/step - loss: 728647744.0000 - val_loss: 884656960.0000\n",
            "Epoch 11579/20000\n",
            "1/1 - 0s - 132ms/step - loss: 728297792.0000 - val_loss: 886275136.0000\n",
            "Epoch 11580/20000\n",
            "1/1 - 0s - 168ms/step - loss: 727916736.0000 - val_loss: 885247488.0000\n",
            "Epoch 11581/20000\n",
            "1/1 - 0s - 127ms/step - loss: 727548864.0000 - val_loss: 884322752.0000\n",
            "Epoch 11582/20000\n",
            "1/1 - 0s - 86ms/step - loss: 727195584.0000 - val_loss: 886247744.0000\n",
            "Epoch 11583/20000\n",
            "1/1 - 0s - 153ms/step - loss: 726847488.0000 - val_loss: 883922112.0000\n",
            "Epoch 11584/20000\n",
            "1/1 - 0s - 103ms/step - loss: 726471744.0000 - val_loss: 883254720.0000\n",
            "Epoch 11585/20000\n",
            "1/1 - 0s - 145ms/step - loss: 726119936.0000 - val_loss: 884769472.0000\n",
            "Epoch 11586/20000\n",
            "1/1 - 0s - 115ms/step - loss: 725762368.0000 - val_loss: 883883264.0000\n",
            "Epoch 11587/20000\n",
            "1/1 - 0s - 138ms/step - loss: 725386048.0000 - val_loss: 881867840.0000\n",
            "Epoch 11588/20000\n",
            "1/1 - 0s - 79ms/step - loss: 725054016.0000 - val_loss: 883382656.0000\n",
            "Epoch 11589/20000\n",
            "1/1 - 0s - 136ms/step - loss: 724686976.0000 - val_loss: 883310592.0000\n",
            "Epoch 11590/20000\n",
            "1/1 - 0s - 116ms/step - loss: 724336512.0000 - val_loss: 881963520.0000\n",
            "Epoch 11591/20000\n",
            "1/1 - 0s - 120ms/step - loss: 723991552.0000 - val_loss: 882444736.0000\n",
            "Epoch 11592/20000\n",
            "1/1 - 0s - 105ms/step - loss: 723617280.0000 - val_loss: 883229760.0000\n",
            "Epoch 11593/20000\n",
            "1/1 - 0s - 79ms/step - loss: 723269184.0000 - val_loss: 881929536.0000\n",
            "Epoch 11594/20000\n",
            "1/1 - 0s - 97ms/step - loss: 722903936.0000 - val_loss: 880602816.0000\n",
            "Epoch 11595/20000\n",
            "1/1 - 0s - 147ms/step - loss: 722576704.0000 - val_loss: 881274368.0000\n",
            "Epoch 11596/20000\n",
            "1/1 - 0s - 80ms/step - loss: 722220672.0000 - val_loss: 881108608.0000\n",
            "Epoch 11597/20000\n",
            "1/1 - 0s - 149ms/step - loss: 721865728.0000 - val_loss: 881211392.0000\n",
            "Epoch 11598/20000\n",
            "1/1 - 0s - 128ms/step - loss: 721510912.0000 - val_loss: 881714048.0000\n",
            "Epoch 11599/20000\n",
            "1/1 - 0s - 102ms/step - loss: 721172928.0000 - val_loss: 879921408.0000\n",
            "Epoch 11600/20000\n",
            "1/1 - 0s - 109ms/step - loss: 720829824.0000 - val_loss: 880521280.0000\n",
            "Epoch 11601/20000\n",
            "1/1 - 0s - 133ms/step - loss: 720472512.0000 - val_loss: 880136832.0000\n",
            "Epoch 11602/20000\n",
            "1/1 - 0s - 117ms/step - loss: 720131712.0000 - val_loss: 878804608.0000\n",
            "Epoch 11603/20000\n",
            "1/1 - 0s - 308ms/step - loss: 719780800.0000 - val_loss: 878420928.0000\n",
            "Epoch 11604/20000\n",
            "1/1 - 0s - 103ms/step - loss: 719425344.0000 - val_loss: 878943360.0000\n",
            "Epoch 11605/20000\n",
            "1/1 - 0s - 89ms/step - loss: 719085440.0000 - val_loss: 878146368.0000\n",
            "Epoch 11606/20000\n",
            "1/1 - 0s - 122ms/step - loss: 718745664.0000 - val_loss: 878205120.0000\n",
            "Epoch 11607/20000\n",
            "1/1 - 0s - 139ms/step - loss: 718404672.0000 - val_loss: 878178624.0000\n",
            "Epoch 11608/20000\n",
            "1/1 - 0s - 62ms/step - loss: 718060672.0000 - val_loss: 878423936.0000\n",
            "Epoch 11609/20000\n",
            "1/1 - 0s - 138ms/step - loss: 717716416.0000 - val_loss: 878248640.0000\n",
            "Epoch 11610/20000\n",
            "1/1 - 0s - 140ms/step - loss: 717376448.0000 - val_loss: 877094976.0000\n",
            "Epoch 11611/20000\n",
            "1/1 - 0s - 61ms/step - loss: 717035136.0000 - val_loss: 876925568.0000\n",
            "Epoch 11612/20000\n",
            "1/1 - 0s - 69ms/step - loss: 716689280.0000 - val_loss: 876625536.0000\n",
            "Epoch 11613/20000\n",
            "1/1 - 0s - 73ms/step - loss: 716344896.0000 - val_loss: 876886528.0000\n",
            "Epoch 11614/20000\n",
            "1/1 - 0s - 64ms/step - loss: 716016448.0000 - val_loss: 875530816.0000\n",
            "Epoch 11615/20000\n",
            "1/1 - 0s - 63ms/step - loss: 715690880.0000 - val_loss: 876509504.0000\n",
            "Epoch 11616/20000\n",
            "1/1 - 0s - 65ms/step - loss: 715350528.0000 - val_loss: 876029952.0000\n",
            "Epoch 11617/20000\n",
            "1/1 - 0s - 66ms/step - loss: 715006848.0000 - val_loss: 875016832.0000\n",
            "Epoch 11618/20000\n",
            "1/1 - 0s - 59ms/step - loss: 714659968.0000 - val_loss: 874926144.0000\n",
            "Epoch 11619/20000\n",
            "1/1 - 0s - 63ms/step - loss: 714318144.0000 - val_loss: 876208640.0000\n",
            "Epoch 11620/20000\n",
            "1/1 - 0s - 139ms/step - loss: 714015872.0000 - val_loss: 874181696.0000\n",
            "Epoch 11621/20000\n",
            "1/1 - 0s - 62ms/step - loss: 713662336.0000 - val_loss: 873906752.0000\n",
            "Epoch 11622/20000\n",
            "1/1 - 0s - 63ms/step - loss: 713318080.0000 - val_loss: 874985088.0000\n",
            "Epoch 11623/20000\n",
            "1/1 - 0s - 137ms/step - loss: 712987776.0000 - val_loss: 873043776.0000\n",
            "Epoch 11624/20000\n",
            "1/1 - 0s - 63ms/step - loss: 712638976.0000 - val_loss: 873470080.0000\n",
            "Epoch 11625/20000\n",
            "1/1 - 0s - 137ms/step - loss: 712303360.0000 - val_loss: 873928832.0000\n",
            "Epoch 11626/20000\n",
            "1/1 - 0s - 63ms/step - loss: 711970304.0000 - val_loss: 873146368.0000\n",
            "Epoch 11627/20000\n",
            "1/1 - 0s - 60ms/step - loss: 711632896.0000 - val_loss: 872969984.0000\n",
            "Epoch 11628/20000\n",
            "1/1 - 0s - 59ms/step - loss: 711289344.0000 - val_loss: 872467456.0000\n",
            "Epoch 11629/20000\n",
            "1/1 - 0s - 61ms/step - loss: 710957120.0000 - val_loss: 873204736.0000\n",
            "Epoch 11630/20000\n",
            "1/1 - 0s - 139ms/step - loss: 710630080.0000 - val_loss: 872502656.0000\n",
            "Epoch 11631/20000\n",
            "1/1 - 0s - 63ms/step - loss: 710297920.0000 - val_loss: 872418240.0000\n",
            "Epoch 11632/20000\n",
            "1/1 - 0s - 62ms/step - loss: 709956416.0000 - val_loss: 872745408.0000\n",
            "Epoch 11633/20000\n",
            "1/1 - 0s - 65ms/step - loss: 709635200.0000 - val_loss: 871935680.0000\n",
            "Epoch 11634/20000\n",
            "1/1 - 0s - 65ms/step - loss: 709303744.0000 - val_loss: 871357952.0000\n",
            "Epoch 11635/20000\n",
            "1/1 - 0s - 67ms/step - loss: 708959296.0000 - val_loss: 871101504.0000\n",
            "Epoch 11636/20000\n",
            "1/1 - 0s - 77ms/step - loss: 708628352.0000 - val_loss: 871067712.0000\n",
            "Epoch 11637/20000\n",
            "1/1 - 0s - 72ms/step - loss: 708289856.0000 - val_loss: 871059904.0000\n",
            "Epoch 11638/20000\n",
            "1/1 - 0s - 135ms/step - loss: 707954176.0000 - val_loss: 870126784.0000\n",
            "Epoch 11639/20000\n",
            "1/1 - 0s - 62ms/step - loss: 707617088.0000 - val_loss: 869385920.0000\n",
            "Epoch 11640/20000\n",
            "1/1 - 0s - 65ms/step - loss: 707285696.0000 - val_loss: 870288768.0000\n",
            "Epoch 11641/20000\n",
            "1/1 - 0s - 63ms/step - loss: 706963712.0000 - val_loss: 868925824.0000\n",
            "Epoch 11642/20000\n",
            "1/1 - 0s - 62ms/step - loss: 706636608.0000 - val_loss: 869572032.0000\n",
            "Epoch 11643/20000\n",
            "1/1 - 0s - 62ms/step - loss: 706296128.0000 - val_loss: 868756992.0000\n",
            "Epoch 11644/20000\n",
            "1/1 - 0s - 62ms/step - loss: 705950400.0000 - val_loss: 868981248.0000\n",
            "Epoch 11645/20000\n",
            "1/1 - 0s - 142ms/step - loss: 705619392.0000 - val_loss: 870126080.0000\n",
            "Epoch 11646/20000\n",
            "1/1 - 0s - 61ms/step - loss: 705321728.0000 - val_loss: 867639872.0000\n",
            "Epoch 11647/20000\n",
            "1/1 - 0s - 149ms/step - loss: 704990976.0000 - val_loss: 867543232.0000\n",
            "Epoch 11648/20000\n",
            "1/1 - 0s - 135ms/step - loss: 704648448.0000 - val_loss: 869514176.0000\n",
            "Epoch 11649/20000\n",
            "1/1 - 0s - 131ms/step - loss: 704352128.0000 - val_loss: 867601536.0000\n",
            "Epoch 11650/20000\n",
            "1/1 - 0s - 141ms/step - loss: 703989696.0000 - val_loss: 865811200.0000\n",
            "Epoch 11651/20000\n",
            "1/1 - 0s - 64ms/step - loss: 703707648.0000 - val_loss: 868136960.0000\n",
            "Epoch 11652/20000\n",
            "1/1 - 0s - 62ms/step - loss: 703340416.0000 - val_loss: 868293760.0000\n",
            "Epoch 11653/20000\n",
            "1/1 - 0s - 64ms/step - loss: 703017024.0000 - val_loss: 865095360.0000\n",
            "Epoch 11654/20000\n",
            "1/1 - 0s - 61ms/step - loss: 702700288.0000 - val_loss: 865759488.0000\n",
            "Epoch 11655/20000\n",
            "1/1 - 0s - 69ms/step - loss: 702336768.0000 - val_loss: 867055744.0000\n",
            "Epoch 11656/20000\n",
            "1/1 - 0s - 134ms/step - loss: 702026048.0000 - val_loss: 865481536.0000\n",
            "Epoch 11657/20000\n",
            "1/1 - 0s - 62ms/step - loss: 701670848.0000 - val_loss: 864244928.0000\n",
            "Epoch 11658/20000\n",
            "1/1 - 0s - 152ms/step - loss: 701365568.0000 - val_loss: 866527296.0000\n",
            "Epoch 11659/20000\n",
            "1/1 - 0s - 129ms/step - loss: 701014720.0000 - val_loss: 865953600.0000\n",
            "Epoch 11660/20000\n",
            "1/1 - 0s - 58ms/step - loss: 700667328.0000 - val_loss: 864457984.0000\n",
            "Epoch 11661/20000\n",
            "1/1 - 0s - 61ms/step - loss: 700346240.0000 - val_loss: 865651520.0000\n",
            "Epoch 11662/20000\n",
            "1/1 - 0s - 58ms/step - loss: 700021376.0000 - val_loss: 864613952.0000\n",
            "Epoch 11663/20000\n",
            "1/1 - 0s - 63ms/step - loss: 699685376.0000 - val_loss: 862897024.0000\n",
            "Epoch 11664/20000\n",
            "1/1 - 0s - 138ms/step - loss: 699367296.0000 - val_loss: 863869248.0000\n",
            "Epoch 11665/20000\n",
            "1/1 - 0s - 139ms/step - loss: 699030080.0000 - val_loss: 864633728.0000\n",
            "Epoch 11666/20000\n",
            "1/1 - 0s - 61ms/step - loss: 698715264.0000 - val_loss: 862939200.0000\n",
            "Epoch 11667/20000\n",
            "1/1 - 0s - 62ms/step - loss: 698402880.0000 - val_loss: 863975232.0000\n",
            "Epoch 11668/20000\n",
            "1/1 - 0s - 60ms/step - loss: 698065408.0000 - val_loss: 864202816.0000\n",
            "Epoch 11669/20000\n",
            "1/1 - 0s - 82ms/step - loss: 697741696.0000 - val_loss: 861887360.0000\n",
            "Epoch 11670/20000\n",
            "1/1 - 0s - 72ms/step - loss: 697405184.0000 - val_loss: 861684864.0000\n",
            "Epoch 11671/20000\n",
            "1/1 - 0s - 70ms/step - loss: 697087936.0000 - val_loss: 863288192.0000\n",
            "Epoch 11672/20000\n",
            "1/1 - 0s - 137ms/step - loss: 696777280.0000 - val_loss: 861857792.0000\n",
            "Epoch 11673/20000\n",
            "1/1 - 0s - 63ms/step - loss: 696434560.0000 - val_loss: 860630144.0000\n",
            "Epoch 11674/20000\n",
            "1/1 - 0s - 65ms/step - loss: 696124288.0000 - val_loss: 862232576.0000\n",
            "Epoch 11675/20000\n",
            "1/1 - 0s - 61ms/step - loss: 695796800.0000 - val_loss: 861585216.0000\n",
            "Epoch 11676/20000\n",
            "1/1 - 0s - 70ms/step - loss: 695469504.0000 - val_loss: 859881472.0000\n",
            "Epoch 11677/20000\n",
            "1/1 - 0s - 62ms/step - loss: 695185856.0000 - val_loss: 862209088.0000\n",
            "Epoch 11678/20000\n",
            "1/1 - 0s - 63ms/step - loss: 694869504.0000 - val_loss: 860350464.0000\n",
            "Epoch 11679/20000\n",
            "1/1 - 0s - 59ms/step - loss: 694521856.0000 - val_loss: 859288960.0000\n",
            "Epoch 11680/20000\n",
            "1/1 - 0s - 57ms/step - loss: 694213376.0000 - val_loss: 861133888.0000\n",
            "Epoch 11681/20000\n",
            "1/1 - 0s - 58ms/step - loss: 693916288.0000 - val_loss: 859415872.0000\n",
            "Epoch 11682/20000\n",
            "1/1 - 0s - 59ms/step - loss: 693582976.0000 - val_loss: 858068032.0000\n",
            "Epoch 11683/20000\n",
            "1/1 - 0s - 85ms/step - loss: 693303104.0000 - val_loss: 860980480.0000\n",
            "Epoch 11684/20000\n",
            "1/1 - 0s - 127ms/step - loss: 693025152.0000 - val_loss: 859267648.0000\n",
            "Epoch 11685/20000\n",
            "1/1 - 0s - 60ms/step - loss: 692673280.0000 - val_loss: 856137344.0000\n",
            "Epoch 11686/20000\n",
            "1/1 - 0s - 59ms/step - loss: 692400512.0000 - val_loss: 858816192.0000\n",
            "Epoch 11687/20000\n",
            "1/1 - 0s - 61ms/step - loss: 692062784.0000 - val_loss: 859046016.0000\n",
            "Epoch 11688/20000\n",
            "1/1 - 0s - 60ms/step - loss: 691768064.0000 - val_loss: 855944128.0000\n",
            "Epoch 11689/20000\n",
            "1/1 - 0s - 141ms/step - loss: 691462656.0000 - val_loss: 856682624.0000\n",
            "Epoch 11690/20000\n",
            "1/1 - 0s - 139ms/step - loss: 691133696.0000 - val_loss: 858438016.0000\n",
            "Epoch 11691/20000\n",
            "1/1 - 0s - 62ms/step - loss: 690857536.0000 - val_loss: 856693440.0000\n",
            "Epoch 11692/20000\n",
            "1/1 - 0s - 139ms/step - loss: 690521728.0000 - val_loss: 855610368.0000\n",
            "Epoch 11693/20000\n",
            "1/1 - 0s - 145ms/step - loss: 690240000.0000 - val_loss: 857929088.0000\n",
            "Epoch 11694/20000\n",
            "1/1 - 0s - 60ms/step - loss: 689944512.0000 - val_loss: 856036416.0000\n",
            "Epoch 11695/20000\n",
            "1/1 - 0s - 61ms/step - loss: 689608960.0000 - val_loss: 855365760.0000\n",
            "Epoch 11696/20000\n",
            "1/1 - 0s - 63ms/step - loss: 689300800.0000 - val_loss: 855429760.0000\n",
            "Epoch 11697/20000\n",
            "1/1 - 0s - 136ms/step - loss: 688990592.0000 - val_loss: 855120000.0000\n",
            "Epoch 11698/20000\n",
            "1/1 - 0s - 61ms/step - loss: 688679936.0000 - val_loss: 854967744.0000\n",
            "Epoch 11699/20000\n",
            "1/1 - 0s - 60ms/step - loss: 688375104.0000 - val_loss: 853572096.0000\n",
            "Epoch 11700/20000\n",
            "1/1 - 0s - 141ms/step - loss: 688084416.0000 - val_loss: 854719616.0000\n",
            "Epoch 11701/20000\n",
            "1/1 - 0s - 62ms/step - loss: 687773888.0000 - val_loss: 854465856.0000\n",
            "Epoch 11702/20000\n",
            "1/1 - 0s - 63ms/step - loss: 687475456.0000 - val_loss: 853170880.0000\n",
            "Epoch 11703/20000\n",
            "1/1 - 0s - 59ms/step - loss: 687189568.0000 - val_loss: 853424512.0000\n",
            "Epoch 11704/20000\n",
            "1/1 - 0s - 62ms/step - loss: 686880000.0000 - val_loss: 854113216.0000\n",
            "Epoch 11705/20000\n",
            "1/1 - 0s - 72ms/step - loss: 686600064.0000 - val_loss: 851867584.0000\n",
            "Epoch 11706/20000\n",
            "1/1 - 0s - 137ms/step - loss: 686282752.0000 - val_loss: 850871616.0000\n",
            "Epoch 11707/20000\n",
            "1/1 - 0s - 60ms/step - loss: 686015808.0000 - val_loss: 853616896.0000\n",
            "Epoch 11708/20000\n",
            "1/1 - 0s - 58ms/step - loss: 685707968.0000 - val_loss: 853026880.0000\n",
            "Epoch 11709/20000\n",
            "1/1 - 0s - 139ms/step - loss: 685398592.0000 - val_loss: 850382784.0000\n",
            "Epoch 11710/20000\n",
            "1/1 - 0s - 141ms/step - loss: 685144256.0000 - val_loss: 852361408.0000\n",
            "Epoch 11711/20000\n",
            "1/1 - 0s - 140ms/step - loss: 684812160.0000 - val_loss: 851752704.0000\n",
            "Epoch 11712/20000\n",
            "1/1 - 0s - 63ms/step - loss: 684508416.0000 - val_loss: 849540864.0000\n",
            "Epoch 11713/20000\n",
            "1/1 - 0s - 62ms/step - loss: 684263936.0000 - val_loss: 852416640.0000\n",
            "Epoch 11714/20000\n",
            "1/1 - 0s - 57ms/step - loss: 683964672.0000 - val_loss: 850489024.0000\n",
            "Epoch 11715/20000\n",
            "1/1 - 0s - 64ms/step - loss: 683642944.0000 - val_loss: 848834240.0000\n",
            "Epoch 11716/20000\n",
            "1/1 - 0s - 140ms/step - loss: 683370176.0000 - val_loss: 849933696.0000\n",
            "Epoch 11717/20000\n",
            "1/1 - 0s - 61ms/step - loss: 683066816.0000 - val_loss: 850093376.0000\n",
            "Epoch 11718/20000\n",
            "1/1 - 0s - 63ms/step - loss: 682782016.0000 - val_loss: 849049536.0000\n",
            "Epoch 11719/20000\n",
            "1/1 - 0s - 86ms/step - loss: 682498688.0000 - val_loss: 849074944.0000\n",
            "Epoch 11720/20000\n",
            "1/1 - 0s - 132ms/step - loss: 682204032.0000 - val_loss: 849546368.0000\n",
            "Epoch 11721/20000\n",
            "1/1 - 0s - 93ms/step - loss: 681916992.0000 - val_loss: 848085760.0000\n",
            "Epoch 11722/20000\n",
            "1/1 - 0s - 89ms/step - loss: 681619456.0000 - val_loss: 847826880.0000\n",
            "Epoch 11723/20000\n",
            "1/1 - 0s - 93ms/step - loss: 681333184.0000 - val_loss: 849110464.0000\n",
            "Epoch 11724/20000\n",
            "1/1 - 0s - 137ms/step - loss: 681060672.0000 - val_loss: 846762816.0000\n",
            "Epoch 11725/20000\n",
            "1/1 - 0s - 116ms/step - loss: 680777472.0000 - val_loss: 847073792.0000\n",
            "Epoch 11726/20000\n",
            "1/1 - 0s - 125ms/step - loss: 680476608.0000 - val_loss: 848392640.0000\n",
            "Epoch 11727/20000\n",
            "1/1 - 0s - 80ms/step - loss: 680200512.0000 - val_loss: 846700160.0000\n",
            "Epoch 11728/20000\n",
            "1/1 - 0s - 82ms/step - loss: 679897344.0000 - val_loss: 846415872.0000\n",
            "Epoch 11729/20000\n",
            "1/1 - 0s - 140ms/step - loss: 679611520.0000 - val_loss: 847497856.0000\n",
            "Epoch 11730/20000\n",
            "1/1 - 0s - 136ms/step - loss: 679335360.0000 - val_loss: 845571520.0000\n",
            "Epoch 11731/20000\n",
            "1/1 - 0s - 160ms/step - loss: 679053120.0000 - val_loss: 846046848.0000\n",
            "Epoch 11732/20000\n",
            "1/1 - 0s - 98ms/step - loss: 678762112.0000 - val_loss: 847271936.0000\n",
            "Epoch 11733/20000\n",
            "1/1 - 0s - 141ms/step - loss: 678500096.0000 - val_loss: 844759552.0000\n",
            "Epoch 11734/20000\n",
            "1/1 - 0s - 92ms/step - loss: 678206784.0000 - val_loss: 845161792.0000\n",
            "Epoch 11735/20000\n",
            "1/1 - 0s - 131ms/step - loss: 677907584.0000 - val_loss: 846085952.0000\n",
            "Epoch 11736/20000\n",
            "1/1 - 0s - 102ms/step - loss: 677633024.0000 - val_loss: 844721728.0000\n",
            "Epoch 11737/20000\n",
            "1/1 - 0s - 119ms/step - loss: 677334016.0000 - val_loss: 843354944.0000\n",
            "Epoch 11738/20000\n",
            "1/1 - 0s - 137ms/step - loss: 677073408.0000 - val_loss: 845752192.0000\n",
            "Epoch 11739/20000\n",
            "1/1 - 0s - 158ms/step - loss: 676795904.0000 - val_loss: 844382720.0000\n",
            "Epoch 11740/20000\n",
            "1/1 - 0s - 93ms/step - loss: 676495680.0000 - val_loss: 842808000.0000\n",
            "Epoch 11741/20000\n",
            "1/1 - 0s - 152ms/step - loss: 676239552.0000 - val_loss: 845035904.0000\n",
            "Epoch 11742/20000\n",
            "1/1 - 0s - 94ms/step - loss: 675947072.0000 - val_loss: 843145984.0000\n",
            "Epoch 11743/20000\n",
            "1/1 - 0s - 168ms/step - loss: 675640960.0000 - val_loss: 842595648.0000\n",
            "Epoch 11744/20000\n",
            "1/1 - 0s - 113ms/step - loss: 675352576.0000 - val_loss: 843343296.0000\n",
            "Epoch 11745/20000\n",
            "1/1 - 0s - 136ms/step - loss: 675075520.0000 - val_loss: 842436864.0000\n",
            "Epoch 11746/20000\n",
            "1/1 - 0s - 158ms/step - loss: 674788928.0000 - val_loss: 842464960.0000\n",
            "Epoch 11747/20000\n",
            "1/1 - 0s - 90ms/step - loss: 674505024.0000 - val_loss: 841629312.0000\n",
            "Epoch 11748/20000\n",
            "1/1 - 0s - 175ms/step - loss: 674222656.0000 - val_loss: 842164672.0000\n",
            "Epoch 11749/20000\n",
            "1/1 - 0s - 87ms/step - loss: 673938688.0000 - val_loss: 841062592.0000\n",
            "Epoch 11750/20000\n",
            "1/1 - 0s - 89ms/step - loss: 673655360.0000 - val_loss: 841001408.0000\n",
            "Epoch 11751/20000\n",
            "1/1 - 0s - 110ms/step - loss: 673369088.0000 - val_loss: 841622592.0000\n",
            "Epoch 11752/20000\n",
            "1/1 - 0s - 121ms/step - loss: 673097408.0000 - val_loss: 840160320.0000\n",
            "Epoch 11753/20000\n",
            "1/1 - 0s - 88ms/step - loss: 672821248.0000 - val_loss: 840107904.0000\n",
            "Epoch 11754/20000\n",
            "1/1 - 0s - 63ms/step - loss: 672533184.0000 - val_loss: 841590784.0000\n",
            "Epoch 11755/20000\n",
            "1/1 - 0s - 61ms/step - loss: 672270272.0000 - val_loss: 838989632.0000\n",
            "Epoch 11756/20000\n",
            "1/1 - 0s - 60ms/step - loss: 671975552.0000 - val_loss: 838999360.0000\n",
            "Epoch 11757/20000\n",
            "1/1 - 0s - 68ms/step - loss: 671681344.0000 - val_loss: 841145984.0000\n",
            "Epoch 11758/20000\n",
            "1/1 - 0s - 64ms/step - loss: 671441856.0000 - val_loss: 838477824.0000\n",
            "Epoch 11759/20000\n",
            "1/1 - 0s - 72ms/step - loss: 671141696.0000 - val_loss: 837867328.0000\n",
            "Epoch 11760/20000\n",
            "1/1 - 0s - 131ms/step - loss: 670869952.0000 - val_loss: 839829312.0000\n",
            "Epoch 11761/20000\n",
            "1/1 - 0s - 63ms/step - loss: 670597504.0000 - val_loss: 838124672.0000\n",
            "Epoch 11762/20000\n",
            "1/1 - 0s - 145ms/step - loss: 670281216.0000 - val_loss: 837004608.0000\n",
            "Epoch 11763/20000\n",
            "1/1 - 0s - 129ms/step - loss: 670003136.0000 - val_loss: 837678528.0000\n",
            "Epoch 11764/20000\n",
            "1/1 - 0s - 65ms/step - loss: 669720256.0000 - val_loss: 838540032.0000\n",
            "Epoch 11765/20000\n",
            "1/1 - 0s - 61ms/step - loss: 669462272.0000 - val_loss: 836123200.0000\n",
            "Epoch 11766/20000\n",
            "1/1 - 0s - 62ms/step - loss: 669209728.0000 - val_loss: 837699968.0000\n",
            "Epoch 11767/20000\n",
            "1/1 - 0s - 66ms/step - loss: 668911936.0000 - val_loss: 837253120.0000\n",
            "Epoch 11768/20000\n",
            "1/1 - 0s - 68ms/step - loss: 668628416.0000 - val_loss: 835537856.0000\n",
            "Epoch 11769/20000\n",
            "1/1 - 0s - 60ms/step - loss: 668360000.0000 - val_loss: 836528128.0000\n",
            "Epoch 11770/20000\n",
            "1/1 - 0s - 61ms/step - loss: 668076992.0000 - val_loss: 835883072.0000\n",
            "Epoch 11771/20000\n",
            "1/1 - 0s - 75ms/step - loss: 667797632.0000 - val_loss: 834558848.0000\n",
            "Epoch 11772/20000\n",
            "1/1 - 0s - 132ms/step - loss: 667542080.0000 - val_loss: 836630464.0000\n",
            "Epoch 11773/20000\n",
            "1/1 - 0s - 61ms/step - loss: 667280896.0000 - val_loss: 835081280.0000\n",
            "Epoch 11774/20000\n",
            "1/1 - 0s - 139ms/step - loss: 666991936.0000 - val_loss: 833901376.0000\n",
            "Epoch 11775/20000\n",
            "1/1 - 0s - 75ms/step - loss: 666734912.0000 - val_loss: 835338560.0000\n",
            "Epoch 11776/20000\n",
            "1/1 - 0s - 60ms/step - loss: 666463936.0000 - val_loss: 833618880.0000\n",
            "Epoch 11777/20000\n",
            "1/1 - 0s - 59ms/step - loss: 666188736.0000 - val_loss: 833490816.0000\n",
            "Epoch 11778/20000\n",
            "1/1 - 0s - 59ms/step - loss: 665921600.0000 - val_loss: 834347136.0000\n",
            "Epoch 11779/20000\n",
            "1/1 - 0s - 59ms/step - loss: 665659456.0000 - val_loss: 833093120.0000\n",
            "Epoch 11780/20000\n",
            "1/1 - 0s - 67ms/step - loss: 665396480.0000 - val_loss: 834190016.0000\n",
            "Epoch 11781/20000\n",
            "1/1 - 0s - 134ms/step - loss: 665130368.0000 - val_loss: 832433664.0000\n",
            "Epoch 11782/20000\n",
            "1/1 - 0s - 68ms/step - loss: 664878336.0000 - val_loss: 833110528.0000\n",
            "Epoch 11783/20000\n",
            "1/1 - 0s - 67ms/step - loss: 664611776.0000 - val_loss: 832654848.0000\n",
            "Epoch 11784/20000\n",
            "1/1 - 0s - 132ms/step - loss: 664357696.0000 - val_loss: 831957632.0000\n",
            "Epoch 11785/20000\n",
            "1/1 - 0s - 56ms/step - loss: 664095680.0000 - val_loss: 833040128.0000\n",
            "Epoch 11786/20000\n",
            "1/1 - 0s - 58ms/step - loss: 663838464.0000 - val_loss: 831437504.0000\n",
            "Epoch 11787/20000\n",
            "1/1 - 0s - 60ms/step - loss: 663555584.0000 - val_loss: 830378240.0000\n",
            "Epoch 11788/20000\n",
            "1/1 - 0s - 77ms/step - loss: 663312000.0000 - val_loss: 832034944.0000\n",
            "Epoch 11789/20000\n",
            "1/1 - 0s - 60ms/step - loss: 663044864.0000 - val_loss: 830881216.0000\n",
            "Epoch 11790/20000\n",
            "1/1 - 0s - 59ms/step - loss: 662769536.0000 - val_loss: 829790144.0000\n",
            "Epoch 11791/20000\n",
            "1/1 - 0s - 59ms/step - loss: 662513664.0000 - val_loss: 832094400.0000\n",
            "Epoch 11792/20000\n",
            "1/1 - 0s - 145ms/step - loss: 662268544.0000 - val_loss: 829556608.0000\n",
            "Epoch 11793/20000\n",
            "1/1 - 0s - 132ms/step - loss: 661977280.0000 - val_loss: 829329664.0000\n",
            "Epoch 11794/20000\n",
            "1/1 - 0s - 76ms/step - loss: 661714560.0000 - val_loss: 831334272.0000\n",
            "Epoch 11795/20000\n",
            "1/1 - 0s - 128ms/step - loss: 661478528.0000 - val_loss: 828855744.0000\n",
            "Epoch 11796/20000\n",
            "1/1 - 0s - 58ms/step - loss: 661187712.0000 - val_loss: 828645248.0000\n",
            "Epoch 11797/20000\n",
            "1/1 - 0s - 59ms/step - loss: 660915200.0000 - val_loss: 829763648.0000\n",
            "Epoch 11798/20000\n",
            "1/1 - 0s - 61ms/step - loss: 660657600.0000 - val_loss: 828543616.0000\n",
            "Epoch 11799/20000\n",
            "1/1 - 0s - 61ms/step - loss: 660403200.0000 - val_loss: 828546176.0000\n",
            "Epoch 11800/20000\n",
            "1/1 - 0s - 70ms/step - loss: 660139776.0000 - val_loss: 829515456.0000\n",
            "Epoch 11801/20000\n",
            "1/1 - 0s - 128ms/step - loss: 659885888.0000 - val_loss: 827025792.0000\n",
            "Epoch 11802/20000\n",
            "1/1 - 0s - 60ms/step - loss: 659613312.0000 - val_loss: 827419392.0000\n",
            "Epoch 11803/20000\n",
            "1/1 - 0s - 61ms/step - loss: 659354048.0000 - val_loss: 827847296.0000\n",
            "Epoch 11804/20000\n",
            "1/1 - 0s - 65ms/step - loss: 659094592.0000 - val_loss: 827341312.0000\n",
            "Epoch 11805/20000\n",
            "1/1 - 0s - 62ms/step - loss: 658834112.0000 - val_loss: 826991360.0000\n",
            "Epoch 11806/20000\n",
            "1/1 - 0s - 66ms/step - loss: 658571264.0000 - val_loss: 827342528.0000\n",
            "Epoch 11807/20000\n",
            "1/1 - 0s - 72ms/step - loss: 658315456.0000 - val_loss: 826341760.0000\n",
            "Epoch 11808/20000\n",
            "1/1 - 0s - 135ms/step - loss: 658061120.0000 - val_loss: 826849984.0000\n",
            "Epoch 11809/20000\n",
            "1/1 - 0s - 78ms/step - loss: 657794816.0000 - val_loss: 826316736.0000\n",
            "Epoch 11810/20000\n",
            "1/1 - 0s - 117ms/step - loss: 657537216.0000 - val_loss: 826048640.0000\n",
            "Epoch 11811/20000\n",
            "1/1 - 0s - 60ms/step - loss: 657288320.0000 - val_loss: 825432256.0000\n",
            "Epoch 11812/20000\n",
            "1/1 - 0s - 72ms/step - loss: 657030080.0000 - val_loss: 825499328.0000\n",
            "Epoch 11813/20000\n",
            "1/1 - 0s - 133ms/step - loss: 656766272.0000 - val_loss: 824799936.0000\n",
            "Epoch 11814/20000\n",
            "1/1 - 0s - 68ms/step - loss: 656505984.0000 - val_loss: 824972096.0000\n",
            "Epoch 11815/20000\n",
            "1/1 - 0s - 132ms/step - loss: 656242240.0000 - val_loss: 824699264.0000\n",
            "Epoch 11816/20000\n",
            "1/1 - 0s - 66ms/step - loss: 655978624.0000 - val_loss: 824324352.0000\n",
            "Epoch 11817/20000\n",
            "1/1 - 0s - 70ms/step - loss: 655727552.0000 - val_loss: 824326912.0000\n",
            "Epoch 11818/20000\n",
            "1/1 - 0s - 72ms/step - loss: 655474368.0000 - val_loss: 824700608.0000\n",
            "Epoch 11819/20000\n",
            "1/1 - 0s - 61ms/step - loss: 655226624.0000 - val_loss: 824214848.0000\n",
            "Epoch 11820/20000\n",
            "1/1 - 0s - 59ms/step - loss: 654976832.0000 - val_loss: 824257728.0000\n",
            "Epoch 11821/20000\n",
            "1/1 - 0s - 63ms/step - loss: 654721792.0000 - val_loss: 822913408.0000\n",
            "Epoch 11822/20000\n",
            "1/1 - 0s - 139ms/step - loss: 654463872.0000 - val_loss: 823344384.0000\n",
            "Epoch 11823/20000\n",
            "1/1 - 0s - 59ms/step - loss: 654210048.0000 - val_loss: 823671360.0000\n",
            "Epoch 11824/20000\n",
            "1/1 - 0s - 76ms/step - loss: 653957440.0000 - val_loss: 821680320.0000\n",
            "Epoch 11825/20000\n",
            "1/1 - 0s - 59ms/step - loss: 653726592.0000 - val_loss: 823122688.0000\n",
            "Epoch 11826/20000\n",
            "1/1 - 0s - 60ms/step - loss: 653461888.0000 - val_loss: 822541568.0000\n",
            "Epoch 11827/20000\n",
            "1/1 - 0s - 63ms/step - loss: 653207936.0000 - val_loss: 821223232.0000\n",
            "Epoch 11828/20000\n",
            "1/1 - 0s - 135ms/step - loss: 652953408.0000 - val_loss: 821754048.0000\n",
            "Epoch 11829/20000\n",
            "1/1 - 0s - 70ms/step - loss: 652687552.0000 - val_loss: 822026112.0000\n",
            "Epoch 11830/20000\n",
            "1/1 - 0s - 74ms/step - loss: 652431936.0000 - val_loss: 821737600.0000\n",
            "Epoch 11831/20000\n",
            "1/1 - 0s - 130ms/step - loss: 652192128.0000 - val_loss: 822255424.0000\n",
            "Epoch 11832/20000\n",
            "1/1 - 0s - 62ms/step - loss: 651944896.0000 - val_loss: 820436096.0000\n",
            "Epoch 11833/20000\n",
            "1/1 - 0s - 59ms/step - loss: 651692992.0000 - val_loss: 821037120.0000\n",
            "Epoch 11834/20000\n",
            "1/1 - 0s - 60ms/step - loss: 651429312.0000 - val_loss: 820771776.0000\n",
            "Epoch 11835/20000\n",
            "1/1 - 0s - 60ms/step - loss: 651181504.0000 - val_loss: 819223104.0000\n",
            "Epoch 11836/20000\n",
            "1/1 - 0s - 62ms/step - loss: 650928896.0000 - val_loss: 820242048.0000\n",
            "Epoch 11837/20000\n",
            "1/1 - 0s - 73ms/step - loss: 650663744.0000 - val_loss: 819890752.0000\n",
            "Epoch 11838/20000\n",
            "1/1 - 0s - 138ms/step - loss: 650408320.0000 - val_loss: 818514560.0000\n",
            "Epoch 11839/20000\n",
            "1/1 - 0s - 59ms/step - loss: 650166208.0000 - val_loss: 820564224.0000\n",
            "Epoch 11840/20000\n",
            "1/1 - 0s - 61ms/step - loss: 649907008.0000 - val_loss: 819046016.0000\n",
            "Epoch 11841/20000\n",
            "1/1 - 0s - 60ms/step - loss: 649634688.0000 - val_loss: 818265600.0000\n",
            "Epoch 11842/20000\n",
            "1/1 - 0s - 70ms/step - loss: 649383296.0000 - val_loss: 819352768.0000\n",
            "Epoch 11843/20000\n",
            "1/1 - 0s - 134ms/step - loss: 649122688.0000 - val_loss: 818529408.0000\n",
            "Epoch 11844/20000\n",
            "1/1 - 0s - 62ms/step - loss: 648852864.0000 - val_loss: 818303808.0000\n",
            "Epoch 11845/20000\n",
            "1/1 - 0s - 60ms/step - loss: 648595840.0000 - val_loss: 818159936.0000\n",
            "Epoch 11846/20000\n",
            "1/1 - 0s - 63ms/step - loss: 648339776.0000 - val_loss: 817105152.0000\n",
            "Epoch 11847/20000\n",
            "1/1 - 0s - 61ms/step - loss: 648086592.0000 - val_loss: 817068992.0000\n",
            "Epoch 11848/20000\n",
            "1/1 - 0s - 61ms/step - loss: 647823488.0000 - val_loss: 817224064.0000\n",
            "Epoch 11849/20000\n",
            "1/1 - 0s - 63ms/step - loss: 647574016.0000 - val_loss: 816651008.0000\n",
            "Epoch 11850/20000\n",
            "1/1 - 0s - 72ms/step - loss: 647318912.0000 - val_loss: 816450752.0000\n",
            "Epoch 11851/20000\n",
            "1/1 - 0s - 62ms/step - loss: 647061312.0000 - val_loss: 816518720.0000\n",
            "Epoch 11852/20000\n",
            "1/1 - 0s - 61ms/step - loss: 646803008.0000 - val_loss: 817645568.0000\n",
            "Epoch 11853/20000\n",
            "1/1 - 0s - 60ms/step - loss: 646562816.0000 - val_loss: 815189952.0000\n",
            "Epoch 11854/20000\n",
            "1/1 - 0s - 64ms/step - loss: 646320640.0000 - val_loss: 815860544.0000\n",
            "Epoch 11855/20000\n",
            "1/1 - 0s - 66ms/step - loss: 646051584.0000 - val_loss: 817037504.0000\n",
            "Epoch 11856/20000\n",
            "1/1 - 0s - 73ms/step - loss: 645812608.0000 - val_loss: 814426176.0000\n",
            "Epoch 11857/20000\n",
            "1/1 - 0s - 133ms/step - loss: 645539456.0000 - val_loss: 814178432.0000\n",
            "Epoch 11858/20000\n",
            "1/1 - 0s - 62ms/step - loss: 645284544.0000 - val_loss: 816351232.0000\n",
            "Epoch 11859/20000\n",
            "1/1 - 0s - 62ms/step - loss: 645053184.0000 - val_loss: 814354496.0000\n",
            "Epoch 11860/20000\n",
            "1/1 - 0s - 61ms/step - loss: 644768960.0000 - val_loss: 813049728.0000\n",
            "Epoch 11861/20000\n",
            "1/1 - 0s - 63ms/step - loss: 644541568.0000 - val_loss: 815901248.0000\n",
            "Epoch 11862/20000\n",
            "1/1 - 0s - 62ms/step - loss: 644305664.0000 - val_loss: 814218176.0000\n",
            "Epoch 11863/20000\n",
            "1/1 - 0s - 148ms/step - loss: 644018112.0000 - val_loss: 811693504.0000\n",
            "Epoch 11864/20000\n",
            "1/1 - 0s - 72ms/step - loss: 643817984.0000 - val_loss: 814764928.0000\n",
            "Epoch 11865/20000\n",
            "1/1 - 0s - 63ms/step - loss: 643545600.0000 - val_loss: 814005952.0000\n",
            "Epoch 11866/20000\n",
            "1/1 - 0s - 65ms/step - loss: 643283840.0000 - val_loss: 811546432.0000\n",
            "Epoch 11867/20000\n",
            "1/1 - 0s - 58ms/step - loss: 643074112.0000 - val_loss: 813376064.0000\n",
            "Epoch 11868/20000\n",
            "1/1 - 0s - 71ms/step - loss: 642797312.0000 - val_loss: 814687104.0000\n",
            "Epoch 11869/20000\n",
            "1/1 - 0s - 131ms/step - loss: 642586240.0000 - val_loss: 811284800.0000\n",
            "Epoch 11870/20000\n",
            "1/1 - 0s - 65ms/step - loss: 642317440.0000 - val_loss: 810883584.0000\n",
            "Epoch 11871/20000\n",
            "1/1 - 0s - 67ms/step - loss: 642078336.0000 - val_loss: 814812672.0000\n",
            "Epoch 11872/20000\n",
            "1/1 - 0s - 63ms/step - loss: 641889664.0000 - val_loss: 812038720.0000\n",
            "Epoch 11873/20000\n",
            "1/1 - 0s - 60ms/step - loss: 641570624.0000 - val_loss: 809080640.0000\n",
            "Epoch 11874/20000\n",
            "1/1 - 0s - 62ms/step - loss: 641401088.0000 - val_loss: 813013184.0000\n",
            "Epoch 11875/20000\n",
            "1/1 - 0s - 175ms/step - loss: 641124608.0000 - val_loss: 812610176.0000\n",
            "Epoch 11876/20000\n",
            "1/1 - 0s - 134ms/step - loss: 640877504.0000 - val_loss: 808610048.0000\n",
            "Epoch 11877/20000\n",
            "1/1 - 0s - 139ms/step - loss: 640654208.0000 - val_loss: 810302592.0000\n",
            "Epoch 11878/20000\n",
            "1/1 - 0s - 122ms/step - loss: 640359360.0000 - val_loss: 811913664.0000\n",
            "Epoch 11879/20000\n",
            "1/1 - 0s - 141ms/step - loss: 640146304.0000 - val_loss: 810058112.0000\n",
            "Epoch 11880/20000\n",
            "1/1 - 0s - 131ms/step - loss: 639896192.0000 - val_loss: 809411712.0000\n",
            "Epoch 11881/20000\n",
            "1/1 - 0s - 131ms/step - loss: 639670848.0000 - val_loss: 811473792.0000\n",
            "Epoch 11882/20000\n",
            "1/1 - 0s - 136ms/step - loss: 639427328.0000 - val_loss: 810402496.0000\n",
            "Epoch 11883/20000\n",
            "1/1 - 0s - 144ms/step - loss: 639176000.0000 - val_loss: 808360960.0000\n",
            "Epoch 11884/20000\n",
            "1/1 - 0s - 136ms/step - loss: 638942848.0000 - val_loss: 809285248.0000\n",
            "Epoch 11885/20000\n",
            "1/1 - 0s - 134ms/step - loss: 638687744.0000 - val_loss: 810200064.0000\n",
            "Epoch 11886/20000\n",
            "1/1 - 0s - 92ms/step - loss: 638459840.0000 - val_loss: 808279424.0000\n",
            "Epoch 11887/20000\n",
            "1/1 - 0s - 133ms/step - loss: 638204480.0000 - val_loss: 808827200.0000\n",
            "Epoch 11888/20000\n",
            "1/1 - 0s - 85ms/step - loss: 637958144.0000 - val_loss: 809447296.0000\n",
            "Epoch 11889/20000\n",
            "1/1 - 0s - 138ms/step - loss: 637732800.0000 - val_loss: 807418240.0000\n",
            "Epoch 11890/20000\n",
            "1/1 - 0s - 113ms/step - loss: 637492160.0000 - val_loss: 808005120.0000\n",
            "Epoch 11891/20000\n",
            "1/1 - 0s - 102ms/step - loss: 637245248.0000 - val_loss: 808162176.0000\n",
            "Epoch 11892/20000\n",
            "1/1 - 0s - 120ms/step - loss: 637003648.0000 - val_loss: 807236032.0000\n",
            "Epoch 11893/20000\n",
            "1/1 - 0s - 110ms/step - loss: 636758656.0000 - val_loss: 807791680.0000\n",
            "Epoch 11894/20000\n",
            "1/1 - 0s - 120ms/step - loss: 636513024.0000 - val_loss: 807917696.0000\n",
            "Epoch 11895/20000\n",
            "1/1 - 0s - 137ms/step - loss: 636274688.0000 - val_loss: 806919104.0000\n",
            "Epoch 11896/20000\n",
            "1/1 - 0s - 148ms/step - loss: 636027392.0000 - val_loss: 807192768.0000\n",
            "Epoch 11897/20000\n",
            "1/1 - 0s - 129ms/step - loss: 635782080.0000 - val_loss: 806250816.0000\n",
            "Epoch 11898/20000\n",
            "1/1 - 0s - 84ms/step - loss: 635534144.0000 - val_loss: 806446400.0000\n",
            "Epoch 11899/20000\n",
            "1/1 - 0s - 151ms/step - loss: 635275264.0000 - val_loss: 807021056.0000\n",
            "Epoch 11900/20000\n",
            "1/1 - 0s - 134ms/step - loss: 635031744.0000 - val_loss: 805357120.0000\n",
            "Epoch 11901/20000\n",
            "1/1 - 0s - 144ms/step - loss: 634779840.0000 - val_loss: 805834432.0000\n",
            "Epoch 11902/20000\n",
            "1/1 - 0s - 110ms/step - loss: 634533056.0000 - val_loss: 805144256.0000\n",
            "Epoch 11903/20000\n",
            "1/1 - 0s - 142ms/step - loss: 634281280.0000 - val_loss: 805358528.0000\n",
            "Epoch 11904/20000\n",
            "1/1 - 0s - 120ms/step - loss: 634033536.0000 - val_loss: 805294784.0000\n",
            "Epoch 11905/20000\n",
            "1/1 - 0s - 65ms/step - loss: 633781952.0000 - val_loss: 804782720.0000\n",
            "Epoch 11906/20000\n",
            "1/1 - 0s - 139ms/step - loss: 633516544.0000 - val_loss: 804657600.0000\n",
            "Epoch 11907/20000\n",
            "1/1 - 0s - 62ms/step - loss: 633259200.0000 - val_loss: 804660608.0000\n",
            "Epoch 11908/20000\n",
            "1/1 - 0s - 60ms/step - loss: 633003712.0000 - val_loss: 804850688.0000\n",
            "Epoch 11909/20000\n",
            "1/1 - 0s - 74ms/step - loss: 632744768.0000 - val_loss: 804151744.0000\n",
            "Epoch 11910/20000\n",
            "1/1 - 0s - 70ms/step - loss: 632475392.0000 - val_loss: 803672640.0000\n",
            "Epoch 11911/20000\n",
            "1/1 - 0s - 80ms/step - loss: 632222400.0000 - val_loss: 803726080.0000\n",
            "Epoch 11912/20000\n",
            "1/1 - 0s - 67ms/step - loss: 631962176.0000 - val_loss: 803246720.0000\n",
            "Epoch 11913/20000\n",
            "1/1 - 0s - 63ms/step - loss: 631692992.0000 - val_loss: 802142784.0000\n",
            "Epoch 11914/20000\n",
            "1/1 - 0s - 61ms/step - loss: 631436352.0000 - val_loss: 803427392.0000\n",
            "Epoch 11915/20000\n",
            "1/1 - 0s - 64ms/step - loss: 631167168.0000 - val_loss: 803262848.0000\n",
            "Epoch 11916/20000\n",
            "1/1 - 0s - 134ms/step - loss: 630910848.0000 - val_loss: 801907840.0000\n",
            "Epoch 11917/20000\n",
            "1/1 - 0s - 62ms/step - loss: 630656064.0000 - val_loss: 802276928.0000\n",
            "Epoch 11918/20000\n",
            "1/1 - 0s - 59ms/step - loss: 630397312.0000 - val_loss: 801833920.0000\n",
            "Epoch 11919/20000\n",
            "1/1 - 0s - 59ms/step - loss: 630139648.0000 - val_loss: 801655680.0000\n",
            "Epoch 11920/20000\n",
            "1/1 - 0s - 63ms/step - loss: 629878912.0000 - val_loss: 801544576.0000\n",
            "Epoch 11921/20000\n",
            "1/1 - 0s - 140ms/step - loss: 629622592.0000 - val_loss: 800738368.0000\n",
            "Epoch 11922/20000\n",
            "1/1 - 0s - 145ms/step - loss: 629377024.0000 - val_loss: 801822464.0000\n",
            "Epoch 11923/20000\n",
            "1/1 - 0s - 132ms/step - loss: 629112128.0000 - val_loss: 801544960.0000\n",
            "Epoch 11924/20000\n",
            "1/1 - 0s - 61ms/step - loss: 628855104.0000 - val_loss: 799488384.0000\n",
            "Epoch 11925/20000\n",
            "1/1 - 0s - 61ms/step - loss: 628619456.0000 - val_loss: 800938112.0000\n",
            "Epoch 11926/20000\n",
            "1/1 - 0s - 60ms/step - loss: 628350080.0000 - val_loss: 801389056.0000\n",
            "Epoch 11927/20000\n",
            "1/1 - 0s - 64ms/step - loss: 628106752.0000 - val_loss: 799074304.0000\n",
            "Epoch 11928/20000\n",
            "1/1 - 0s - 63ms/step - loss: 627876544.0000 - val_loss: 800535296.0000\n",
            "Epoch 11929/20000\n",
            "1/1 - 0s - 60ms/step - loss: 627618304.0000 - val_loss: 800847040.0000\n",
            "Epoch 11930/20000\n",
            "1/1 - 0s - 62ms/step - loss: 627377472.0000 - val_loss: 799108608.0000\n",
            "Epoch 11931/20000\n",
            "1/1 - 0s - 60ms/step - loss: 627118784.0000 - val_loss: 798618688.0000\n",
            "Epoch 11932/20000\n",
            "1/1 - 0s - 62ms/step - loss: 626870592.0000 - val_loss: 799974656.0000\n",
            "Epoch 11933/20000\n",
            "1/1 - 0s - 62ms/step - loss: 626634432.0000 - val_loss: 798975872.0000\n",
            "Epoch 11934/20000\n",
            "1/1 - 0s - 62ms/step - loss: 626381696.0000 - val_loss: 797692992.0000\n",
            "Epoch 11935/20000\n",
            "1/1 - 0s - 102ms/step - loss: 626148800.0000 - val_loss: 799011648.0000\n",
            "Epoch 11936/20000\n",
            "1/1 - 0s - 70ms/step - loss: 625900608.0000 - val_loss: 799061120.0000\n",
            "Epoch 11937/20000\n",
            "1/1 - 0s - 65ms/step - loss: 625673856.0000 - val_loss: 797496064.0000\n",
            "Epoch 11938/20000\n",
            "1/1 - 0s - 60ms/step - loss: 625438272.0000 - val_loss: 797176896.0000\n",
            "Epoch 11939/20000\n",
            "1/1 - 0s - 61ms/step - loss: 625199936.0000 - val_loss: 798463936.0000\n",
            "Epoch 11940/20000\n",
            "1/1 - 0s - 64ms/step - loss: 624965440.0000 - val_loss: 797316928.0000\n",
            "Epoch 11941/20000\n",
            "1/1 - 0s - 61ms/step - loss: 624709952.0000 - val_loss: 796057728.0000\n",
            "Epoch 11942/20000\n",
            "1/1 - 0s - 138ms/step - loss: 624477888.0000 - val_loss: 797035456.0000\n",
            "Epoch 11943/20000\n",
            "1/1 - 0s - 61ms/step - loss: 624249792.0000 - val_loss: 796439488.0000\n",
            "Epoch 11944/20000\n",
            "1/1 - 0s - 58ms/step - loss: 624009536.0000 - val_loss: 795426496.0000\n",
            "Epoch 11945/20000\n",
            "1/1 - 0s - 61ms/step - loss: 623762944.0000 - val_loss: 796305792.0000\n",
            "Epoch 11946/20000\n",
            "1/1 - 0s - 68ms/step - loss: 623508416.0000 - val_loss: 796316736.0000\n",
            "Epoch 11947/20000\n",
            "1/1 - 0s - 150ms/step - loss: 623265216.0000 - val_loss: 795529024.0000\n",
            "Epoch 11948/20000\n",
            "1/1 - 0s - 82ms/step - loss: 623011328.0000 - val_loss: 795259776.0000\n",
            "Epoch 11949/20000\n",
            "1/1 - 0s - 70ms/step - loss: 622751872.0000 - val_loss: 794801792.0000\n",
            "Epoch 11950/20000\n",
            "1/1 - 0s - 60ms/step - loss: 622493184.0000 - val_loss: 795183424.0000\n",
            "Epoch 11951/20000\n",
            "1/1 - 0s - 63ms/step - loss: 622230400.0000 - val_loss: 794752128.0000\n",
            "Epoch 11952/20000\n",
            "1/1 - 0s - 139ms/step - loss: 621968960.0000 - val_loss: 794091328.0000\n",
            "Epoch 11953/20000\n",
            "1/1 - 0s - 65ms/step - loss: 621712064.0000 - val_loss: 794451648.0000\n",
            "Epoch 11954/20000\n",
            "1/1 - 0s - 63ms/step - loss: 621448192.0000 - val_loss: 793991680.0000\n",
            "Epoch 11955/20000\n",
            "1/1 - 0s - 61ms/step - loss: 621183808.0000 - val_loss: 792862592.0000\n",
            "Epoch 11956/20000\n",
            "1/1 - 0s - 60ms/step - loss: 620925888.0000 - val_loss: 793524992.0000\n",
            "Epoch 11957/20000\n",
            "1/1 - 0s - 64ms/step - loss: 620685504.0000 - val_loss: 792861440.0000\n",
            "Epoch 11958/20000\n",
            "1/1 - 0s - 65ms/step - loss: 620447744.0000 - val_loss: 792702848.0000\n",
            "Epoch 11959/20000\n",
            "1/1 - 0s - 67ms/step - loss: 620218304.0000 - val_loss: 793012096.0000\n",
            "Epoch 11960/20000\n",
            "1/1 - 0s - 155ms/step - loss: 619987840.0000 - val_loss: 791983360.0000\n",
            "Epoch 11961/20000\n",
            "1/1 - 0s - 115ms/step - loss: 619770048.0000 - val_loss: 793327616.0000\n",
            "Epoch 11962/20000\n",
            "1/1 - 0s - 66ms/step - loss: 619559296.0000 - val_loss: 792003328.0000\n",
            "Epoch 11963/20000\n",
            "1/1 - 0s - 66ms/step - loss: 619338304.0000 - val_loss: 790717376.0000\n",
            "Epoch 11964/20000\n",
            "1/1 - 0s - 62ms/step - loss: 619136448.0000 - val_loss: 792535168.0000\n",
            "Epoch 11965/20000\n",
            "1/1 - 0s - 63ms/step - loss: 618901760.0000 - val_loss: 791867520.0000\n",
            "Epoch 11966/20000\n",
            "1/1 - 0s - 65ms/step - loss: 618671232.0000 - val_loss: 789817344.0000\n",
            "Epoch 11967/20000\n",
            "1/1 - 0s - 62ms/step - loss: 618470144.0000 - val_loss: 790802688.0000\n",
            "Epoch 11968/20000\n",
            "1/1 - 0s - 64ms/step - loss: 618230208.0000 - val_loss: 791913216.0000\n",
            "Epoch 11969/20000\n",
            "1/1 - 0s - 63ms/step - loss: 618013440.0000 - val_loss: 789675136.0000\n",
            "Epoch 11970/20000\n",
            "1/1 - 0s - 61ms/step - loss: 617776960.0000 - val_loss: 789825216.0000\n",
            "Epoch 11971/20000\n",
            "1/1 - 0s - 141ms/step - loss: 617544640.0000 - val_loss: 791281152.0000\n",
            "Epoch 11972/20000\n",
            "1/1 - 0s - 152ms/step - loss: 617326976.0000 - val_loss: 789682112.0000\n",
            "Epoch 11973/20000\n",
            "1/1 - 0s - 69ms/step - loss: 617086080.0000 - val_loss: 789068416.0000\n",
            "Epoch 11974/20000\n",
            "1/1 - 0s - 60ms/step - loss: 616867456.0000 - val_loss: 790444416.0000\n",
            "Epoch 11975/20000\n",
            "1/1 - 0s - 64ms/step - loss: 616654336.0000 - val_loss: 788965376.0000\n",
            "Epoch 11976/20000\n",
            "1/1 - 0s - 64ms/step - loss: 616418496.0000 - val_loss: 788554368.0000\n",
            "Epoch 11977/20000\n",
            "1/1 - 0s - 61ms/step - loss: 616198208.0000 - val_loss: 789644032.0000\n",
            "Epoch 11978/20000\n",
            "1/1 - 0s - 62ms/step - loss: 615978816.0000 - val_loss: 788606720.0000\n",
            "Epoch 11979/20000\n",
            "1/1 - 0s - 62ms/step - loss: 615750976.0000 - val_loss: 788264384.0000\n",
            "Epoch 11980/20000\n",
            "1/1 - 0s - 63ms/step - loss: 615533376.0000 - val_loss: 789186752.0000\n",
            "Epoch 11981/20000\n",
            "1/1 - 0s - 61ms/step - loss: 615324800.0000 - val_loss: 788028928.0000\n",
            "Epoch 11982/20000\n",
            "1/1 - 0s - 142ms/step - loss: 615103488.0000 - val_loss: 787743040.0000\n",
            "Epoch 11983/20000\n",
            "1/1 - 0s - 149ms/step - loss: 614882432.0000 - val_loss: 788682752.0000\n",
            "Epoch 11984/20000\n",
            "1/1 - 0s - 137ms/step - loss: 614662592.0000 - val_loss: 788087488.0000\n",
            "Epoch 11985/20000\n",
            "1/1 - 0s - 64ms/step - loss: 614440448.0000 - val_loss: 787412160.0000\n",
            "Epoch 11986/20000\n",
            "1/1 - 0s - 59ms/step - loss: 614235264.0000 - val_loss: 787725760.0000\n",
            "Epoch 11987/20000\n",
            "1/1 - 0s - 61ms/step - loss: 614027648.0000 - val_loss: 786769344.0000\n",
            "Epoch 11988/20000\n",
            "1/1 - 0s - 60ms/step - loss: 613821056.0000 - val_loss: 787714048.0000\n",
            "Epoch 11989/20000\n",
            "1/1 - 0s - 63ms/step - loss: 613602048.0000 - val_loss: 786749824.0000\n",
            "Epoch 11990/20000\n",
            "1/1 - 0s - 64ms/step - loss: 613386048.0000 - val_loss: 786164928.0000\n",
            "Epoch 11991/20000\n",
            "1/1 - 0s - 65ms/step - loss: 613176384.0000 - val_loss: 786710976.0000\n",
            "Epoch 11992/20000\n",
            "1/1 - 0s - 140ms/step - loss: 612966336.0000 - val_loss: 786519168.0000\n",
            "Epoch 11993/20000\n",
            "1/1 - 0s - 62ms/step - loss: 612756288.0000 - val_loss: 785914176.0000\n",
            "Epoch 11994/20000\n",
            "1/1 - 0s - 66ms/step - loss: 612543808.0000 - val_loss: 786011520.0000\n",
            "Epoch 11995/20000\n",
            "1/1 - 0s - 70ms/step - loss: 612331008.0000 - val_loss: 785938880.0000\n",
            "Epoch 11996/20000\n",
            "1/1 - 0s - 81ms/step - loss: 612120832.0000 - val_loss: 785458432.0000\n",
            "Epoch 11997/20000\n",
            "1/1 - 0s - 132ms/step - loss: 611908864.0000 - val_loss: 785469888.0000\n",
            "Epoch 11998/20000\n",
            "1/1 - 0s - 61ms/step - loss: 611699072.0000 - val_loss: 785479104.0000\n",
            "Epoch 11999/20000\n",
            "1/1 - 0s - 67ms/step - loss: 611496320.0000 - val_loss: 784220864.0000\n",
            "Epoch 12000/20000\n",
            "1/1 - 0s - 63ms/step - loss: 611295744.0000 - val_loss: 785714240.0000\n",
            "Epoch 12001/20000\n",
            "1/1 - 0s - 62ms/step - loss: 611090560.0000 - val_loss: 785057216.0000\n",
            "Epoch 12002/20000\n",
            "1/1 - 0s - 64ms/step - loss: 610880512.0000 - val_loss: 783794944.0000\n",
            "Epoch 12003/20000\n",
            "1/1 - 0s - 61ms/step - loss: 610677056.0000 - val_loss: 784693632.0000\n",
            "Epoch 12004/20000\n",
            "1/1 - 0s - 140ms/step - loss: 610466496.0000 - val_loss: 784743552.0000\n",
            "Epoch 12005/20000\n",
            "1/1 - 0s - 63ms/step - loss: 610261120.0000 - val_loss: 783522048.0000\n",
            "Epoch 12006/20000\n",
            "1/1 - 0s - 63ms/step - loss: 610054464.0000 - val_loss: 784290304.0000\n",
            "Epoch 12007/20000\n",
            "1/1 - 0s - 68ms/step - loss: 609851648.0000 - val_loss: 783707136.0000\n",
            "Epoch 12008/20000\n",
            "1/1 - 0s - 74ms/step - loss: 609648704.0000 - val_loss: 784160256.0000\n",
            "Epoch 12009/20000\n",
            "1/1 - 0s - 146ms/step - loss: 609442944.0000 - val_loss: 783249280.0000\n",
            "Epoch 12010/20000\n",
            "1/1 - 0s - 133ms/step - loss: 609237696.0000 - val_loss: 783332864.0000\n",
            "Epoch 12011/20000\n",
            "1/1 - 0s - 66ms/step - loss: 609033856.0000 - val_loss: 783465216.0000\n",
            "Epoch 12012/20000\n",
            "1/1 - 0s - 61ms/step - loss: 608830528.0000 - val_loss: 782401792.0000\n",
            "Epoch 12013/20000\n",
            "1/1 - 0s - 62ms/step - loss: 608626112.0000 - val_loss: 782615744.0000\n",
            "Epoch 12014/20000\n",
            "1/1 - 0s - 62ms/step - loss: 608421120.0000 - val_loss: 782913408.0000\n",
            "Epoch 12015/20000\n",
            "1/1 - 0s - 64ms/step - loss: 608217792.0000 - val_loss: 782160000.0000\n",
            "Epoch 12016/20000\n",
            "1/1 - 0s - 64ms/step - loss: 608014848.0000 - val_loss: 781831104.0000\n",
            "Epoch 12017/20000\n",
            "1/1 - 0s - 140ms/step - loss: 607814400.0000 - val_loss: 782303104.0000\n",
            "Epoch 12018/20000\n",
            "1/1 - 0s - 79ms/step - loss: 607617216.0000 - val_loss: 781202496.0000\n",
            "Epoch 12019/20000\n",
            "1/1 - 0s - 134ms/step - loss: 607423040.0000 - val_loss: 781778624.0000\n",
            "Epoch 12020/20000\n",
            "1/1 - 0s - 66ms/step - loss: 607214144.0000 - val_loss: 782392256.0000\n",
            "Epoch 12021/20000\n",
            "1/1 - 0s - 75ms/step - loss: 607015424.0000 - val_loss: 781564224.0000\n",
            "Epoch 12022/20000\n",
            "1/1 - 0s - 140ms/step - loss: 606811520.0000 - val_loss: 781388096.0000\n",
            "Epoch 12023/20000\n",
            "1/1 - 0s - 63ms/step - loss: 606611648.0000 - val_loss: 781147776.0000\n",
            "Epoch 12024/20000\n",
            "1/1 - 0s - 84ms/step - loss: 606412928.0000 - val_loss: 780438208.0000\n",
            "Epoch 12025/20000\n",
            "1/1 - 0s - 145ms/step - loss: 606217856.0000 - val_loss: 781659648.0000\n",
            "Epoch 12026/20000\n",
            "1/1 - 0s - 78ms/step - loss: 606017856.0000 - val_loss: 780985984.0000\n",
            "Epoch 12027/20000\n",
            "1/1 - 0s - 161ms/step - loss: 605815296.0000 - val_loss: 780222784.0000\n",
            "Epoch 12028/20000\n",
            "1/1 - 0s - 125ms/step - loss: 605622976.0000 - val_loss: 780373952.0000\n",
            "Epoch 12029/20000\n",
            "1/1 - 0s - 133ms/step - loss: 605416256.0000 - val_loss: 780544000.0000\n",
            "Epoch 12030/20000\n",
            "1/1 - 0s - 143ms/step - loss: 605216896.0000 - val_loss: 780033920.0000\n",
            "Epoch 12031/20000\n",
            "1/1 - 0s - 94ms/step - loss: 605017856.0000 - val_loss: 779507904.0000\n",
            "Epoch 12032/20000\n",
            "1/1 - 0s - 85ms/step - loss: 604826880.0000 - val_loss: 779259840.0000\n",
            "Epoch 12033/20000\n",
            "1/1 - 0s - 151ms/step - loss: 604626496.0000 - val_loss: 779661184.0000\n",
            "Epoch 12034/20000\n",
            "1/1 - 0s - 96ms/step - loss: 604424256.0000 - val_loss: 779722624.0000\n",
            "Epoch 12035/20000\n",
            "1/1 - 0s - 147ms/step - loss: 604224640.0000 - val_loss: 779459520.0000\n",
            "Epoch 12036/20000\n",
            "1/1 - 0s - 104ms/step - loss: 604024320.0000 - val_loss: 779207040.0000\n",
            "Epoch 12037/20000\n",
            "1/1 - 0s - 135ms/step - loss: 603821120.0000 - val_loss: 779437504.0000\n",
            "Epoch 12038/20000\n",
            "1/1 - 0s - 141ms/step - loss: 603628608.0000 - val_loss: 777930752.0000\n",
            "Epoch 12039/20000\n",
            "1/1 - 0s - 96ms/step - loss: 603432192.0000 - val_loss: 778483136.0000\n",
            "Epoch 12040/20000\n",
            "1/1 - 0s - 92ms/step - loss: 603223040.0000 - val_loss: 778545088.0000\n",
            "Epoch 12041/20000\n",
            "1/1 - 0s - 137ms/step - loss: 603027648.0000 - val_loss: 778866624.0000\n",
            "Epoch 12042/20000\n",
            "1/1 - 0s - 137ms/step - loss: 602830848.0000 - val_loss: 778209280.0000\n",
            "Epoch 12043/20000\n",
            "1/1 - 0s - 173ms/step - loss: 602625728.0000 - val_loss: 777705856.0000\n",
            "Epoch 12044/20000\n",
            "1/1 - 0s - 122ms/step - loss: 602424064.0000 - val_loss: 778507712.0000\n",
            "Epoch 12045/20000\n",
            "1/1 - 0s - 99ms/step - loss: 602214912.0000 - val_loss: 778035328.0000\n",
            "Epoch 12046/20000\n",
            "1/1 - 0s - 131ms/step - loss: 602006592.0000 - val_loss: 777409792.0000\n",
            "Epoch 12047/20000\n",
            "1/1 - 0s - 88ms/step - loss: 601801984.0000 - val_loss: 777542016.0000\n",
            "Epoch 12048/20000\n",
            "1/1 - 0s - 130ms/step - loss: 601595200.0000 - val_loss: 777489856.0000\n",
            "Epoch 12049/20000\n",
            "1/1 - 0s - 97ms/step - loss: 601384576.0000 - val_loss: 776991424.0000\n",
            "Epoch 12050/20000\n",
            "1/1 - 0s - 106ms/step - loss: 601185344.0000 - val_loss: 776821568.0000\n",
            "Epoch 12051/20000\n",
            "1/1 - 0s - 86ms/step - loss: 600984000.0000 - val_loss: 776781376.0000\n",
            "Epoch 12052/20000\n",
            "1/1 - 0s - 157ms/step - loss: 600784448.0000 - val_loss: 776551168.0000\n",
            "Epoch 12053/20000\n",
            "1/1 - 0s - 135ms/step - loss: 600589824.0000 - val_loss: 776966656.0000\n",
            "Epoch 12054/20000\n",
            "1/1 - 0s - 106ms/step - loss: 600387840.0000 - val_loss: 775603456.0000\n",
            "Epoch 12055/20000\n",
            "1/1 - 0s - 134ms/step - loss: 600192384.0000 - val_loss: 776237952.0000\n",
            "Epoch 12056/20000\n",
            "1/1 - 0s - 94ms/step - loss: 599997184.0000 - val_loss: 776474176.0000\n",
            "Epoch 12057/20000\n",
            "1/1 - 0s - 90ms/step - loss: 599804544.0000 - val_loss: 775250560.0000\n",
            "Epoch 12058/20000\n",
            "1/1 - 0s - 124ms/step - loss: 599609088.0000 - val_loss: 775877888.0000\n",
            "Epoch 12059/20000\n",
            "1/1 - 0s - 61ms/step - loss: 599407232.0000 - val_loss: 775188608.0000\n",
            "Epoch 12060/20000\n",
            "1/1 - 0s - 146ms/step - loss: 599202880.0000 - val_loss: 775007936.0000\n",
            "Epoch 12061/20000\n",
            "1/1 - 0s - 131ms/step - loss: 599001088.0000 - val_loss: 775396288.0000\n",
            "Epoch 12062/20000\n",
            "1/1 - 0s - 58ms/step - loss: 598804096.0000 - val_loss: 774478784.0000\n",
            "Epoch 12063/20000\n",
            "1/1 - 0s - 63ms/step - loss: 598603776.0000 - val_loss: 774890560.0000\n",
            "Epoch 12064/20000\n",
            "1/1 - 0s - 144ms/step - loss: 598402880.0000 - val_loss: 774623872.0000\n",
            "Epoch 12065/20000\n",
            "1/1 - 0s - 61ms/step - loss: 598196032.0000 - val_loss: 773935744.0000\n",
            "Epoch 12066/20000\n",
            "1/1 - 0s - 61ms/step - loss: 597994816.0000 - val_loss: 774696256.0000\n",
            "Epoch 12067/20000\n",
            "1/1 - 0s - 61ms/step - loss: 597804608.0000 - val_loss: 774077824.0000\n",
            "Epoch 12068/20000\n",
            "1/1 - 0s - 65ms/step - loss: 597611392.0000 - val_loss: 773529536.0000\n",
            "Epoch 12069/20000\n",
            "1/1 - 0s - 136ms/step - loss: 597413952.0000 - val_loss: 774482560.0000\n",
            "Epoch 12070/20000\n",
            "1/1 - 0s - 145ms/step - loss: 597217088.0000 - val_loss: 773214208.0000\n",
            "Epoch 12071/20000\n",
            "1/1 - 0s - 72ms/step - loss: 597026048.0000 - val_loss: 774642240.0000\n",
            "Epoch 12072/20000\n",
            "1/1 - 0s - 61ms/step - loss: 596828288.0000 - val_loss: 772692928.0000\n",
            "Epoch 12073/20000\n",
            "1/1 - 0s - 64ms/step - loss: 596631552.0000 - val_loss: 773160320.0000\n",
            "Epoch 12074/20000\n",
            "1/1 - 0s - 62ms/step - loss: 596432960.0000 - val_loss: 774362624.0000\n",
            "Epoch 12075/20000\n",
            "1/1 - 0s - 61ms/step - loss: 596254912.0000 - val_loss: 771944768.0000\n",
            "Epoch 12076/20000\n",
            "1/1 - 0s - 60ms/step - loss: 596063808.0000 - val_loss: 772700352.0000\n",
            "Epoch 12077/20000\n",
            "1/1 - 0s - 144ms/step - loss: 595855232.0000 - val_loss: 773708608.0000\n",
            "Epoch 12078/20000\n",
            "1/1 - 0s - 62ms/step - loss: 595668480.0000 - val_loss: 771888064.0000\n",
            "Epoch 12079/20000\n",
            "1/1 - 0s - 59ms/step - loss: 595466432.0000 - val_loss: 771656448.0000\n",
            "Epoch 12080/20000\n",
            "1/1 - 0s - 63ms/step - loss: 595271104.0000 - val_loss: 773112384.0000\n",
            "Epoch 12081/20000\n",
            "1/1 - 0s - 138ms/step - loss: 595079744.0000 - val_loss: 771784000.0000\n",
            "Epoch 12082/20000\n",
            "1/1 - 0s - 70ms/step - loss: 594886336.0000 - val_loss: 772090112.0000\n",
            "Epoch 12083/20000\n",
            "1/1 - 0s - 83ms/step - loss: 594692928.0000 - val_loss: 772162624.0000\n",
            "Epoch 12084/20000\n",
            "1/1 - 0s - 70ms/step - loss: 594501120.0000 - val_loss: 771103360.0000\n",
            "Epoch 12085/20000\n",
            "1/1 - 0s - 62ms/step - loss: 594318016.0000 - val_loss: 772003840.0000\n",
            "Epoch 12086/20000\n",
            "1/1 - 0s - 60ms/step - loss: 594123648.0000 - val_loss: 771285760.0000\n",
            "Epoch 12087/20000\n",
            "1/1 - 0s - 66ms/step - loss: 593925120.0000 - val_loss: 770593024.0000\n",
            "Epoch 12088/20000\n",
            "1/1 - 0s - 148ms/step - loss: 593742400.0000 - val_loss: 772267648.0000\n",
            "Epoch 12089/20000\n",
            "1/1 - 0s - 67ms/step - loss: 593556096.0000 - val_loss: 770945024.0000\n",
            "Epoch 12090/20000\n",
            "1/1 - 0s - 138ms/step - loss: 593356032.0000 - val_loss: 770000192.0000\n",
            "Epoch 12091/20000\n",
            "1/1 - 0s - 58ms/step - loss: 593169536.0000 - val_loss: 771103040.0000\n",
            "Epoch 12092/20000\n",
            "1/1 - 0s - 61ms/step - loss: 592974720.0000 - val_loss: 769999360.0000\n",
            "Epoch 12093/20000\n",
            "1/1 - 0s - 64ms/step - loss: 592784320.0000 - val_loss: 770782336.0000\n",
            "Epoch 12094/20000\n",
            "1/1 - 0s - 143ms/step - loss: 592593088.0000 - val_loss: 769232576.0000\n",
            "Epoch 12095/20000\n",
            "1/1 - 0s - 61ms/step - loss: 592401664.0000 - val_loss: 771406144.0000\n",
            "Epoch 12096/20000\n",
            "1/1 - 0s - 62ms/step - loss: 592220992.0000 - val_loss: 769592064.0000\n",
            "Epoch 12097/20000\n",
            "1/1 - 0s - 61ms/step - loss: 592016832.0000 - val_loss: 768957952.0000\n",
            "Epoch 12098/20000\n",
            "1/1 - 0s - 139ms/step - loss: 591827968.0000 - val_loss: 771272576.0000\n",
            "Epoch 12099/20000\n",
            "1/1 - 0s - 62ms/step - loss: 591659136.0000 - val_loss: 768563328.0000\n",
            "Epoch 12100/20000\n",
            "1/1 - 0s - 140ms/step - loss: 591446336.0000 - val_loss: 768623296.0000\n",
            "Epoch 12101/20000\n",
            "1/1 - 0s - 64ms/step - loss: 591244672.0000 - val_loss: 770463424.0000\n",
            "Epoch 12102/20000\n",
            "1/1 - 0s - 61ms/step - loss: 591061312.0000 - val_loss: 768810496.0000\n",
            "Epoch 12103/20000\n",
            "1/1 - 0s - 143ms/step - loss: 590850048.0000 - val_loss: 768028416.0000\n",
            "Epoch 12104/20000\n",
            "1/1 - 0s - 77ms/step - loss: 590655808.0000 - val_loss: 769257280.0000\n",
            "Epoch 12105/20000\n",
            "1/1 - 0s - 131ms/step - loss: 590458944.0000 - val_loss: 767882112.0000\n",
            "Epoch 12106/20000\n",
            "1/1 - 0s - 60ms/step - loss: 590268800.0000 - val_loss: 769917056.0000\n",
            "Epoch 12107/20000\n",
            "1/1 - 0s - 61ms/step - loss: 590086912.0000 - val_loss: 766422912.0000\n",
            "Epoch 12108/20000\n",
            "1/1 - 0s - 62ms/step - loss: 589921344.0000 - val_loss: 770206528.0000\n",
            "Epoch 12109/20000\n",
            "1/1 - 0s - 65ms/step - loss: 589726016.0000 - val_loss: 767963264.0000\n",
            "Epoch 12110/20000\n",
            "1/1 - 0s - 63ms/step - loss: 589500992.0000 - val_loss: 765839552.0000\n",
            "Epoch 12111/20000\n",
            "1/1 - 0s - 62ms/step - loss: 589354944.0000 - val_loss: 768780096.0000\n",
            "Epoch 12112/20000\n",
            "1/1 - 0s - 79ms/step - loss: 589137792.0000 - val_loss: 768620160.0000\n",
            "Epoch 12113/20000\n",
            "1/1 - 0s - 131ms/step - loss: 588947008.0000 - val_loss: 765703360.0000\n",
            "Epoch 12114/20000\n",
            "1/1 - 0s - 64ms/step - loss: 588768896.0000 - val_loss: 767330752.0000\n",
            "Epoch 12115/20000\n",
            "1/1 - 0s - 63ms/step - loss: 588556736.0000 - val_loss: 768184000.0000\n",
            "Epoch 12116/20000\n",
            "1/1 - 0s - 64ms/step - loss: 588385472.0000 - val_loss: 765332032.0000\n",
            "Epoch 12117/20000\n",
            "1/1 - 0s - 77ms/step - loss: 588216896.0000 - val_loss: 767742528.0000\n",
            "Epoch 12118/20000\n",
            "1/1 - 0s - 153ms/step - loss: 588008576.0000 - val_loss: 766409280.0000\n",
            "Epoch 12119/20000\n",
            "1/1 - 0s - 123ms/step - loss: 587802944.0000 - val_loss: 765387456.0000\n",
            "Epoch 12120/20000\n",
            "1/1 - 0s - 67ms/step - loss: 587622016.0000 - val_loss: 766844736.0000\n",
            "Epoch 12121/20000\n",
            "1/1 - 0s - 64ms/step - loss: 587444096.0000 - val_loss: 766158784.0000\n",
            "Epoch 12122/20000\n",
            "1/1 - 0s - 65ms/step - loss: 587250432.0000 - val_loss: 765003776.0000\n",
            "Epoch 12123/20000\n",
            "1/1 - 0s - 139ms/step - loss: 587075456.0000 - val_loss: 766331520.0000\n",
            "Epoch 12124/20000\n",
            "1/1 - 0s - 62ms/step - loss: 586888064.0000 - val_loss: 766279936.0000\n",
            "Epoch 12125/20000\n",
            "1/1 - 0s - 139ms/step - loss: 586711168.0000 - val_loss: 763830336.0000\n",
            "Epoch 12126/20000\n",
            "1/1 - 0s - 64ms/step - loss: 586565184.0000 - val_loss: 766713152.0000\n",
            "Epoch 12127/20000\n",
            "1/1 - 0s - 76ms/step - loss: 586371712.0000 - val_loss: 764870144.0000\n",
            "Epoch 12128/20000\n",
            "1/1 - 0s - 75ms/step - loss: 586170880.0000 - val_loss: 763890752.0000\n",
            "Epoch 12129/20000\n",
            "1/1 - 0s - 64ms/step - loss: 585993984.0000 - val_loss: 765288704.0000\n",
            "Epoch 12130/20000\n",
            "1/1 - 0s - 64ms/step - loss: 585811776.0000 - val_loss: 764572352.0000\n",
            "Epoch 12131/20000\n",
            "1/1 - 0s - 138ms/step - loss: 585624576.0000 - val_loss: 763253376.0000\n",
            "Epoch 12132/20000\n",
            "1/1 - 0s - 61ms/step - loss: 585458752.0000 - val_loss: 765669056.0000\n",
            "Epoch 12133/20000\n",
            "1/1 - 0s - 63ms/step - loss: 585294912.0000 - val_loss: 763316032.0000\n",
            "Epoch 12134/20000\n",
            "1/1 - 0s - 62ms/step - loss: 585091840.0000 - val_loss: 762754112.0000\n",
            "Epoch 12135/20000\n",
            "1/1 - 0s - 140ms/step - loss: 584909952.0000 - val_loss: 764446720.0000\n",
            "Epoch 12136/20000\n",
            "1/1 - 0s - 60ms/step - loss: 584727168.0000 - val_loss: 763435520.0000\n",
            "Epoch 12137/20000\n",
            "1/1 - 0s - 63ms/step - loss: 584541120.0000 - val_loss: 762946432.0000\n",
            "Epoch 12138/20000\n",
            "1/1 - 0s - 66ms/step - loss: 584370176.0000 - val_loss: 764732992.0000\n",
            "Epoch 12139/20000\n",
            "1/1 - 0s - 144ms/step - loss: 584202816.0000 - val_loss: 761382208.0000\n",
            "Epoch 12140/20000\n",
            "1/1 - 0s - 127ms/step - loss: 584039744.0000 - val_loss: 764093056.0000\n",
            "Epoch 12141/20000\n",
            "1/1 - 0s - 61ms/step - loss: 583835584.0000 - val_loss: 763387200.0000\n",
            "Epoch 12142/20000\n",
            "1/1 - 0s - 65ms/step - loss: 583648512.0000 - val_loss: 760516416.0000\n",
            "Epoch 12143/20000\n",
            "1/1 - 0s - 62ms/step - loss: 583496000.0000 - val_loss: 763272832.0000\n",
            "Epoch 12144/20000\n",
            "1/1 - 0s - 61ms/step - loss: 583292736.0000 - val_loss: 762627776.0000\n",
            "Epoch 12145/20000\n",
            "1/1 - 0s - 62ms/step - loss: 583101504.0000 - val_loss: 760389952.0000\n",
            "Epoch 12146/20000\n",
            "1/1 - 0s - 65ms/step - loss: 582946368.0000 - val_loss: 762832384.0000\n",
            "Epoch 12147/20000\n",
            "1/1 - 0s - 139ms/step - loss: 582748992.0000 - val_loss: 761117376.0000\n",
            "Epoch 12148/20000\n",
            "1/1 - 0s - 61ms/step - loss: 582545088.0000 - val_loss: 760961600.0000\n",
            "Epoch 12149/20000\n",
            "1/1 - 0s - 64ms/step - loss: 582353280.0000 - val_loss: 762657664.0000\n",
            "Epoch 12150/20000\n",
            "1/1 - 0s - 140ms/step - loss: 582181312.0000 - val_loss: 759701248.0000\n",
            "Epoch 12151/20000\n",
            "1/1 - 0s - 137ms/step - loss: 581992000.0000 - val_loss: 760876416.0000\n",
            "Epoch 12152/20000\n",
            "1/1 - 0s - 140ms/step - loss: 581780992.0000 - val_loss: 762109120.0000\n",
            "Epoch 12153/20000\n",
            "1/1 - 0s - 63ms/step - loss: 581608768.0000 - val_loss: 759227008.0000\n",
            "Epoch 12154/20000\n",
            "1/1 - 0s - 62ms/step - loss: 581435520.0000 - val_loss: 761169088.0000\n",
            "Epoch 12155/20000\n",
            "1/1 - 0s - 61ms/step - loss: 581214080.0000 - val_loss: 760513408.0000\n",
            "Epoch 12156/20000\n",
            "1/1 - 0s - 63ms/step - loss: 581021824.0000 - val_loss: 759874240.0000\n",
            "Epoch 12157/20000\n",
            "1/1 - 0s - 64ms/step - loss: 580828608.0000 - val_loss: 759790912.0000\n",
            "Epoch 12158/20000\n",
            "1/1 - 0s - 138ms/step - loss: 580642816.0000 - val_loss: 759926080.0000\n",
            "Epoch 12159/20000\n",
            "1/1 - 0s - 59ms/step - loss: 580455168.0000 - val_loss: 759108864.0000\n",
            "Epoch 12160/20000\n",
            "1/1 - 0s - 65ms/step - loss: 580278528.0000 - val_loss: 760004160.0000\n",
            "Epoch 12161/20000\n",
            "1/1 - 0s - 147ms/step - loss: 580091136.0000 - val_loss: 757964224.0000\n",
            "Epoch 12162/20000\n",
            "1/1 - 0s - 66ms/step - loss: 579915584.0000 - val_loss: 759265984.0000\n",
            "Epoch 12163/20000\n",
            "1/1 - 0s - 139ms/step - loss: 579720000.0000 - val_loss: 759453504.0000\n",
            "Epoch 12164/20000\n",
            "1/1 - 0s - 64ms/step - loss: 579539008.0000 - val_loss: 757379200.0000\n",
            "Epoch 12165/20000\n",
            "1/1 - 0s - 60ms/step - loss: 579356224.0000 - val_loss: 758398336.0000\n",
            "Epoch 12166/20000\n",
            "1/1 - 0s - 67ms/step - loss: 579158720.0000 - val_loss: 759541248.0000\n",
            "Epoch 12167/20000\n",
            "1/1 - 0s - 139ms/step - loss: 578991680.0000 - val_loss: 755930944.0000\n",
            "Epoch 12168/20000\n",
            "1/1 - 0s - 79ms/step - loss: 578852416.0000 - val_loss: 759744704.0000\n",
            "Epoch 12169/20000\n",
            "1/1 - 0s - 141ms/step - loss: 578648064.0000 - val_loss: 757833664.0000\n",
            "Epoch 12170/20000\n",
            "1/1 - 0s - 163ms/step - loss: 578428032.0000 - val_loss: 756087296.0000\n",
            "Epoch 12171/20000\n",
            "1/1 - 0s - 106ms/step - loss: 578269056.0000 - val_loss: 759233664.0000\n",
            "Epoch 12172/20000\n",
            "1/1 - 0s - 133ms/step - loss: 578105152.0000 - val_loss: 755789696.0000\n",
            "Epoch 12173/20000\n",
            "1/1 - 0s - 140ms/step - loss: 577912320.0000 - val_loss: 756770368.0000\n",
            "Epoch 12174/20000\n",
            "1/1 - 0s - 84ms/step - loss: 577712640.0000 - val_loss: 758460864.0000\n",
            "Epoch 12175/20000\n",
            "1/1 - 0s - 139ms/step - loss: 577567296.0000 - val_loss: 754313920.0000\n",
            "Epoch 12176/20000\n",
            "1/1 - 0s - 155ms/step - loss: 577412544.0000 - val_loss: 756725952.0000\n",
            "Epoch 12177/20000\n",
            "1/1 - 0s - 141ms/step - loss: 577190016.0000 - val_loss: 757717952.0000\n",
            "Epoch 12178/20000\n",
            "1/1 - 0s - 122ms/step - loss: 577030144.0000 - val_loss: 754884544.0000\n",
            "Epoch 12179/20000\n",
            "1/1 - 0s - 138ms/step - loss: 576856832.0000 - val_loss: 756662784.0000\n",
            "Epoch 12180/20000\n",
            "1/1 - 0s - 84ms/step - loss: 576651584.0000 - val_loss: 756945472.0000\n",
            "Epoch 12181/20000\n",
            "1/1 - 0s - 137ms/step - loss: 576479424.0000 - val_loss: 754198272.0000\n",
            "Epoch 12182/20000\n",
            "1/1 - 0s - 145ms/step - loss: 576317184.0000 - val_loss: 755609280.0000\n",
            "Epoch 12183/20000\n",
            "1/1 - 0s - 154ms/step - loss: 576114816.0000 - val_loss: 756661440.0000\n",
            "Epoch 12184/20000\n",
            "1/1 - 0s - 118ms/step - loss: 575957568.0000 - val_loss: 753920448.0000\n",
            "Epoch 12185/20000\n",
            "1/1 - 0s - 110ms/step - loss: 575787904.0000 - val_loss: 755672128.0000\n",
            "Epoch 12186/20000\n",
            "1/1 - 0s - 132ms/step - loss: 575600960.0000 - val_loss: 754893888.0000\n",
            "Epoch 12187/20000\n",
            "1/1 - 0s - 112ms/step - loss: 575421888.0000 - val_loss: 754509248.0000\n",
            "Epoch 12188/20000\n",
            "1/1 - 0s - 123ms/step - loss: 575252928.0000 - val_loss: 755167552.0000\n",
            "Epoch 12189/20000\n",
            "1/1 - 0s - 103ms/step - loss: 575078528.0000 - val_loss: 753916224.0000\n",
            "Epoch 12190/20000\n",
            "1/1 - 0s - 138ms/step - loss: 574908672.0000 - val_loss: 755394176.0000\n",
            "Epoch 12191/20000\n",
            "1/1 - 0s - 151ms/step - loss: 574739328.0000 - val_loss: 753761600.0000\n",
            "Epoch 12192/20000\n",
            "1/1 - 0s - 131ms/step - loss: 574569088.0000 - val_loss: 754973248.0000\n",
            "Epoch 12193/20000\n",
            "1/1 - 0s - 132ms/step - loss: 574389952.0000 - val_loss: 753749824.0000\n",
            "Epoch 12194/20000\n",
            "1/1 - 0s - 94ms/step - loss: 574214592.0000 - val_loss: 754234944.0000\n",
            "Epoch 12195/20000\n",
            "1/1 - 0s - 141ms/step - loss: 574048512.0000 - val_loss: 753648896.0000\n",
            "Epoch 12196/20000\n",
            "1/1 - 0s - 92ms/step - loss: 573876864.0000 - val_loss: 753814976.0000\n",
            "Epoch 12197/20000\n",
            "1/1 - 0s - 169ms/step - loss: 573700672.0000 - val_loss: 753787584.0000\n",
            "Epoch 12198/20000\n",
            "1/1 - 0s - 100ms/step - loss: 573523968.0000 - val_loss: 753173568.0000\n",
            "Epoch 12199/20000\n",
            "1/1 - 0s - 153ms/step - loss: 573355520.0000 - val_loss: 754229184.0000\n",
            "Epoch 12200/20000\n",
            "1/1 - 0s - 121ms/step - loss: 573189824.0000 - val_loss: 752125248.0000\n",
            "Epoch 12201/20000\n",
            "1/1 - 0s - 67ms/step - loss: 573020736.0000 - val_loss: 754661760.0000\n",
            "Epoch 12202/20000\n",
            "1/1 - 0s - 65ms/step - loss: 572865216.0000 - val_loss: 752406016.0000\n",
            "Epoch 12203/20000\n",
            "1/1 - 0s - 65ms/step - loss: 572678720.0000 - val_loss: 752429952.0000\n",
            "Epoch 12204/20000\n",
            "1/1 - 0s - 63ms/step - loss: 572505152.0000 - val_loss: 753760640.0000\n",
            "Epoch 12205/20000\n",
            "1/1 - 0s - 65ms/step - loss: 572339584.0000 - val_loss: 751534272.0000\n",
            "Epoch 12206/20000\n",
            "1/1 - 0s - 62ms/step - loss: 572183552.0000 - val_loss: 753468160.0000\n",
            "Epoch 12207/20000\n",
            "1/1 - 0s - 139ms/step - loss: 571995520.0000 - val_loss: 752392512.0000\n",
            "Epoch 12208/20000\n",
            "1/1 - 0s - 140ms/step - loss: 571821056.0000 - val_loss: 751751424.0000\n",
            "Epoch 12209/20000\n",
            "1/1 - 0s - 65ms/step - loss: 571655744.0000 - val_loss: 752965120.0000\n",
            "Epoch 12210/20000\n",
            "1/1 - 0s - 64ms/step - loss: 571487616.0000 - val_loss: 751434688.0000\n",
            "Epoch 12211/20000\n",
            "1/1 - 0s - 63ms/step - loss: 571320704.0000 - val_loss: 752484800.0000\n",
            "Epoch 12212/20000\n",
            "1/1 - 0s - 145ms/step - loss: 571148864.0000 - val_loss: 751988416.0000\n",
            "Epoch 12213/20000\n",
            "1/1 - 0s - 128ms/step - loss: 570979648.0000 - val_loss: 751079424.0000\n",
            "Epoch 12214/20000\n",
            "1/1 - 0s - 61ms/step - loss: 570810368.0000 - val_loss: 751811136.0000\n",
            "Epoch 12215/20000\n",
            "1/1 - 0s - 61ms/step - loss: 570640000.0000 - val_loss: 752241920.0000\n",
            "Epoch 12216/20000\n",
            "1/1 - 0s - 140ms/step - loss: 570476160.0000 - val_loss: 750128384.0000\n",
            "Epoch 12217/20000\n",
            "1/1 - 0s - 65ms/step - loss: 570351232.0000 - val_loss: 754126144.0000\n",
            "Epoch 12218/20000\n",
            "1/1 - 0s - 138ms/step - loss: 570224960.0000 - val_loss: 750009024.0000\n",
            "Epoch 12219/20000\n",
            "1/1 - 0s - 144ms/step - loss: 569999744.0000 - val_loss: 750410496.0000\n",
            "Epoch 12220/20000\n",
            "1/1 - 0s - 155ms/step - loss: 569818048.0000 - val_loss: 752245696.0000\n",
            "Epoch 12221/20000\n",
            "1/1 - 0s - 115ms/step - loss: 569673792.0000 - val_loss: 750020416.0000\n",
            "Epoch 12222/20000\n",
            "1/1 - 0s - 139ms/step - loss: 569505216.0000 - val_loss: 751526976.0000\n",
            "Epoch 12223/20000\n",
            "1/1 - 0s - 63ms/step - loss: 569335680.0000 - val_loss: 751087680.0000\n",
            "Epoch 12224/20000\n",
            "1/1 - 0s - 61ms/step - loss: 569161536.0000 - val_loss: 749542720.0000\n",
            "Epoch 12225/20000\n",
            "1/1 - 0s - 63ms/step - loss: 569003328.0000 - val_loss: 751380288.0000\n",
            "Epoch 12226/20000\n",
            "1/1 - 0s - 139ms/step - loss: 568838016.0000 - val_loss: 750493568.0000\n",
            "Epoch 12227/20000\n",
            "1/1 - 0s - 65ms/step - loss: 568663616.0000 - val_loss: 749241600.0000\n",
            "Epoch 12228/20000\n",
            "1/1 - 0s - 64ms/step - loss: 568509504.0000 - val_loss: 751021504.0000\n",
            "Epoch 12229/20000\n",
            "1/1 - 0s - 62ms/step - loss: 568337152.0000 - val_loss: 750425920.0000\n",
            "Epoch 12230/20000\n",
            "1/1 - 0s - 61ms/step - loss: 568172352.0000 - val_loss: 749521472.0000\n",
            "Epoch 12231/20000\n",
            "1/1 - 0s - 168ms/step - loss: 568021824.0000 - val_loss: 751478400.0000\n",
            "Epoch 12232/20000\n",
            "1/1 - 0s - 65ms/step - loss: 567874240.0000 - val_loss: 748426240.0000\n",
            "Epoch 12233/20000\n",
            "1/1 - 0s - 64ms/step - loss: 567715456.0000 - val_loss: 749718976.0000\n",
            "Epoch 12234/20000\n",
            "1/1 - 0s - 141ms/step - loss: 567528832.0000 - val_loss: 750418688.0000\n",
            "Epoch 12235/20000\n",
            "1/1 - 0s - 63ms/step - loss: 567376320.0000 - val_loss: 748443968.0000\n",
            "Epoch 12236/20000\n",
            "1/1 - 0s - 64ms/step - loss: 567222528.0000 - val_loss: 749443840.0000\n",
            "Epoch 12237/20000\n",
            "1/1 - 0s - 64ms/step - loss: 567051136.0000 - val_loss: 750734016.0000\n",
            "Epoch 12238/20000\n",
            "1/1 - 0s - 62ms/step - loss: 566913024.0000 - val_loss: 747321280.0000\n",
            "Epoch 12239/20000\n",
            "1/1 - 0s - 136ms/step - loss: 566764800.0000 - val_loss: 749370944.0000\n",
            "Epoch 12240/20000\n",
            "1/1 - 0s - 62ms/step - loss: 566562304.0000 - val_loss: 749756544.0000\n",
            "Epoch 12241/20000\n",
            "1/1 - 0s - 65ms/step - loss: 566410560.0000 - val_loss: 746879104.0000\n",
            "Epoch 12242/20000\n",
            "1/1 - 0s - 66ms/step - loss: 566275776.0000 - val_loss: 749355392.0000\n",
            "Epoch 12243/20000\n",
            "1/1 - 0s - 79ms/step - loss: 566100544.0000 - val_loss: 748738304.0000\n",
            "Epoch 12244/20000\n",
            "1/1 - 0s - 82ms/step - loss: 565935040.0000 - val_loss: 746666368.0000\n",
            "Epoch 12245/20000\n",
            "1/1 - 0s - 63ms/step - loss: 565798528.0000 - val_loss: 750081344.0000\n",
            "Epoch 12246/20000\n",
            "1/1 - 0s - 60ms/step - loss: 565645632.0000 - val_loss: 747970880.0000\n",
            "Epoch 12247/20000\n",
            "1/1 - 0s - 62ms/step - loss: 565452480.0000 - val_loss: 746908800.0000\n",
            "Epoch 12248/20000\n",
            "1/1 - 0s - 61ms/step - loss: 565301696.0000 - val_loss: 749412608.0000\n",
            "Epoch 12249/20000\n",
            "1/1 - 0s - 59ms/step - loss: 565158464.0000 - val_loss: 747076608.0000\n",
            "Epoch 12250/20000\n",
            "1/1 - 0s - 59ms/step - loss: 564982784.0000 - val_loss: 747432256.0000\n",
            "Epoch 12251/20000\n",
            "1/1 - 0s - 59ms/step - loss: 564816000.0000 - val_loss: 747947648.0000\n",
            "Epoch 12252/20000\n",
            "1/1 - 0s - 60ms/step - loss: 564667776.0000 - val_loss: 746523968.0000\n",
            "Epoch 12253/20000\n",
            "1/1 - 0s - 61ms/step - loss: 564513344.0000 - val_loss: 748005952.0000\n",
            "Epoch 12254/20000\n",
            "1/1 - 0s - 60ms/step - loss: 564351680.0000 - val_loss: 746465600.0000\n",
            "Epoch 12255/20000\n",
            "1/1 - 0s - 59ms/step - loss: 564182336.0000 - val_loss: 747465408.0000\n",
            "Epoch 12256/20000\n",
            "1/1 - 0s - 63ms/step - loss: 564029632.0000 - val_loss: 746324224.0000\n",
            "Epoch 12257/20000\n",
            "1/1 - 0s - 144ms/step - loss: 563866816.0000 - val_loss: 746738240.0000\n",
            "Epoch 12258/20000\n",
            "1/1 - 0s - 130ms/step - loss: 563706816.0000 - val_loss: 747232192.0000\n",
            "Epoch 12259/20000\n",
            "1/1 - 0s - 61ms/step - loss: 563559104.0000 - val_loss: 745067264.0000\n",
            "Epoch 12260/20000\n",
            "1/1 - 0s - 70ms/step - loss: 563413504.0000 - val_loss: 747621120.0000\n",
            "Epoch 12261/20000\n",
            "1/1 - 0s - 61ms/step - loss: 563253568.0000 - val_loss: 746126272.0000\n",
            "Epoch 12262/20000\n",
            "1/1 - 0s - 78ms/step - loss: 563080768.0000 - val_loss: 745298432.0000\n",
            "Epoch 12263/20000\n",
            "1/1 - 0s - 64ms/step - loss: 562936000.0000 - val_loss: 747415360.0000\n",
            "Epoch 12264/20000\n",
            "1/1 - 0s - 64ms/step - loss: 562785984.0000 - val_loss: 745344896.0000\n",
            "Epoch 12265/20000\n",
            "1/1 - 0s - 60ms/step - loss: 562610048.0000 - val_loss: 745568192.0000\n",
            "Epoch 12266/20000\n",
            "1/1 - 0s - 138ms/step - loss: 562450880.0000 - val_loss: 746391360.0000\n",
            "Epoch 12267/20000\n",
            "1/1 - 0s - 60ms/step - loss: 562304000.0000 - val_loss: 744148096.0000\n",
            "Epoch 12268/20000\n",
            "1/1 - 0s - 76ms/step - loss: 562153280.0000 - val_loss: 745555648.0000\n",
            "Epoch 12269/20000\n",
            "1/1 - 0s - 84ms/step - loss: 561978816.0000 - val_loss: 745252992.0000\n",
            "Epoch 12270/20000\n",
            "1/1 - 0s - 74ms/step - loss: 561819776.0000 - val_loss: 744847744.0000\n",
            "Epoch 12271/20000\n",
            "1/1 - 0s - 61ms/step - loss: 561670720.0000 - val_loss: 746604736.0000\n",
            "Epoch 12272/20000\n",
            "1/1 - 0s - 66ms/step - loss: 561528256.0000 - val_loss: 743239808.0000\n",
            "Epoch 12273/20000\n",
            "1/1 - 0s - 62ms/step - loss: 561411264.0000 - val_loss: 746793216.0000\n",
            "Epoch 12274/20000\n",
            "1/1 - 0s - 65ms/step - loss: 561235200.0000 - val_loss: 743962240.0000\n",
            "Epoch 12275/20000\n",
            "1/1 - 0s - 62ms/step - loss: 561057920.0000 - val_loss: 743892224.0000\n",
            "Epoch 12276/20000\n",
            "1/1 - 0s - 60ms/step - loss: 560901632.0000 - val_loss: 745328448.0000\n",
            "Epoch 12277/20000\n",
            "1/1 - 0s - 71ms/step - loss: 560754240.0000 - val_loss: 744287360.0000\n",
            "Epoch 12278/20000\n",
            "1/1 - 0s - 59ms/step - loss: 560588608.0000 - val_loss: 743271680.0000\n",
            "Epoch 12279/20000\n",
            "1/1 - 0s - 60ms/step - loss: 560439232.0000 - val_loss: 745385472.0000\n",
            "Epoch 12280/20000\n",
            "1/1 - 0s - 64ms/step - loss: 560293184.0000 - val_loss: 743495232.0000\n",
            "Epoch 12281/20000\n",
            "1/1 - 0s - 66ms/step - loss: 560119616.0000 - val_loss: 743544000.0000\n",
            "Epoch 12282/20000\n",
            "1/1 - 0s - 142ms/step - loss: 559963840.0000 - val_loss: 743662336.0000\n",
            "Epoch 12283/20000\n",
            "1/1 - 0s - 146ms/step - loss: 559809280.0000 - val_loss: 742787648.0000\n",
            "Epoch 12284/20000\n",
            "1/1 - 0s - 61ms/step - loss: 559659008.0000 - val_loss: 744209664.0000\n",
            "Epoch 12285/20000\n",
            "1/1 - 0s - 64ms/step - loss: 559507904.0000 - val_loss: 742693568.0000\n",
            "Epoch 12286/20000\n",
            "1/1 - 0s - 63ms/step - loss: 559354880.0000 - val_loss: 744456192.0000\n",
            "Epoch 12287/20000\n",
            "1/1 - 0s - 67ms/step - loss: 559203712.0000 - val_loss: 742813184.0000\n",
            "Epoch 12288/20000\n",
            "1/1 - 0s - 65ms/step - loss: 559043072.0000 - val_loss: 742988928.0000\n",
            "Epoch 12289/20000\n",
            "1/1 - 0s - 61ms/step - loss: 558889664.0000 - val_loss: 742326016.0000\n",
            "Epoch 12290/20000\n",
            "1/1 - 0s - 65ms/step - loss: 558738944.0000 - val_loss: 743013120.0000\n",
            "Epoch 12291/20000\n",
            "1/1 - 0s - 63ms/step - loss: 558586240.0000 - val_loss: 741602176.0000\n",
            "Epoch 12292/20000\n",
            "1/1 - 0s - 61ms/step - loss: 558449088.0000 - val_loss: 743975680.0000\n",
            "Epoch 12293/20000\n",
            "1/1 - 0s - 62ms/step - loss: 558308160.0000 - val_loss: 740993280.0000\n",
            "Epoch 12294/20000\n",
            "1/1 - 0s - 72ms/step - loss: 558147264.0000 - val_loss: 742017600.0000\n",
            "Epoch 12295/20000\n",
            "1/1 - 0s - 80ms/step - loss: 557972736.0000 - val_loss: 742798656.0000\n",
            "Epoch 12296/20000\n",
            "1/1 - 0s - 129ms/step - loss: 557827712.0000 - val_loss: 740722048.0000\n",
            "Epoch 12297/20000\n",
            "1/1 - 0s - 82ms/step - loss: 557685568.0000 - val_loss: 742343360.0000\n",
            "Epoch 12298/20000\n",
            "1/1 - 0s - 66ms/step - loss: 557518208.0000 - val_loss: 741623680.0000\n",
            "Epoch 12299/20000\n",
            "1/1 - 0s - 64ms/step - loss: 557360448.0000 - val_loss: 740453504.0000\n",
            "Epoch 12300/20000\n",
            "1/1 - 0s - 139ms/step - loss: 557235648.0000 - val_loss: 743149440.0000\n",
            "Epoch 12301/20000\n",
            "1/1 - 0s - 66ms/step - loss: 557096704.0000 - val_loss: 740010304.0000\n",
            "Epoch 12302/20000\n",
            "1/1 - 0s - 70ms/step - loss: 556933248.0000 - val_loss: 741184384.0000\n",
            "Epoch 12303/20000\n",
            "1/1 - 0s - 64ms/step - loss: 556756160.0000 - val_loss: 741752128.0000\n",
            "Epoch 12304/20000\n",
            "1/1 - 0s - 62ms/step - loss: 556610240.0000 - val_loss: 739239424.0000\n",
            "Epoch 12305/20000\n",
            "1/1 - 0s - 59ms/step - loss: 556503936.0000 - val_loss: 742550720.0000\n",
            "Epoch 12306/20000\n",
            "1/1 - 0s - 64ms/step - loss: 556342016.0000 - val_loss: 740686336.0000\n",
            "Epoch 12307/20000\n",
            "1/1 - 0s - 76ms/step - loss: 556165568.0000 - val_loss: 739330240.0000\n",
            "Epoch 12308/20000\n",
            "1/1 - 0s - 66ms/step - loss: 556022912.0000 - val_loss: 741037312.0000\n",
            "Epoch 12309/20000\n",
            "1/1 - 0s - 61ms/step - loss: 555873856.0000 - val_loss: 740086080.0000\n",
            "Epoch 12310/20000\n",
            "1/1 - 0s - 81ms/step - loss: 555718272.0000 - val_loss: 739285248.0000\n",
            "Epoch 12311/20000\n",
            "1/1 - 0s - 118ms/step - loss: 555573184.0000 - val_loss: 741386240.0000\n",
            "Epoch 12312/20000\n",
            "1/1 - 0s - 63ms/step - loss: 555443904.0000 - val_loss: 739085440.0000\n",
            "Epoch 12313/20000\n",
            "1/1 - 0s - 60ms/step - loss: 555282816.0000 - val_loss: 739638656.0000\n",
            "Epoch 12314/20000\n",
            "1/1 - 0s - 65ms/step - loss: 555128768.0000 - val_loss: 740408320.0000\n",
            "Epoch 12315/20000\n",
            "1/1 - 0s - 61ms/step - loss: 554985920.0000 - val_loss: 738567744.0000\n",
            "Epoch 12316/20000\n",
            "1/1 - 0s - 63ms/step - loss: 554835392.0000 - val_loss: 739162048.0000\n",
            "Epoch 12317/20000\n",
            "1/1 - 0s - 61ms/step - loss: 554683200.0000 - val_loss: 739794048.0000\n",
            "Epoch 12318/20000\n",
            "1/1 - 0s - 62ms/step - loss: 554540160.0000 - val_loss: 737854848.0000\n",
            "Epoch 12319/20000\n",
            "1/1 - 0s - 62ms/step - loss: 554402880.0000 - val_loss: 739953792.0000\n",
            "Epoch 12320/20000\n",
            "1/1 - 0s - 173ms/step - loss: 554249024.0000 - val_loss: 738476672.0000\n",
            "Epoch 12321/20000\n",
            "1/1 - 0s - 119ms/step - loss: 554098304.0000 - val_loss: 738618432.0000\n",
            "Epoch 12322/20000\n",
            "1/1 - 0s - 141ms/step - loss: 553944192.0000 - val_loss: 739428736.0000\n",
            "Epoch 12323/20000\n",
            "1/1 - 0s - 139ms/step - loss: 553800960.0000 - val_loss: 737747264.0000\n",
            "Epoch 12324/20000\n",
            "1/1 - 0s - 82ms/step - loss: 553666240.0000 - val_loss: 740120128.0000\n",
            "Epoch 12325/20000\n",
            "1/1 - 0s - 87ms/step - loss: 553551296.0000 - val_loss: 737341248.0000\n",
            "Epoch 12326/20000\n",
            "1/1 - 0s - 136ms/step - loss: 553381248.0000 - val_loss: 736845504.0000\n",
            "Epoch 12327/20000\n",
            "1/1 - 0s - 141ms/step - loss: 553239424.0000 - val_loss: 738820224.0000\n",
            "Epoch 12328/20000\n",
            "1/1 - 0s - 99ms/step - loss: 553093696.0000 - val_loss: 737526656.0000\n",
            "Epoch 12329/20000\n",
            "1/1 - 0s - 108ms/step - loss: 552937472.0000 - val_loss: 737899712.0000\n",
            "Epoch 12330/20000\n",
            "1/1 - 0s - 101ms/step - loss: 552795264.0000 - val_loss: 738595776.0000\n",
            "Epoch 12331/20000\n",
            "1/1 - 0s - 123ms/step - loss: 552656960.0000 - val_loss: 736210816.0000\n",
            "Epoch 12332/20000\n",
            "1/1 - 0s - 116ms/step - loss: 552528640.0000 - val_loss: 738973696.0000\n",
            "Epoch 12333/20000\n",
            "1/1 - 0s - 93ms/step - loss: 552391168.0000 - val_loss: 735910912.0000\n",
            "Epoch 12334/20000\n",
            "1/1 - 0s - 86ms/step - loss: 552249536.0000 - val_loss: 737678976.0000\n",
            "Epoch 12335/20000\n",
            "1/1 - 0s - 81ms/step - loss: 552090624.0000 - val_loss: 737525824.0000\n",
            "Epoch 12336/20000\n",
            "1/1 - 0s - 170ms/step - loss: 551946944.0000 - val_loss: 735969152.0000\n",
            "Epoch 12337/20000\n",
            "1/1 - 0s - 89ms/step - loss: 551816768.0000 - val_loss: 737254720.0000\n",
            "Epoch 12338/20000\n",
            "1/1 - 0s - 136ms/step - loss: 551665152.0000 - val_loss: 735940480.0000\n",
            "Epoch 12339/20000\n",
            "1/1 - 0s - 85ms/step - loss: 551510784.0000 - val_loss: 736281856.0000\n",
            "Epoch 12340/20000\n",
            "1/1 - 0s - 112ms/step - loss: 551367360.0000 - val_loss: 736261568.0000\n",
            "Epoch 12341/20000\n",
            "1/1 - 0s - 91ms/step - loss: 551218560.0000 - val_loss: 735780352.0000\n",
            "Epoch 12342/20000\n",
            "1/1 - 0s - 143ms/step - loss: 551085184.0000 - val_loss: 736955456.0000\n",
            "Epoch 12343/20000\n",
            "1/1 - 0s - 143ms/step - loss: 550938560.0000 - val_loss: 734447296.0000\n",
            "Epoch 12344/20000\n",
            "1/1 - 0s - 144ms/step - loss: 550809280.0000 - val_loss: 737509120.0000\n",
            "Epoch 12345/20000\n",
            "1/1 - 0s - 125ms/step - loss: 550674112.0000 - val_loss: 734734144.0000\n",
            "Epoch 12346/20000\n",
            "1/1 - 0s - 139ms/step - loss: 550495744.0000 - val_loss: 733984960.0000\n",
            "Epoch 12347/20000\n",
            "1/1 - 0s - 149ms/step - loss: 550353536.0000 - val_loss: 736070400.0000\n",
            "Epoch 12348/20000\n",
            "1/1 - 0s - 128ms/step - loss: 550206656.0000 - val_loss: 734704128.0000\n",
            "Epoch 12349/20000\n",
            "1/1 - 0s - 142ms/step - loss: 550055360.0000 - val_loss: 734336064.0000\n",
            "Epoch 12350/20000\n",
            "1/1 - 0s - 139ms/step - loss: 549909760.0000 - val_loss: 736004480.0000\n",
            "Epoch 12351/20000\n",
            "1/1 - 0s - 147ms/step - loss: 549757632.0000 - val_loss: 734278912.0000\n",
            "Epoch 12352/20000\n",
            "1/1 - 0s - 140ms/step - loss: 549579008.0000 - val_loss: 734122304.0000\n",
            "Epoch 12353/20000\n",
            "1/1 - 0s - 64ms/step - loss: 549426752.0000 - val_loss: 734544064.0000\n",
            "Epoch 12354/20000\n",
            "1/1 - 0s - 137ms/step - loss: 549280896.0000 - val_loss: 733783104.0000\n",
            "Epoch 12355/20000\n",
            "1/1 - 0s - 61ms/step - loss: 549140480.0000 - val_loss: 734326720.0000\n",
            "Epoch 12356/20000\n",
            "1/1 - 0s - 78ms/step - loss: 548989120.0000 - val_loss: 733198080.0000\n",
            "Epoch 12357/20000\n",
            "1/1 - 0s - 60ms/step - loss: 548836416.0000 - val_loss: 734195712.0000\n",
            "Epoch 12358/20000\n",
            "1/1 - 0s - 61ms/step - loss: 548690688.0000 - val_loss: 733073344.0000\n",
            "Epoch 12359/20000\n",
            "1/1 - 0s - 59ms/step - loss: 548536832.0000 - val_loss: 733878656.0000\n",
            "Epoch 12360/20000\n",
            "1/1 - 0s - 60ms/step - loss: 548385920.0000 - val_loss: 732554624.0000\n",
            "Epoch 12361/20000\n",
            "1/1 - 0s - 61ms/step - loss: 548245248.0000 - val_loss: 734181376.0000\n",
            "Epoch 12362/20000\n",
            "1/1 - 0s - 60ms/step - loss: 548098048.0000 - val_loss: 731606144.0000\n",
            "Epoch 12363/20000\n",
            "1/1 - 0s - 77ms/step - loss: 547964352.0000 - val_loss: 734582016.0000\n",
            "Epoch 12364/20000\n",
            "1/1 - 0s - 70ms/step - loss: 547808704.0000 - val_loss: 731636800.0000\n",
            "Epoch 12365/20000\n",
            "1/1 - 0s - 132ms/step - loss: 547629120.0000 - val_loss: 732017920.0000\n",
            "Epoch 12366/20000\n",
            "1/1 - 0s - 66ms/step - loss: 547481600.0000 - val_loss: 733039552.0000\n",
            "Epoch 12367/20000\n",
            "1/1 - 0s - 62ms/step - loss: 547342144.0000 - val_loss: 731012032.0000\n",
            "Epoch 12368/20000\n",
            "1/1 - 0s - 60ms/step - loss: 547187008.0000 - val_loss: 732447296.0000\n",
            "Epoch 12369/20000\n",
            "1/1 - 0s - 63ms/step - loss: 547016256.0000 - val_loss: 732049088.0000\n",
            "Epoch 12370/20000\n",
            "1/1 - 0s - 73ms/step - loss: 546858112.0000 - val_loss: 731396224.0000\n",
            "Epoch 12371/20000\n",
            "1/1 - 0s - 60ms/step - loss: 546713728.0000 - val_loss: 732736256.0000\n",
            "Epoch 12372/20000\n",
            "1/1 - 0s - 64ms/step - loss: 546561152.0000 - val_loss: 731151744.0000\n",
            "Epoch 12373/20000\n",
            "1/1 - 0s - 60ms/step - loss: 546401088.0000 - val_loss: 731427776.0000\n",
            "Epoch 12374/20000\n",
            "1/1 - 0s - 59ms/step - loss: 546241536.0000 - val_loss: 729631808.0000\n",
            "Epoch 12375/20000\n",
            "1/1 - 0s - 64ms/step - loss: 546102784.0000 - val_loss: 732909824.0000\n",
            "Epoch 12376/20000\n",
            "1/1 - 0s - 61ms/step - loss: 545968640.0000 - val_loss: 729909824.0000\n",
            "Epoch 12377/20000\n",
            "1/1 - 0s - 144ms/step - loss: 545762624.0000 - val_loss: 730503872.0000\n",
            "Epoch 12378/20000\n",
            "1/1 - 0s - 61ms/step - loss: 545579904.0000 - val_loss: 731380480.0000\n",
            "Epoch 12379/20000\n",
            "1/1 - 0s - 61ms/step - loss: 545405312.0000 - val_loss: 729524096.0000\n",
            "Epoch 12380/20000\n",
            "1/1 - 0s - 60ms/step - loss: 545231104.0000 - val_loss: 731165376.0000\n",
            "Epoch 12381/20000\n",
            "1/1 - 0s - 139ms/step - loss: 545030848.0000 - val_loss: 730101568.0000\n",
            "Epoch 12382/20000\n",
            "1/1 - 0s - 62ms/step - loss: 544824448.0000 - val_loss: 729681344.0000\n",
            "Epoch 12383/20000\n",
            "1/1 - 0s - 138ms/step - loss: 544618880.0000 - val_loss: 729801792.0000\n",
            "Epoch 12384/20000\n",
            "1/1 - 0s - 61ms/step - loss: 544414784.0000 - val_loss: 729784896.0000\n",
            "Epoch 12385/20000\n",
            "1/1 - 0s - 60ms/step - loss: 544219840.0000 - val_loss: 729276288.0000\n",
            "Epoch 12386/20000\n",
            "1/1 - 0s - 62ms/step - loss: 544031360.0000 - val_loss: 730088256.0000\n",
            "Epoch 12387/20000\n",
            "1/1 - 0s - 63ms/step - loss: 543847616.0000 - val_loss: 728510848.0000\n",
            "Epoch 12388/20000\n",
            "1/1 - 0s - 69ms/step - loss: 543660608.0000 - val_loss: 730664128.0000\n",
            "Epoch 12389/20000\n",
            "1/1 - 0s - 135ms/step - loss: 543502848.0000 - val_loss: 727833536.0000\n",
            "Epoch 12390/20000\n",
            "1/1 - 0s - 59ms/step - loss: 543282240.0000 - val_loss: 727727040.0000\n",
            "Epoch 12391/20000\n",
            "1/1 - 0s - 60ms/step - loss: 543057984.0000 - val_loss: 732007360.0000\n",
            "Epoch 12392/20000\n",
            "1/1 - 0s - 63ms/step - loss: 542931840.0000 - val_loss: 726601472.0000\n",
            "Epoch 12393/20000\n",
            "1/1 - 0s - 67ms/step - loss: 542639936.0000 - val_loss: 727970624.0000\n",
            "Epoch 12394/20000\n",
            "1/1 - 0s - 63ms/step - loss: 542371072.0000 - val_loss: 729727680.0000\n",
            "Epoch 12395/20000\n",
            "1/1 - 0s - 63ms/step - loss: 542152448.0000 - val_loss: 726459840.0000\n",
            "Epoch 12396/20000\n",
            "1/1 - 0s - 71ms/step - loss: 541925504.0000 - val_loss: 729038912.0000\n",
            "Epoch 12397/20000\n",
            "1/1 - 0s - 61ms/step - loss: 541655360.0000 - val_loss: 728133312.0000\n",
            "Epoch 12398/20000\n",
            "1/1 - 0s - 64ms/step - loss: 541401792.0000 - val_loss: 726925632.0000\n",
            "Epoch 12399/20000\n",
            "1/1 - 0s - 59ms/step - loss: 541167744.0000 - val_loss: 726807360.0000\n",
            "Epoch 12400/20000\n",
            "1/1 - 0s - 61ms/step - loss: 540918848.0000 - val_loss: 728810816.0000\n",
            "Epoch 12401/20000\n",
            "1/1 - 0s - 61ms/step - loss: 540681088.0000 - val_loss: 725442240.0000\n",
            "Epoch 12402/20000\n",
            "1/1 - 0s - 146ms/step - loss: 540404864.0000 - val_loss: 727773952.0000\n",
            "Epoch 12403/20000\n",
            "1/1 - 0s - 134ms/step - loss: 540127744.0000 - val_loss: 725205504.0000\n",
            "Epoch 12404/20000\n",
            "1/1 - 0s - 60ms/step - loss: 539848576.0000 - val_loss: 726053440.0000\n",
            "Epoch 12405/20000\n",
            "1/1 - 0s - 65ms/step - loss: 539577856.0000 - val_loss: 725856896.0000\n",
            "Epoch 12406/20000\n",
            "1/1 - 0s - 61ms/step - loss: 539312256.0000 - val_loss: 723836224.0000\n",
            "Epoch 12407/20000\n",
            "1/1 - 0s - 63ms/step - loss: 539064384.0000 - val_loss: 726425216.0000\n",
            "Epoch 12408/20000\n",
            "1/1 - 0s - 60ms/step - loss: 538795904.0000 - val_loss: 723737024.0000\n",
            "Epoch 12409/20000\n",
            "1/1 - 0s - 137ms/step - loss: 538515904.0000 - val_loss: 725104192.0000\n",
            "Epoch 12410/20000\n",
            "1/1 - 0s - 59ms/step - loss: 538250752.0000 - val_loss: 723770368.0000\n",
            "Epoch 12411/20000\n",
            "1/1 - 0s - 61ms/step - loss: 537990976.0000 - val_loss: 724214656.0000\n",
            "Epoch 12412/20000\n",
            "1/1 - 0s - 66ms/step - loss: 537719552.0000 - val_loss: 723482752.0000\n",
            "Epoch 12413/20000\n",
            "1/1 - 0s - 67ms/step - loss: 537442304.0000 - val_loss: 722433728.0000\n",
            "Epoch 12414/20000\n",
            "1/1 - 0s - 74ms/step - loss: 537188672.0000 - val_loss: 722946048.0000\n",
            "Epoch 12415/20000\n",
            "1/1 - 0s - 66ms/step - loss: 536945152.0000 - val_loss: 722785984.0000\n",
            "Epoch 12416/20000\n",
            "1/1 - 0s - 59ms/step - loss: 536686880.0000 - val_loss: 721832960.0000\n",
            "Epoch 12417/20000\n",
            "1/1 - 0s - 61ms/step - loss: 536424832.0000 - val_loss: 722356672.0000\n",
            "Epoch 12418/20000\n",
            "1/1 - 0s - 59ms/step - loss: 536171648.0000 - val_loss: 720616896.0000\n",
            "Epoch 12419/20000\n",
            "1/1 - 0s - 142ms/step - loss: 535944832.0000 - val_loss: 722572544.0000\n",
            "Epoch 12420/20000\n",
            "1/1 - 0s - 59ms/step - loss: 535694496.0000 - val_loss: 721513088.0000\n",
            "Epoch 12421/20000\n",
            "1/1 - 0s - 62ms/step - loss: 535428928.0000 - val_loss: 720466432.0000\n",
            "Epoch 12422/20000\n",
            "1/1 - 0s - 72ms/step - loss: 535163744.0000 - val_loss: 722273344.0000\n",
            "Epoch 12423/20000\n",
            "1/1 - 0s - 59ms/step - loss: 534921408.0000 - val_loss: 718804224.0000\n",
            "Epoch 12424/20000\n",
            "1/1 - 0s - 159ms/step - loss: 534665440.0000 - val_loss: 721849088.0000\n",
            "Epoch 12425/20000\n",
            "1/1 - 0s - 64ms/step - loss: 534395872.0000 - val_loss: 719565312.0000\n",
            "Epoch 12426/20000\n",
            "1/1 - 0s - 82ms/step - loss: 534128128.0000 - val_loss: 718802816.0000\n",
            "Epoch 12427/20000\n",
            "1/1 - 0s - 132ms/step - loss: 533905248.0000 - val_loss: 722297920.0000\n",
            "Epoch 12428/20000\n",
            "1/1 - 0s - 63ms/step - loss: 533727872.0000 - val_loss: 717826048.0000\n",
            "Epoch 12429/20000\n",
            "1/1 - 0s - 63ms/step - loss: 533470976.0000 - val_loss: 718608064.0000\n",
            "Epoch 12430/20000\n",
            "1/1 - 0s - 70ms/step - loss: 533206784.0000 - val_loss: 720815424.0000\n",
            "Epoch 12431/20000\n",
            "1/1 - 0s - 139ms/step - loss: 532990560.0000 - val_loss: 717678208.0000\n",
            "Epoch 12432/20000\n",
            "1/1 - 0s - 62ms/step - loss: 532752064.0000 - val_loss: 719490368.0000\n",
            "Epoch 12433/20000\n",
            "1/1 - 0s - 74ms/step - loss: 532489440.0000 - val_loss: 719380992.0000\n",
            "Epoch 12434/20000\n",
            "1/1 - 0s - 67ms/step - loss: 532227488.0000 - val_loss: 716480448.0000\n",
            "Epoch 12435/20000\n",
            "1/1 - 0s - 64ms/step - loss: 532025088.0000 - val_loss: 720252416.0000\n",
            "Epoch 12436/20000\n",
            "1/1 - 0s - 60ms/step - loss: 531791808.0000 - val_loss: 718289088.0000\n",
            "Epoch 12437/20000\n",
            "1/1 - 0s - 64ms/step - loss: 531525632.0000 - val_loss: 716166016.0000\n",
            "Epoch 12438/20000\n",
            "1/1 - 0s - 139ms/step - loss: 531308800.0000 - val_loss: 719658432.0000\n",
            "Epoch 12439/20000\n",
            "1/1 - 0s - 59ms/step - loss: 531078592.0000 - val_loss: 717093184.0000\n",
            "Epoch 12440/20000\n",
            "1/1 - 0s - 62ms/step - loss: 530783040.0000 - val_loss: 715592640.0000\n",
            "Epoch 12441/20000\n",
            "1/1 - 0s - 138ms/step - loss: 530578208.0000 - val_loss: 719337792.0000\n",
            "Epoch 12442/20000\n",
            "1/1 - 0s - 65ms/step - loss: 530384480.0000 - val_loss: 715879680.0000\n",
            "Epoch 12443/20000\n",
            "1/1 - 0s - 64ms/step - loss: 530117248.0000 - val_loss: 715410432.0000\n",
            "Epoch 12444/20000\n",
            "1/1 - 0s - 63ms/step - loss: 529896768.0000 - val_loss: 718605248.0000\n",
            "Epoch 12445/20000\n",
            "1/1 - 0s - 145ms/step - loss: 529670144.0000 - val_loss: 716057920.0000\n",
            "Epoch 12446/20000\n",
            "1/1 - 0s - 61ms/step - loss: 529418592.0000 - val_loss: 716225152.0000\n",
            "Epoch 12447/20000\n",
            "1/1 - 0s - 60ms/step - loss: 529195168.0000 - val_loss: 717515264.0000\n",
            "Epoch 12448/20000\n",
            "1/1 - 0s - 63ms/step - loss: 528978560.0000 - val_loss: 714791808.0000\n",
            "Epoch 12449/20000\n",
            "1/1 - 0s - 148ms/step - loss: 528765568.0000 - val_loss: 716405440.0000\n",
            "Epoch 12450/20000\n",
            "1/1 - 0s - 126ms/step - loss: 528519744.0000 - val_loss: 716186624.0000\n",
            "Epoch 12451/20000\n",
            "1/1 - 0s - 63ms/step - loss: 528287616.0000 - val_loss: 714358912.0000\n",
            "Epoch 12452/20000\n",
            "1/1 - 0s - 62ms/step - loss: 528064032.0000 - val_loss: 715373952.0000\n",
            "Epoch 12453/20000\n",
            "1/1 - 0s - 63ms/step - loss: 527833408.0000 - val_loss: 714820928.0000\n",
            "Epoch 12454/20000\n",
            "1/1 - 0s - 65ms/step - loss: 527589728.0000 - val_loss: 713625152.0000\n",
            "Epoch 12455/20000\n",
            "1/1 - 0s - 62ms/step - loss: 527351904.0000 - val_loss: 714780544.0000\n",
            "Epoch 12456/20000\n",
            "1/1 - 0s - 60ms/step - loss: 527106784.0000 - val_loss: 714013248.0000\n",
            "Epoch 12457/20000\n",
            "1/1 - 0s - 66ms/step - loss: 526885792.0000 - val_loss: 714938368.0000\n",
            "Epoch 12458/20000\n",
            "1/1 - 0s - 74ms/step - loss: 526653088.0000 - val_loss: 713692544.0000\n",
            "Epoch 12459/20000\n",
            "1/1 - 0s - 133ms/step - loss: 526401248.0000 - val_loss: 713883072.0000\n",
            "Epoch 12460/20000\n",
            "1/1 - 0s - 70ms/step - loss: 526139872.0000 - val_loss: 713606464.0000\n",
            "Epoch 12461/20000\n",
            "1/1 - 0s - 136ms/step - loss: 525882752.0000 - val_loss: 711139648.0000\n",
            "Epoch 12462/20000\n",
            "1/1 - 0s - 63ms/step - loss: 525661024.0000 - val_loss: 713895424.0000\n",
            "Epoch 12463/20000\n",
            "1/1 - 0s - 60ms/step - loss: 525409920.0000 - val_loss: 712466304.0000\n",
            "Epoch 12464/20000\n",
            "1/1 - 0s - 139ms/step - loss: 525145216.0000 - val_loss: 711225216.0000\n",
            "Epoch 12465/20000\n",
            "1/1 - 0s - 73ms/step - loss: 524918880.0000 - val_loss: 713692032.0000\n",
            "Epoch 12466/20000\n",
            "1/1 - 0s - 65ms/step - loss: 524708960.0000 - val_loss: 710701760.0000\n",
            "Epoch 12467/20000\n",
            "1/1 - 0s - 135ms/step - loss: 524444416.0000 - val_loss: 711337728.0000\n",
            "Epoch 12468/20000\n",
            "1/1 - 0s - 75ms/step - loss: 524189856.0000 - val_loss: 711789632.0000\n",
            "Epoch 12469/20000\n",
            "1/1 - 0s - 129ms/step - loss: 523969088.0000 - val_loss: 709745472.0000\n",
            "Epoch 12470/20000\n",
            "1/1 - 0s - 67ms/step - loss: 523756576.0000 - val_loss: 712263232.0000\n",
            "Epoch 12471/20000\n",
            "1/1 - 0s - 128ms/step - loss: 523538880.0000 - val_loss: 709297792.0000\n",
            "Epoch 12472/20000\n",
            "1/1 - 0s - 69ms/step - loss: 523294944.0000 - val_loss: 710763328.0000\n",
            "Epoch 12473/20000\n",
            "1/1 - 0s - 152ms/step - loss: 523057568.0000 - val_loss: 709707328.0000\n",
            "Epoch 12474/20000\n",
            "1/1 - 0s - 158ms/step - loss: 522818368.0000 - val_loss: 709072448.0000\n",
            "Epoch 12475/20000\n",
            "1/1 - 0s - 114ms/step - loss: 522577312.0000 - val_loss: 710176192.0000\n",
            "Epoch 12476/20000\n",
            "1/1 - 0s - 99ms/step - loss: 522344128.0000 - val_loss: 708685504.0000\n",
            "Epoch 12477/20000\n",
            "1/1 - 0s - 123ms/step - loss: 522093504.0000 - val_loss: 709500928.0000\n",
            "Epoch 12478/20000\n",
            "1/1 - 0s - 137ms/step - loss: 521846400.0000 - val_loss: 708437824.0000\n",
            "Epoch 12479/20000\n",
            "1/1 - 0s - 110ms/step - loss: 521603328.0000 - val_loss: 709257152.0000\n",
            "Epoch 12480/20000\n",
            "1/1 - 0s - 139ms/step - loss: 521363072.0000 - val_loss: 707953984.0000\n",
            "Epoch 12481/20000\n",
            "1/1 - 0s - 147ms/step - loss: 521111808.0000 - val_loss: 708774592.0000\n",
            "Epoch 12482/20000\n",
            "1/1 - 0s - 104ms/step - loss: 520862496.0000 - val_loss: 706722560.0000\n",
            "Epoch 12483/20000\n",
            "1/1 - 0s - 84ms/step - loss: 520642464.0000 - val_loss: 709217920.0000\n",
            "Epoch 12484/20000\n",
            "1/1 - 0s - 147ms/step - loss: 520427296.0000 - val_loss: 706719232.0000\n",
            "Epoch 12485/20000\n",
            "1/1 - 0s - 122ms/step - loss: 520164160.0000 - val_loss: 707258560.0000\n",
            "Epoch 12486/20000\n",
            "1/1 - 0s - 125ms/step - loss: 519916224.0000 - val_loss: 707827520.0000\n",
            "Epoch 12487/20000\n",
            "1/1 - 0s - 116ms/step - loss: 519686656.0000 - val_loss: 706203328.0000\n",
            "Epoch 12488/20000\n",
            "1/1 - 0s - 95ms/step - loss: 519471200.0000 - val_loss: 708664832.0000\n",
            "Epoch 12489/20000\n",
            "1/1 - 0s - 94ms/step - loss: 519257152.0000 - val_loss: 706119104.0000\n",
            "Epoch 12490/20000\n",
            "1/1 - 0s - 145ms/step - loss: 519007040.0000 - val_loss: 706606016.0000\n",
            "Epoch 12491/20000\n",
            "1/1 - 0s - 128ms/step - loss: 518769952.0000 - val_loss: 705852224.0000\n",
            "Epoch 12492/20000\n",
            "1/1 - 0s - 90ms/step - loss: 518533472.0000 - val_loss: 705445312.0000\n",
            "Epoch 12493/20000\n",
            "1/1 - 0s - 149ms/step - loss: 518301664.0000 - val_loss: 705918336.0000\n",
            "Epoch 12494/20000\n",
            "1/1 - 0s - 155ms/step - loss: 518065824.0000 - val_loss: 705394240.0000\n",
            "Epoch 12495/20000\n",
            "1/1 - 0s - 86ms/step - loss: 517837632.0000 - val_loss: 706092928.0000\n",
            "Epoch 12496/20000\n",
            "1/1 - 0s - 98ms/step - loss: 517612768.0000 - val_loss: 704189696.0000\n",
            "Epoch 12497/20000\n",
            "1/1 - 0s - 142ms/step - loss: 517420320.0000 - val_loss: 706564992.0000\n",
            "Epoch 12498/20000\n",
            "1/1 - 0s - 136ms/step - loss: 517210304.0000 - val_loss: 704812288.0000\n",
            "Epoch 12499/20000\n",
            "1/1 - 0s - 138ms/step - loss: 516970144.0000 - val_loss: 703228416.0000\n",
            "Epoch 12500/20000\n",
            "1/1 - 0s - 167ms/step - loss: 516770912.0000 - val_loss: 706123392.0000\n",
            "Epoch 12501/20000\n",
            "1/1 - 0s - 114ms/step - loss: 516584640.0000 - val_loss: 702731328.0000\n",
            "Epoch 12502/20000\n",
            "1/1 - 0s - 146ms/step - loss: 516347552.0000 - val_loss: 703279808.0000\n",
            "Epoch 12503/20000\n",
            "1/1 - 0s - 116ms/step - loss: 516125120.0000 - val_loss: 705049088.0000\n",
            "Epoch 12504/20000\n",
            "1/1 - 0s - 79ms/step - loss: 515942848.0000 - val_loss: 702535296.0000\n",
            "Epoch 12505/20000\n",
            "1/1 - 0s - 62ms/step - loss: 515753440.0000 - val_loss: 704436992.0000\n",
            "Epoch 12506/20000\n",
            "1/1 - 0s - 143ms/step - loss: 515554976.0000 - val_loss: 703563456.0000\n",
            "Epoch 12507/20000\n",
            "1/1 - 0s - 62ms/step - loss: 515354560.0000 - val_loss: 702372736.0000\n",
            "Epoch 12508/20000\n",
            "1/1 - 0s - 57ms/step - loss: 515186400.0000 - val_loss: 703943168.0000\n",
            "Epoch 12509/20000\n",
            "1/1 - 0s - 60ms/step - loss: 515011808.0000 - val_loss: 702878016.0000\n",
            "Epoch 12510/20000\n",
            "1/1 - 0s - 62ms/step - loss: 514828320.0000 - val_loss: 701916608.0000\n",
            "Epoch 12511/20000\n",
            "1/1 - 0s - 61ms/step - loss: 514659232.0000 - val_loss: 704307904.0000\n",
            "Epoch 12512/20000\n",
            "1/1 - 0s - 65ms/step - loss: 514533536.0000 - val_loss: 700443904.0000\n",
            "Epoch 12513/20000\n",
            "1/1 - 0s - 145ms/step - loss: 514365216.0000 - val_loss: 702081984.0000\n",
            "Epoch 12514/20000\n",
            "1/1 - 0s - 63ms/step - loss: 514173856.0000 - val_loss: 703636480.0000\n",
            "Epoch 12515/20000\n",
            "1/1 - 0s - 62ms/step - loss: 514048672.0000 - val_loss: 699769984.0000\n",
            "Epoch 12516/20000\n",
            "1/1 - 0s - 61ms/step - loss: 513856032.0000 - val_loss: 701010048.0000\n",
            "Epoch 12517/20000\n",
            "1/1 - 0s - 61ms/step - loss: 513654592.0000 - val_loss: 702675968.0000\n",
            "Epoch 12518/20000\n",
            "1/1 - 0s - 60ms/step - loss: 513514592.0000 - val_loss: 700140160.0000\n",
            "Epoch 12519/20000\n",
            "1/1 - 0s - 61ms/step - loss: 513374240.0000 - val_loss: 702703808.0000\n",
            "Epoch 12520/20000\n",
            "1/1 - 0s - 64ms/step - loss: 513204672.0000 - val_loss: 701080640.0000\n",
            "Epoch 12521/20000\n",
            "1/1 - 0s - 60ms/step - loss: 513020576.0000 - val_loss: 700015872.0000\n",
            "Epoch 12522/20000\n",
            "1/1 - 0s - 60ms/step - loss: 512874144.0000 - val_loss: 701559552.0000\n",
            "Epoch 12523/20000\n",
            "1/1 - 0s - 66ms/step - loss: 512729664.0000 - val_loss: 699859648.0000\n",
            "Epoch 12524/20000\n",
            "1/1 - 0s - 137ms/step - loss: 512565600.0000 - val_loss: 699494720.0000\n",
            "Epoch 12525/20000\n",
            "1/1 - 0s - 62ms/step - loss: 512414912.0000 - val_loss: 701480064.0000\n",
            "Epoch 12526/20000\n",
            "1/1 - 0s - 149ms/step - loss: 512285472.0000 - val_loss: 698250816.0000\n",
            "Epoch 12527/20000\n",
            "1/1 - 0s - 66ms/step - loss: 512123168.0000 - val_loss: 700493312.0000\n",
            "Epoch 12528/20000\n",
            "1/1 - 0s - 61ms/step - loss: 511959232.0000 - val_loss: 699621824.0000\n",
            "Epoch 12529/20000\n",
            "1/1 - 0s - 59ms/step - loss: 511792512.0000 - val_loss: 697841280.0000\n",
            "Epoch 12530/20000\n",
            "1/1 - 0s - 142ms/step - loss: 511661824.0000 - val_loss: 700232768.0000\n",
            "Epoch 12531/20000\n",
            "1/1 - 0s - 65ms/step - loss: 511515424.0000 - val_loss: 698469248.0000\n",
            "Epoch 12532/20000\n",
            "1/1 - 0s - 63ms/step - loss: 511353216.0000 - val_loss: 698051648.0000\n",
            "Epoch 12533/20000\n",
            "1/1 - 0s - 67ms/step - loss: 511209312.0000 - val_loss: 701072256.0000\n",
            "Epoch 12534/20000\n",
            "1/1 - 0s - 58ms/step - loss: 511126528.0000 - val_loss: 696356608.0000\n",
            "Epoch 12535/20000\n",
            "1/1 - 0s - 58ms/step - loss: 510962688.0000 - val_loss: 698700096.0000\n",
            "Epoch 12536/20000\n",
            "1/1 - 0s - 59ms/step - loss: 510764832.0000 - val_loss: 698297856.0000\n",
            "Epoch 12537/20000\n",
            "1/1 - 0s - 58ms/step - loss: 510613440.0000 - val_loss: 696914432.0000\n",
            "Epoch 12538/20000\n",
            "1/1 - 0s - 68ms/step - loss: 510474112.0000 - val_loss: 698268800.0000\n",
            "Epoch 12539/20000\n",
            "1/1 - 0s - 137ms/step - loss: 510331232.0000 - val_loss: 696697728.0000\n",
            "Epoch 12540/20000\n",
            "1/1 - 0s - 137ms/step - loss: 510201120.0000 - val_loss: 698215488.0000\n",
            "Epoch 12541/20000\n",
            "1/1 - 0s - 59ms/step - loss: 510064992.0000 - val_loss: 697182016.0000\n",
            "Epoch 12542/20000\n",
            "1/1 - 0s - 61ms/step - loss: 509920352.0000 - val_loss: 697224896.0000\n",
            "Epoch 12543/20000\n",
            "1/1 - 0s - 141ms/step - loss: 509771232.0000 - val_loss: 696976832.0000\n",
            "Epoch 12544/20000\n",
            "1/1 - 0s - 63ms/step - loss: 509628576.0000 - val_loss: 696807232.0000\n",
            "Epoch 12545/20000\n",
            "1/1 - 0s - 135ms/step - loss: 509488320.0000 - val_loss: 695485120.0000\n",
            "Epoch 12546/20000\n",
            "1/1 - 0s - 65ms/step - loss: 509366368.0000 - val_loss: 697911296.0000\n",
            "Epoch 12547/20000\n",
            "1/1 - 0s - 138ms/step - loss: 509247424.0000 - val_loss: 694665536.0000\n",
            "Epoch 12548/20000\n",
            "1/1 - 0s - 78ms/step - loss: 509124352.0000 - val_loss: 697520896.0000\n",
            "Epoch 12549/20000\n",
            "1/1 - 0s - 77ms/step - loss: 508973984.0000 - val_loss: 696058944.0000\n",
            "Epoch 12550/20000\n",
            "1/1 - 0s - 64ms/step - loss: 508803520.0000 - val_loss: 694482496.0000\n",
            "Epoch 12551/20000\n",
            "1/1 - 0s - 136ms/step - loss: 508676352.0000 - val_loss: 697711616.0000\n",
            "Epoch 12552/20000\n",
            "1/1 - 0s - 63ms/step - loss: 508576672.0000 - val_loss: 694593536.0000\n",
            "Epoch 12553/20000\n",
            "1/1 - 0s - 62ms/step - loss: 508416064.0000 - val_loss: 695068096.0000\n",
            "Epoch 12554/20000\n",
            "1/1 - 0s - 65ms/step - loss: 508272000.0000 - val_loss: 697087808.0000\n",
            "Epoch 12555/20000\n",
            "1/1 - 0s - 68ms/step - loss: 508163488.0000 - val_loss: 694539712.0000\n",
            "Epoch 12556/20000\n",
            "1/1 - 0s - 60ms/step - loss: 508009952.0000 - val_loss: 694835200.0000\n",
            "Epoch 12557/20000\n",
            "1/1 - 0s - 60ms/step - loss: 507860352.0000 - val_loss: 696063168.0000\n",
            "Epoch 12558/20000\n",
            "1/1 - 0s - 59ms/step - loss: 507726336.0000 - val_loss: 694203584.0000\n",
            "Epoch 12559/20000\n",
            "1/1 - 0s - 61ms/step - loss: 507588960.0000 - val_loss: 695442560.0000\n",
            "Epoch 12560/20000\n",
            "1/1 - 0s - 62ms/step - loss: 507443136.0000 - val_loss: 695739264.0000\n",
            "Epoch 12561/20000\n",
            "1/1 - 0s - 152ms/step - loss: 507317408.0000 - val_loss: 694030528.0000\n",
            "Epoch 12562/20000\n",
            "1/1 - 0s - 124ms/step - loss: 507188800.0000 - val_loss: 695331520.0000\n",
            "Epoch 12563/20000\n",
            "1/1 - 0s - 138ms/step - loss: 507058240.0000 - val_loss: 694389312.0000\n",
            "Epoch 12564/20000\n",
            "1/1 - 0s - 61ms/step - loss: 506921056.0000 - val_loss: 692842432.0000\n",
            "Epoch 12565/20000\n",
            "1/1 - 0s - 61ms/step - loss: 506813984.0000 - val_loss: 696341184.0000\n",
            "Epoch 12566/20000\n",
            "1/1 - 0s - 140ms/step - loss: 506704640.0000 - val_loss: 693510272.0000\n",
            "Epoch 12567/20000\n",
            "1/1 - 0s - 141ms/step - loss: 506532192.0000 - val_loss: 693505920.0000\n",
            "Epoch 12568/20000\n",
            "1/1 - 0s - 60ms/step - loss: 506393216.0000 - val_loss: 695292800.0000\n",
            "Epoch 12569/20000\n",
            "1/1 - 0s - 58ms/step - loss: 506269952.0000 - val_loss: 693009152.0000\n",
            "Epoch 12570/20000\n",
            "1/1 - 0s - 142ms/step - loss: 506141568.0000 - val_loss: 695476736.0000\n",
            "Epoch 12571/20000\n",
            "1/1 - 0s - 62ms/step - loss: 506017824.0000 - val_loss: 692846912.0000\n",
            "Epoch 12572/20000\n",
            "1/1 - 0s - 132ms/step - loss: 505869984.0000 - val_loss: 693820608.0000\n",
            "Epoch 12573/20000\n",
            "1/1 - 0s - 64ms/step - loss: 505720096.0000 - val_loss: 693534080.0000\n",
            "Epoch 12574/20000\n",
            "1/1 - 0s - 138ms/step - loss: 505588256.0000 - val_loss: 693314048.0000\n",
            "Epoch 12575/20000\n",
            "1/1 - 0s - 68ms/step - loss: 505452864.0000 - val_loss: 694165568.0000\n",
            "Epoch 12576/20000\n",
            "1/1 - 0s - 62ms/step - loss: 505313216.0000 - val_loss: 692116480.0000\n",
            "Epoch 12577/20000\n",
            "1/1 - 0s - 63ms/step - loss: 505183136.0000 - val_loss: 695172800.0000\n",
            "Epoch 12578/20000\n",
            "1/1 - 0s - 140ms/step - loss: 505069376.0000 - val_loss: 692117760.0000\n",
            "Epoch 12579/20000\n",
            "1/1 - 0s - 146ms/step - loss: 504879232.0000 - val_loss: 691757376.0000\n",
            "Epoch 12580/20000\n",
            "1/1 - 0s - 134ms/step - loss: 504743904.0000 - val_loss: 694998720.0000\n",
            "Epoch 12581/20000\n",
            "1/1 - 0s - 69ms/step - loss: 504651520.0000 - val_loss: 692161152.0000\n",
            "Epoch 12582/20000\n",
            "1/1 - 0s - 149ms/step - loss: 504470976.0000 - val_loss: 691684224.0000\n",
            "Epoch 12583/20000\n",
            "1/1 - 0s - 61ms/step - loss: 504338464.0000 - val_loss: 694601536.0000\n",
            "Epoch 12584/20000\n",
            "1/1 - 0s - 63ms/step - loss: 504214464.0000 - val_loss: 691852928.0000\n",
            "Epoch 12585/20000\n",
            "1/1 - 0s - 64ms/step - loss: 504075072.0000 - val_loss: 693294912.0000\n",
            "Epoch 12586/20000\n",
            "1/1 - 0s - 60ms/step - loss: 503910240.0000 - val_loss: 693420096.0000\n",
            "Epoch 12587/20000\n",
            "1/1 - 0s - 64ms/step - loss: 503773504.0000 - val_loss: 690905088.0000\n",
            "Epoch 12588/20000\n",
            "1/1 - 0s - 61ms/step - loss: 503667680.0000 - val_loss: 694153024.0000\n",
            "Epoch 12589/20000\n",
            "1/1 - 0s - 63ms/step - loss: 503551680.0000 - val_loss: 691324928.0000\n",
            "Epoch 12590/20000\n",
            "1/1 - 0s - 143ms/step - loss: 503414048.0000 - val_loss: 691265088.0000\n",
            "Epoch 12591/20000\n",
            "1/1 - 0s - 142ms/step - loss: 503299808.0000 - val_loss: 692375872.0000\n",
            "Epoch 12592/20000\n",
            "1/1 - 0s - 65ms/step - loss: 503178336.0000 - val_loss: 691753152.0000\n",
            "Epoch 12593/20000\n",
            "1/1 - 0s - 78ms/step - loss: 503058688.0000 - val_loss: 691323008.0000\n",
            "Epoch 12594/20000\n",
            "1/1 - 0s - 62ms/step - loss: 502928192.0000 - val_loss: 692026752.0000\n",
            "Epoch 12595/20000\n",
            "1/1 - 0s - 140ms/step - loss: 502791616.0000 - val_loss: 690861888.0000\n",
            "Epoch 12596/20000\n",
            "1/1 - 0s - 67ms/step - loss: 502658528.0000 - val_loss: 691610176.0000\n",
            "Epoch 12597/20000\n",
            "1/1 - 0s - 63ms/step - loss: 502525312.0000 - val_loss: 691189952.0000\n",
            "Epoch 12598/20000\n",
            "1/1 - 0s - 62ms/step - loss: 502402880.0000 - val_loss: 690307200.0000\n",
            "Epoch 12599/20000\n",
            "1/1 - 0s - 61ms/step - loss: 502305312.0000 - val_loss: 691128000.0000\n",
            "Epoch 12600/20000\n",
            "1/1 - 0s - 63ms/step - loss: 502184480.0000 - val_loss: 691030784.0000\n",
            "Epoch 12601/20000\n",
            "1/1 - 0s - 63ms/step - loss: 502053952.0000 - val_loss: 689263168.0000\n",
            "Epoch 12602/20000\n",
            "1/1 - 0s - 143ms/step - loss: 501940704.0000 - val_loss: 691679296.0000\n",
            "Epoch 12603/20000\n",
            "1/1 - 0s - 71ms/step - loss: 501821984.0000 - val_loss: 689556992.0000\n",
            "Epoch 12604/20000\n",
            "1/1 - 0s - 60ms/step - loss: 501679808.0000 - val_loss: 689790272.0000\n",
            "Epoch 12605/20000\n",
            "1/1 - 0s - 70ms/step - loss: 501548640.0000 - val_loss: 690718592.0000\n",
            "Epoch 12606/20000\n",
            "1/1 - 0s - 138ms/step - loss: 501437248.0000 - val_loss: 689601280.0000\n",
            "Epoch 12607/20000\n",
            "1/1 - 0s - 61ms/step - loss: 501323520.0000 - val_loss: 690461632.0000\n",
            "Epoch 12608/20000\n",
            "1/1 - 0s - 135ms/step - loss: 501204864.0000 - val_loss: 689152704.0000\n",
            "Epoch 12609/20000\n",
            "1/1 - 0s - 138ms/step - loss: 501081120.0000 - val_loss: 689684992.0000\n",
            "Epoch 12610/20000\n",
            "1/1 - 0s - 139ms/step - loss: 500956032.0000 - val_loss: 689478976.0000\n",
            "Epoch 12611/20000\n",
            "1/1 - 0s - 144ms/step - loss: 500833088.0000 - val_loss: 688600896.0000\n",
            "Epoch 12612/20000\n",
            "1/1 - 0s - 63ms/step - loss: 500711904.0000 - val_loss: 689064512.0000\n",
            "Epoch 12613/20000\n",
            "1/1 - 0s - 60ms/step - loss: 500592288.0000 - val_loss: 689183104.0000\n",
            "Epoch 12614/20000\n",
            "1/1 - 0s - 61ms/step - loss: 500470016.0000 - val_loss: 689260096.0000\n",
            "Epoch 12615/20000\n",
            "1/1 - 0s - 86ms/step - loss: 500345568.0000 - val_loss: 687864960.0000\n",
            "Epoch 12616/20000\n",
            "1/1 - 0s - 137ms/step - loss: 500240320.0000 - val_loss: 690075520.0000\n",
            "Epoch 12617/20000\n",
            "1/1 - 0s - 101ms/step - loss: 500144832.0000 - val_loss: 687403904.0000\n",
            "Epoch 12618/20000\n",
            "1/1 - 0s - 104ms/step - loss: 500017088.0000 - val_loss: 688534400.0000\n",
            "Epoch 12619/20000\n",
            "1/1 - 0s - 117ms/step - loss: 499882976.0000 - val_loss: 688440320.0000\n",
            "Epoch 12620/20000\n",
            "1/1 - 0s - 81ms/step - loss: 499765504.0000 - val_loss: 687802752.0000\n",
            "Epoch 12621/20000\n",
            "1/1 - 0s - 145ms/step - loss: 499649984.0000 - val_loss: 688375040.0000\n",
            "Epoch 12622/20000\n",
            "1/1 - 0s - 146ms/step - loss: 499527584.0000 - val_loss: 687609984.0000\n",
            "Epoch 12623/20000\n",
            "1/1 - 0s - 113ms/step - loss: 499401856.0000 - val_loss: 687786752.0000\n",
            "Epoch 12624/20000\n",
            "1/1 - 0s - 112ms/step - loss: 499277152.0000 - val_loss: 688093376.0000\n",
            "Epoch 12625/20000\n",
            "1/1 - 0s - 143ms/step - loss: 499159968.0000 - val_loss: 687251456.0000\n",
            "Epoch 12626/20000\n",
            "1/1 - 0s - 139ms/step - loss: 499048160.0000 - val_loss: 687474240.0000\n",
            "Epoch 12627/20000\n",
            "1/1 - 0s - 137ms/step - loss: 498932608.0000 - val_loss: 687745408.0000\n",
            "Epoch 12628/20000\n",
            "1/1 - 0s - 161ms/step - loss: 498809920.0000 - val_loss: 686682112.0000\n",
            "Epoch 12629/20000\n",
            "1/1 - 0s - 128ms/step - loss: 498711232.0000 - val_loss: 689520256.0000\n",
            "Epoch 12630/20000\n",
            "1/1 - 0s - 121ms/step - loss: 498640864.0000 - val_loss: 685078720.0000\n",
            "Epoch 12631/20000\n",
            "1/1 - 0s - 124ms/step - loss: 498536256.0000 - val_loss: 688096960.0000\n",
            "Epoch 12632/20000\n",
            "1/1 - 0s - 137ms/step - loss: 498386144.0000 - val_loss: 686830848.0000\n",
            "Epoch 12633/20000\n",
            "1/1 - 0s - 104ms/step - loss: 498251264.0000 - val_loss: 685912384.0000\n",
            "Epoch 12634/20000\n",
            "1/1 - 0s - 84ms/step - loss: 498144640.0000 - val_loss: 687645888.0000\n",
            "Epoch 12635/20000\n",
            "1/1 - 0s - 140ms/step - loss: 498038848.0000 - val_loss: 685729728.0000\n",
            "Epoch 12636/20000\n",
            "1/1 - 0s - 144ms/step - loss: 497925280.0000 - val_loss: 687051520.0000\n",
            "Epoch 12637/20000\n",
            "1/1 - 0s - 126ms/step - loss: 497792096.0000 - val_loss: 686642240.0000\n",
            "Epoch 12638/20000\n",
            "1/1 - 0s - 82ms/step - loss: 497673248.0000 - val_loss: 685675584.0000\n",
            "Epoch 12639/20000\n",
            "1/1 - 0s - 136ms/step - loss: 497560000.0000 - val_loss: 686617856.0000\n",
            "Epoch 12640/20000\n",
            "1/1 - 0s - 111ms/step - loss: 497452064.0000 - val_loss: 686316672.0000\n",
            "Epoch 12641/20000\n",
            "1/1 - 0s - 131ms/step - loss: 497339840.0000 - val_loss: 684752768.0000\n",
            "Epoch 12642/20000\n",
            "1/1 - 0s - 139ms/step - loss: 497253152.0000 - val_loss: 687647232.0000\n",
            "Epoch 12643/20000\n",
            "1/1 - 0s - 148ms/step - loss: 497144928.0000 - val_loss: 684027072.0000\n",
            "Epoch 12644/20000\n",
            "1/1 - 0s - 113ms/step - loss: 497048640.0000 - val_loss: 686567168.0000\n",
            "Epoch 12645/20000\n",
            "1/1 - 0s - 116ms/step - loss: 496902912.0000 - val_loss: 686952576.0000\n",
            "Epoch 12646/20000\n",
            "1/1 - 0s - 93ms/step - loss: 496807104.0000 - val_loss: 682993088.0000\n",
            "Epoch 12647/20000\n",
            "1/1 - 0s - 125ms/step - loss: 496766592.0000 - val_loss: 687246080.0000\n",
            "Epoch 12648/20000\n",
            "1/1 - 0s - 67ms/step - loss: 496610752.0000 - val_loss: 686018112.0000\n",
            "Epoch 12649/20000\n",
            "1/1 - 0s - 68ms/step - loss: 496468064.0000 - val_loss: 683099520.0000\n",
            "Epoch 12650/20000\n",
            "1/1 - 0s - 65ms/step - loss: 496398688.0000 - val_loss: 686580992.0000\n",
            "Epoch 12651/20000\n",
            "1/1 - 0s - 63ms/step - loss: 496260480.0000 - val_loss: 685041216.0000\n",
            "Epoch 12652/20000\n",
            "1/1 - 0s - 62ms/step - loss: 496112928.0000 - val_loss: 682924032.0000\n",
            "Epoch 12653/20000\n",
            "1/1 - 0s - 140ms/step - loss: 496025984.0000 - val_loss: 686762560.0000\n",
            "Epoch 12654/20000\n",
            "1/1 - 0s - 160ms/step - loss: 495941664.0000 - val_loss: 683587392.0000\n",
            "Epoch 12655/20000\n",
            "1/1 - 0s - 116ms/step - loss: 495787552.0000 - val_loss: 684498176.0000\n",
            "Epoch 12656/20000\n",
            "1/1 - 0s - 142ms/step - loss: 495662592.0000 - val_loss: 685066048.0000\n",
            "Epoch 12657/20000\n",
            "1/1 - 0s - 61ms/step - loss: 495551616.0000 - val_loss: 683579968.0000\n",
            "Epoch 12658/20000\n",
            "1/1 - 0s - 63ms/step - loss: 495442144.0000 - val_loss: 685165056.0000\n",
            "Epoch 12659/20000\n",
            "1/1 - 0s - 142ms/step - loss: 495342880.0000 - val_loss: 683063360.0000\n",
            "Epoch 12660/20000\n",
            "1/1 - 0s - 62ms/step - loss: 495225728.0000 - val_loss: 684255424.0000\n",
            "Epoch 12661/20000\n",
            "1/1 - 0s - 61ms/step - loss: 495099968.0000 - val_loss: 684957824.0000\n",
            "Epoch 12662/20000\n",
            "1/1 - 0s - 60ms/step - loss: 494997312.0000 - val_loss: 682477120.0000\n",
            "Epoch 12663/20000\n",
            "1/1 - 0s - 63ms/step - loss: 494892256.0000 - val_loss: 684316864.0000\n",
            "Epoch 12664/20000\n",
            "1/1 - 0s - 149ms/step - loss: 494769536.0000 - val_loss: 683213504.0000\n",
            "Epoch 12665/20000\n",
            "1/1 - 0s - 132ms/step - loss: 494662176.0000 - val_loss: 683490432.0000\n",
            "Epoch 12666/20000\n",
            "1/1 - 0s - 63ms/step - loss: 494550400.0000 - val_loss: 684024192.0000\n",
            "Epoch 12667/20000\n",
            "1/1 - 0s - 60ms/step - loss: 494434368.0000 - val_loss: 682382144.0000\n",
            "Epoch 12668/20000\n",
            "1/1 - 0s - 139ms/step - loss: 494315360.0000 - val_loss: 683305152.0000\n",
            "Epoch 12669/20000\n",
            "1/1 - 0s - 60ms/step - loss: 494203072.0000 - val_loss: 682425088.0000\n",
            "Epoch 12670/20000\n",
            "1/1 - 0s - 140ms/step - loss: 494100576.0000 - val_loss: 683792640.0000\n",
            "Epoch 12671/20000\n",
            "1/1 - 0s - 60ms/step - loss: 493984832.0000 - val_loss: 682405760.0000\n",
            "Epoch 12672/20000\n",
            "1/1 - 0s - 63ms/step - loss: 493860992.0000 - val_loss: 683299712.0000\n",
            "Epoch 12673/20000\n",
            "1/1 - 0s - 141ms/step - loss: 493746816.0000 - val_loss: 682181632.0000\n",
            "Epoch 12674/20000\n",
            "1/1 - 0s - 72ms/step - loss: 493634112.0000 - val_loss: 682338240.0000\n",
            "Epoch 12675/20000\n",
            "1/1 - 0s - 132ms/step - loss: 493518752.0000 - val_loss: 683010624.0000\n",
            "Epoch 12676/20000\n",
            "1/1 - 0s - 63ms/step - loss: 493410144.0000 - val_loss: 681586496.0000\n",
            "Epoch 12677/20000\n",
            "1/1 - 0s - 62ms/step - loss: 493295008.0000 - val_loss: 682092096.0000\n",
            "Epoch 12678/20000\n",
            "1/1 - 0s - 62ms/step - loss: 493177760.0000 - val_loss: 681711616.0000\n",
            "Epoch 12679/20000\n",
            "1/1 - 0s - 60ms/step - loss: 493063232.0000 - val_loss: 682172288.0000\n",
            "Epoch 12680/20000\n",
            "1/1 - 0s - 139ms/step - loss: 492950656.0000 - val_loss: 682787008.0000\n",
            "Epoch 12681/20000\n",
            "1/1 - 0s - 139ms/step - loss: 492845760.0000 - val_loss: 680995776.0000\n",
            "Epoch 12682/20000\n",
            "1/1 - 0s - 63ms/step - loss: 492720672.0000 - val_loss: 681797056.0000\n",
            "Epoch 12683/20000\n",
            "1/1 - 0s - 63ms/step - loss: 492596864.0000 - val_loss: 681157376.0000\n",
            "Epoch 12684/20000\n",
            "1/1 - 0s - 65ms/step - loss: 492488768.0000 - val_loss: 681840128.0000\n",
            "Epoch 12685/20000\n",
            "1/1 - 0s - 73ms/step - loss: 492367808.0000 - val_loss: 681816192.0000\n",
            "Epoch 12686/20000\n",
            "1/1 - 0s - 73ms/step - loss: 492245440.0000 - val_loss: 679976128.0000\n",
            "Epoch 12687/20000\n",
            "1/1 - 0s - 131ms/step - loss: 492166528.0000 - val_loss: 683162560.0000\n",
            "Epoch 12688/20000\n",
            "1/1 - 0s - 67ms/step - loss: 492058848.0000 - val_loss: 681289984.0000\n",
            "Epoch 12689/20000\n",
            "1/1 - 0s - 65ms/step - loss: 491904640.0000 - val_loss: 679743360.0000\n",
            "Epoch 12690/20000\n",
            "1/1 - 0s - 59ms/step - loss: 491802176.0000 - val_loss: 682078592.0000\n",
            "Epoch 12691/20000\n",
            "1/1 - 0s - 62ms/step - loss: 491677760.0000 - val_loss: 680003264.0000\n",
            "Epoch 12692/20000\n",
            "1/1 - 0s - 66ms/step - loss: 491556768.0000 - val_loss: 680515008.0000\n",
            "Epoch 12693/20000\n",
            "1/1 - 0s - 62ms/step - loss: 491435968.0000 - val_loss: 681411584.0000\n",
            "Epoch 12694/20000\n",
            "1/1 - 0s - 136ms/step - loss: 491321472.0000 - val_loss: 680147840.0000\n",
            "Epoch 12695/20000\n",
            "1/1 - 0s - 61ms/step - loss: 491208864.0000 - val_loss: 681584256.0000\n",
            "Epoch 12696/20000\n",
            "1/1 - 0s - 60ms/step - loss: 491094464.0000 - val_loss: 679636992.0000\n",
            "Epoch 12697/20000\n",
            "1/1 - 0s - 163ms/step - loss: 490995040.0000 - val_loss: 681125952.0000\n",
            "Epoch 12698/20000\n",
            "1/1 - 0s - 74ms/step - loss: 490880288.0000 - val_loss: 680232768.0000\n",
            "Epoch 12699/20000\n",
            "1/1 - 0s - 64ms/step - loss: 490755552.0000 - val_loss: 679144960.0000\n",
            "Epoch 12700/20000\n",
            "1/1 - 0s - 59ms/step - loss: 490659840.0000 - val_loss: 681810432.0000\n",
            "Epoch 12701/20000\n",
            "1/1 - 0s - 64ms/step - loss: 490572000.0000 - val_loss: 679509120.0000\n",
            "Epoch 12702/20000\n",
            "1/1 - 0s - 137ms/step - loss: 490427968.0000 - val_loss: 678899648.0000\n",
            "Epoch 12703/20000\n",
            "1/1 - 0s - 139ms/step - loss: 490327904.0000 - val_loss: 681064384.0000\n",
            "Epoch 12704/20000\n",
            "1/1 - 0s - 136ms/step - loss: 490230912.0000 - val_loss: 678873536.0000\n",
            "Epoch 12705/20000\n",
            "1/1 - 0s - 68ms/step - loss: 490108768.0000 - val_loss: 679363264.0000\n",
            "Epoch 12706/20000\n",
            "1/1 - 0s - 144ms/step - loss: 489980864.0000 - val_loss: 681352640.0000\n",
            "Epoch 12707/20000\n",
            "1/1 - 0s - 72ms/step - loss: 489897824.0000 - val_loss: 677598528.0000\n",
            "Epoch 12708/20000\n",
            "1/1 - 0s - 135ms/step - loss: 489809696.0000 - val_loss: 680431296.0000\n",
            "Epoch 12709/20000\n",
            "1/1 - 0s - 65ms/step - loss: 489667648.0000 - val_loss: 680080576.0000\n",
            "Epoch 12710/20000\n",
            "1/1 - 0s - 60ms/step - loss: 489553824.0000 - val_loss: 678198784.0000\n",
            "Epoch 12711/20000\n",
            "1/1 - 0s - 60ms/step - loss: 489456192.0000 - val_loss: 679853312.0000\n",
            "Epoch 12712/20000\n",
            "1/1 - 0s - 60ms/step - loss: 489343488.0000 - val_loss: 678663680.0000\n",
            "Epoch 12713/20000\n",
            "1/1 - 0s - 62ms/step - loss: 489225856.0000 - val_loss: 678150336.0000\n",
            "Epoch 12714/20000\n",
            "1/1 - 0s - 60ms/step - loss: 489119104.0000 - val_loss: 679540352.0000\n",
            "Epoch 12715/20000\n",
            "1/1 - 0s - 59ms/step - loss: 489010656.0000 - val_loss: 678135232.0000\n",
            "Epoch 12716/20000\n",
            "1/1 - 0s - 142ms/step - loss: 488902496.0000 - val_loss: 678816896.0000\n",
            "Epoch 12717/20000\n",
            "1/1 - 0s - 66ms/step - loss: 488793888.0000 - val_loss: 680119616.0000\n",
            "Epoch 12718/20000\n",
            "1/1 - 0s - 65ms/step - loss: 488704640.0000 - val_loss: 676872064.0000\n",
            "Epoch 12719/20000\n",
            "1/1 - 0s - 76ms/step - loss: 488628320.0000 - val_loss: 679493952.0000\n",
            "Epoch 12720/20000\n",
            "1/1 - 0s - 78ms/step - loss: 488481824.0000 - val_loss: 678929216.0000\n",
            "Epoch 12721/20000\n",
            "1/1 - 0s - 60ms/step - loss: 488364736.0000 - val_loss: 676950208.0000\n",
            "Epoch 12722/20000\n",
            "1/1 - 0s - 61ms/step - loss: 488282240.0000 - val_loss: 679227968.0000\n",
            "Epoch 12723/20000\n",
            "1/1 - 0s - 144ms/step - loss: 488173632.0000 - val_loss: 678009152.0000\n",
            "Epoch 12724/20000\n",
            "1/1 - 0s - 64ms/step - loss: 488056128.0000 - val_loss: 677876352.0000\n",
            "Epoch 12725/20000\n",
            "1/1 - 0s - 61ms/step - loss: 487953920.0000 - val_loss: 677454144.0000\n",
            "Epoch 12726/20000\n",
            "1/1 - 0s - 60ms/step - loss: 487849312.0000 - val_loss: 677491584.0000\n",
            "Epoch 12727/20000\n",
            "1/1 - 0s - 61ms/step - loss: 487742912.0000 - val_loss: 677983296.0000\n",
            "Epoch 12728/20000\n",
            "1/1 - 0s - 60ms/step - loss: 487632160.0000 - val_loss: 676924288.0000\n",
            "Epoch 12729/20000\n",
            "1/1 - 0s - 75ms/step - loss: 487527616.0000 - val_loss: 677899712.0000\n",
            "Epoch 12730/20000\n",
            "1/1 - 0s - 133ms/step - loss: 487421600.0000 - val_loss: 677338624.0000\n",
            "Epoch 12731/20000\n",
            "1/1 - 0s - 140ms/step - loss: 487320352.0000 - val_loss: 677964928.0000\n",
            "Epoch 12732/20000\n",
            "1/1 - 0s - 65ms/step - loss: 487213312.0000 - val_loss: 676763264.0000\n",
            "Epoch 12733/20000\n",
            "1/1 - 0s - 59ms/step - loss: 487110880.0000 - val_loss: 676969600.0000\n",
            "Epoch 12734/20000\n",
            "1/1 - 0s - 62ms/step - loss: 487004576.0000 - val_loss: 677310336.0000\n",
            "Epoch 12735/20000\n",
            "1/1 - 0s - 64ms/step - loss: 486909824.0000 - val_loss: 676384896.0000\n",
            "Epoch 12736/20000\n",
            "1/1 - 0s - 61ms/step - loss: 486798912.0000 - val_loss: 676606912.0000\n",
            "Epoch 12737/20000\n",
            "1/1 - 0s - 64ms/step - loss: 486688640.0000 - val_loss: 676798976.0000\n",
            "Epoch 12738/20000\n",
            "1/1 - 0s - 67ms/step - loss: 486583264.0000 - val_loss: 676063104.0000\n",
            "Epoch 12739/20000\n",
            "1/1 - 0s - 141ms/step - loss: 486481920.0000 - val_loss: 676486144.0000\n",
            "Epoch 12740/20000\n",
            "1/1 - 0s - 67ms/step - loss: 486384000.0000 - val_loss: 676977920.0000\n",
            "Epoch 12741/20000\n",
            "1/1 - 0s - 75ms/step - loss: 486276544.0000 - val_loss: 675123584.0000\n",
            "Epoch 12742/20000\n",
            "1/1 - 0s - 61ms/step - loss: 486181472.0000 - val_loss: 677514752.0000\n",
            "Epoch 12743/20000\n",
            "1/1 - 0s - 72ms/step - loss: 486102112.0000 - val_loss: 674941632.0000\n",
            "Epoch 12744/20000\n",
            "1/1 - 0s - 76ms/step - loss: 485973568.0000 - val_loss: 674222528.0000\n",
            "Epoch 12745/20000\n",
            "1/1 - 0s - 65ms/step - loss: 485889728.0000 - val_loss: 677959104.0000\n",
            "Epoch 12746/20000\n",
            "1/1 - 0s - 64ms/step - loss: 485823776.0000 - val_loss: 674772736.0000\n",
            "Epoch 12747/20000\n",
            "1/1 - 0s - 64ms/step - loss: 485681408.0000 - val_loss: 674608256.0000\n",
            "Epoch 12748/20000\n",
            "1/1 - 0s - 139ms/step - loss: 485579552.0000 - val_loss: 676877888.0000\n",
            "Epoch 12749/20000\n",
            "1/1 - 0s - 61ms/step - loss: 485474976.0000 - val_loss: 675044544.0000\n",
            "Epoch 12750/20000\n",
            "1/1 - 0s - 61ms/step - loss: 485346688.0000 - val_loss: 674222592.0000\n",
            "Epoch 12751/20000\n",
            "1/1 - 0s - 58ms/step - loss: 485259616.0000 - val_loss: 676178368.0000\n",
            "Epoch 12752/20000\n",
            "1/1 - 0s - 60ms/step - loss: 485166560.0000 - val_loss: 674114304.0000\n",
            "Epoch 12753/20000\n",
            "1/1 - 0s - 66ms/step - loss: 485047840.0000 - val_loss: 673673728.0000\n",
            "Epoch 12754/20000\n",
            "1/1 - 0s - 139ms/step - loss: 484943552.0000 - val_loss: 675898880.0000\n",
            "Epoch 12755/20000\n",
            "1/1 - 0s - 67ms/step - loss: 484851872.0000 - val_loss: 674017344.0000\n",
            "Epoch 12756/20000\n",
            "1/1 - 0s - 72ms/step - loss: 484745216.0000 - val_loss: 674884416.0000\n",
            "Epoch 12757/20000\n",
            "1/1 - 0s - 132ms/step - loss: 484637952.0000 - val_loss: 675201152.0000\n",
            "Epoch 12758/20000\n",
            "1/1 - 0s - 61ms/step - loss: 484542720.0000 - val_loss: 673316288.0000\n",
            "Epoch 12759/20000\n",
            "1/1 - 0s - 63ms/step - loss: 484444768.0000 - val_loss: 674294848.0000\n",
            "Epoch 12760/20000\n",
            "1/1 - 0s - 61ms/step - loss: 484338368.0000 - val_loss: 673746496.0000\n",
            "Epoch 12761/20000\n",
            "1/1 - 0s - 96ms/step - loss: 484243392.0000 - val_loss: 673583296.0000\n",
            "Epoch 12762/20000\n",
            "1/1 - 0s - 152ms/step - loss: 484138816.0000 - val_loss: 673667136.0000\n",
            "Epoch 12763/20000\n",
            "1/1 - 0s - 145ms/step - loss: 484031552.0000 - val_loss: 674435008.0000\n",
            "Epoch 12764/20000\n",
            "1/1 - 0s - 106ms/step - loss: 483927808.0000 - val_loss: 672187712.0000\n",
            "Epoch 12765/20000\n",
            "1/1 - 0s - 143ms/step - loss: 483854304.0000 - val_loss: 675151232.0000\n",
            "Epoch 12766/20000\n",
            "1/1 - 0s - 122ms/step - loss: 483750080.0000 - val_loss: 672633280.0000\n",
            "Epoch 12767/20000\n",
            "1/1 - 0s - 80ms/step - loss: 483625280.0000 - val_loss: 673141056.0000\n",
            "Epoch 12768/20000\n",
            "1/1 - 0s - 138ms/step - loss: 483510528.0000 - val_loss: 673520256.0000\n",
            "Epoch 12769/20000\n",
            "1/1 - 0s - 139ms/step - loss: 483409440.0000 - val_loss: 671826048.0000\n",
            "Epoch 12770/20000\n",
            "1/1 - 0s - 131ms/step - loss: 483322944.0000 - val_loss: 673992128.0000\n",
            "Epoch 12771/20000\n",
            "1/1 - 0s - 159ms/step - loss: 483222752.0000 - val_loss: 672155584.0000\n",
            "Epoch 12772/20000\n",
            "1/1 - 0s - 146ms/step - loss: 483109984.0000 - val_loss: 673251776.0000\n",
            "Epoch 12773/20000\n",
            "1/1 - 0s - 104ms/step - loss: 482996992.0000 - val_loss: 672118656.0000\n",
            "Epoch 12774/20000\n",
            "1/1 - 0s - 117ms/step - loss: 482888224.0000 - val_loss: 672291968.0000\n",
            "Epoch 12775/20000\n",
            "1/1 - 0s - 86ms/step - loss: 482785728.0000 - val_loss: 672309952.0000\n",
            "Epoch 12776/20000\n",
            "1/1 - 0s - 101ms/step - loss: 482681600.0000 - val_loss: 672211008.0000\n",
            "Epoch 12777/20000\n",
            "1/1 - 0s - 138ms/step - loss: 482577952.0000 - val_loss: 672809856.0000\n",
            "Epoch 12778/20000\n",
            "1/1 - 0s - 108ms/step - loss: 482481920.0000 - val_loss: 670914944.0000\n",
            "Epoch 12779/20000\n",
            "1/1 - 0s - 138ms/step - loss: 482396768.0000 - val_loss: 673664128.0000\n",
            "Epoch 12780/20000\n",
            "1/1 - 0s - 173ms/step - loss: 482318304.0000 - val_loss: 670779584.0000\n",
            "Epoch 12781/20000\n",
            "1/1 - 0s - 132ms/step - loss: 482176576.0000 - val_loss: 670706816.0000\n",
            "Epoch 12782/20000\n",
            "1/1 - 0s - 101ms/step - loss: 482070880.0000 - val_loss: 673117888.0000\n",
            "Epoch 12783/20000\n",
            "1/1 - 0s - 140ms/step - loss: 481995008.0000 - val_loss: 670674624.0000\n",
            "Epoch 12784/20000\n",
            "1/1 - 0s - 130ms/step - loss: 481882272.0000 - val_loss: 671360256.0000\n",
            "Epoch 12785/20000\n",
            "1/1 - 0s - 155ms/step - loss: 481765920.0000 - val_loss: 671785088.0000\n",
            "Epoch 12786/20000\n",
            "1/1 - 0s - 140ms/step - loss: 481667232.0000 - val_loss: 670158720.0000\n",
            "Epoch 12787/20000\n",
            "1/1 - 0s - 113ms/step - loss: 481572544.0000 - val_loss: 671624512.0000\n",
            "Epoch 12788/20000\n",
            "1/1 - 0s - 127ms/step - loss: 481458528.0000 - val_loss: 671102592.0000\n",
            "Epoch 12789/20000\n",
            "1/1 - 0s - 128ms/step - loss: 481354592.0000 - val_loss: 670948544.0000\n",
            "Epoch 12790/20000\n",
            "1/1 - 0s - 104ms/step - loss: 481252672.0000 - val_loss: 670589888.0000\n",
            "Epoch 12791/20000\n",
            "1/1 - 0s - 150ms/step - loss: 481155584.0000 - val_loss: 670674880.0000\n",
            "Epoch 12792/20000\n",
            "1/1 - 0s - 104ms/step - loss: 481063104.0000 - val_loss: 670690688.0000\n",
            "Epoch 12793/20000\n",
            "1/1 - 0s - 62ms/step - loss: 480970432.0000 - val_loss: 669705216.0000\n",
            "Epoch 12794/20000\n",
            "1/1 - 0s - 63ms/step - loss: 480872480.0000 - val_loss: 670975808.0000\n",
            "Epoch 12795/20000\n",
            "1/1 - 0s - 69ms/step - loss: 480777440.0000 - val_loss: 669622400.0000\n",
            "Epoch 12796/20000\n",
            "1/1 - 0s - 138ms/step - loss: 480679200.0000 - val_loss: 670304640.0000\n",
            "Epoch 12797/20000\n",
            "1/1 - 0s - 132ms/step - loss: 480580704.0000 - val_loss: 670594688.0000\n",
            "Epoch 12798/20000\n",
            "1/1 - 0s - 62ms/step - loss: 480485920.0000 - val_loss: 669371072.0000\n",
            "Epoch 12799/20000\n",
            "1/1 - 0s - 58ms/step - loss: 480388800.0000 - val_loss: 669955776.0000\n",
            "Epoch 12800/20000\n",
            "1/1 - 0s - 63ms/step - loss: 480281696.0000 - val_loss: 669698112.0000\n",
            "Epoch 12801/20000\n",
            "1/1 - 0s - 61ms/step - loss: 480185152.0000 - val_loss: 669619648.0000\n",
            "Epoch 12802/20000\n",
            "1/1 - 0s - 64ms/step - loss: 480092736.0000 - val_loss: 670563840.0000\n",
            "Epoch 12803/20000\n",
            "1/1 - 0s - 62ms/step - loss: 480005024.0000 - val_loss: 668426112.0000\n",
            "Epoch 12804/20000\n",
            "1/1 - 0s - 141ms/step - loss: 479912384.0000 - val_loss: 670012160.0000\n",
            "Epoch 12805/20000\n",
            "1/1 - 0s - 139ms/step - loss: 479797152.0000 - val_loss: 668992256.0000\n",
            "Epoch 12806/20000\n",
            "1/1 - 0s - 61ms/step - loss: 479698016.0000 - val_loss: 668219008.0000\n",
            "Epoch 12807/20000\n",
            "1/1 - 0s - 144ms/step - loss: 479621280.0000 - val_loss: 670788480.0000\n",
            "Epoch 12808/20000\n",
            "1/1 - 0s - 100ms/step - loss: 479551968.0000 - val_loss: 668147136.0000\n",
            "Epoch 12809/20000\n",
            "1/1 - 0s - 66ms/step - loss: 479422240.0000 - val_loss: 667909056.0000\n",
            "Epoch 12810/20000\n",
            "1/1 - 0s - 61ms/step - loss: 479319840.0000 - val_loss: 669702912.0000\n",
            "Epoch 12811/20000\n",
            "1/1 - 0s - 59ms/step - loss: 479234592.0000 - val_loss: 667984128.0000\n",
            "Epoch 12812/20000\n",
            "1/1 - 0s - 62ms/step - loss: 479142272.0000 - val_loss: 668913600.0000\n",
            "Epoch 12813/20000\n",
            "1/1 - 0s - 62ms/step - loss: 479035232.0000 - val_loss: 669229248.0000\n",
            "Epoch 12814/20000\n",
            "1/1 - 0s - 60ms/step - loss: 478936448.0000 - val_loss: 666839872.0000\n",
            "Epoch 12815/20000\n",
            "1/1 - 0s - 64ms/step - loss: 478876192.0000 - val_loss: 669935744.0000\n",
            "Epoch 12816/20000\n",
            "1/1 - 0s - 58ms/step - loss: 478780288.0000 - val_loss: 667937280.0000\n",
            "Epoch 12817/20000\n",
            "1/1 - 0s - 58ms/step - loss: 478646848.0000 - val_loss: 666056448.0000\n",
            "Epoch 12818/20000\n",
            "1/1 - 0s - 60ms/step - loss: 478594464.0000 - val_loss: 669881408.0000\n",
            "Epoch 12819/20000\n",
            "1/1 - 0s - 138ms/step - loss: 478518048.0000 - val_loss: 667220480.0000\n",
            "Epoch 12820/20000\n",
            "1/1 - 0s - 69ms/step - loss: 478372608.0000 - val_loss: 665491776.0000\n",
            "Epoch 12821/20000\n",
            "1/1 - 0s - 147ms/step - loss: 478323744.0000 - val_loss: 669921600.0000\n",
            "Epoch 12822/20000\n",
            "1/1 - 0s - 124ms/step - loss: 478247936.0000 - val_loss: 667056768.0000\n",
            "Epoch 12823/20000\n",
            "1/1 - 0s - 63ms/step - loss: 478072928.0000 - val_loss: 664617664.0000\n",
            "Epoch 12824/20000\n",
            "1/1 - 0s - 62ms/step - loss: 478072768.0000 - val_loss: 670905344.0000\n",
            "Epoch 12825/20000\n",
            "1/1 - 0s - 60ms/step - loss: 478052800.0000 - val_loss: 666178048.0000\n",
            "Epoch 12826/20000\n",
            "1/1 - 0s - 62ms/step - loss: 477806144.0000 - val_loss: 664309696.0000\n",
            "Epoch 12827/20000\n",
            "1/1 - 0s - 139ms/step - loss: 477789376.0000 - val_loss: 670621504.0000\n",
            "Epoch 12828/20000\n",
            "1/1 - 0s - 63ms/step - loss: 477781632.0000 - val_loss: 666368896.0000\n",
            "Epoch 12829/20000\n",
            "1/1 - 0s - 59ms/step - loss: 477536544.0000 - val_loss: 663269120.0000\n",
            "Epoch 12830/20000\n",
            "1/1 - 0s - 61ms/step - loss: 477574048.0000 - val_loss: 669446592.0000\n",
            "Epoch 12831/20000\n",
            "1/1 - 0s - 65ms/step - loss: 477442496.0000 - val_loss: 667341440.0000\n",
            "Epoch 12832/20000\n",
            "1/1 - 0s - 76ms/step - loss: 477265728.0000 - val_loss: 663240640.0000\n",
            "Epoch 12833/20000\n",
            "1/1 - 0s - 145ms/step - loss: 477261056.0000 - val_loss: 667282816.0000\n",
            "Epoch 12834/20000\n",
            "1/1 - 0s - 60ms/step - loss: 477085312.0000 - val_loss: 667979008.0000\n",
            "Epoch 12835/20000\n",
            "1/1 - 0s - 137ms/step - loss: 477010432.0000 - val_loss: 663601152.0000\n",
            "Epoch 12836/20000\n",
            "1/1 - 0s - 60ms/step - loss: 476943584.0000 - val_loss: 667003264.0000\n",
            "Epoch 12837/20000\n",
            "1/1 - 0s - 64ms/step - loss: 476794624.0000 - val_loss: 667034240.0000\n",
            "Epoch 12838/20000\n",
            "1/1 - 0s - 66ms/step - loss: 476701440.0000 - val_loss: 663501760.0000\n",
            "Epoch 12839/20000\n",
            "1/1 - 0s - 63ms/step - loss: 476658656.0000 - val_loss: 666660480.0000\n",
            "Epoch 12840/20000\n",
            "1/1 - 0s - 60ms/step - loss: 476519008.0000 - val_loss: 666576832.0000\n",
            "Epoch 12841/20000\n",
            "1/1 - 0s - 61ms/step - loss: 476420512.0000 - val_loss: 663332672.0000\n",
            "Epoch 12842/20000\n",
            "1/1 - 0s - 61ms/step - loss: 476373568.0000 - val_loss: 666799808.0000\n",
            "Epoch 12843/20000\n",
            "1/1 - 0s - 64ms/step - loss: 476253152.0000 - val_loss: 665698368.0000\n",
            "Epoch 12844/20000\n",
            "1/1 - 0s - 69ms/step - loss: 476133984.0000 - val_loss: 662954112.0000\n",
            "Epoch 12845/20000\n",
            "1/1 - 0s - 76ms/step - loss: 476107680.0000 - val_loss: 667440384.0000\n",
            "Epoch 12846/20000\n",
            "1/1 - 0s - 130ms/step - loss: 476016896.0000 - val_loss: 664614592.0000\n",
            "Epoch 12847/20000\n",
            "1/1 - 0s - 136ms/step - loss: 475861632.0000 - val_loss: 662576640.0000\n",
            "Epoch 12848/20000\n",
            "1/1 - 0s - 62ms/step - loss: 475836352.0000 - val_loss: 668180608.0000\n",
            "Epoch 12849/20000\n",
            "1/1 - 0s - 68ms/step - loss: 475808704.0000 - val_loss: 664384960.0000\n",
            "Epoch 12850/20000\n",
            "1/1 - 0s - 66ms/step - loss: 475599808.0000 - val_loss: 661360000.0000\n",
            "Epoch 12851/20000\n",
            "1/1 - 0s - 59ms/step - loss: 475631968.0000 - val_loss: 667162816.0000\n",
            "Epoch 12852/20000\n",
            "1/1 - 0s - 60ms/step - loss: 475505472.0000 - val_loss: 665705024.0000\n",
            "Epoch 12853/20000\n",
            "1/1 - 0s - 141ms/step - loss: 475350336.0000 - val_loss: 660616640.0000\n",
            "Epoch 12854/20000\n",
            "1/1 - 0s - 60ms/step - loss: 475402304.0000 - val_loss: 666491776.0000\n",
            "Epoch 12855/20000\n",
            "1/1 - 0s - 70ms/step - loss: 475201600.0000 - val_loss: 666043264.0000\n",
            "Epoch 12856/20000\n",
            "1/1 - 0s - 72ms/step - loss: 475098880.0000 - val_loss: 660934528.0000\n",
            "Epoch 12857/20000\n",
            "1/1 - 0s - 70ms/step - loss: 475068256.0000 - val_loss: 664318272.0000\n",
            "Epoch 12858/20000\n",
            "1/1 - 0s - 65ms/step - loss: 474881088.0000 - val_loss: 666203776.0000\n",
            "Epoch 12859/20000\n",
            "1/1 - 0s - 136ms/step - loss: 474848608.0000 - val_loss: 662035648.0000\n",
            "Epoch 12860/20000\n",
            "1/1 - 0s - 135ms/step - loss: 474721568.0000 - val_loss: 663107904.0000\n",
            "Epoch 12861/20000\n",
            "1/1 - 0s - 64ms/step - loss: 474603328.0000 - val_loss: 665525376.0000\n",
            "Epoch 12862/20000\n",
            "1/1 - 0s - 136ms/step - loss: 474575392.0000 - val_loss: 662198464.0000\n",
            "Epoch 12863/20000\n",
            "1/1 - 0s - 62ms/step - loss: 474428448.0000 - val_loss: 661755456.0000\n",
            "Epoch 12864/20000\n",
            "1/1 - 0s - 59ms/step - loss: 474354528.0000 - val_loss: 666017728.0000\n",
            "Epoch 12865/20000\n",
            "1/1 - 0s - 64ms/step - loss: 474333760.0000 - val_loss: 662882560.0000\n",
            "Epoch 12866/20000\n",
            "1/1 - 0s - 143ms/step - loss: 474165408.0000 - val_loss: 660921088.0000\n",
            "Epoch 12867/20000\n",
            "1/1 - 0s - 62ms/step - loss: 474152224.0000 - val_loss: 666263424.0000\n",
            "Epoch 12868/20000\n",
            "1/1 - 0s - 64ms/step - loss: 474096672.0000 - val_loss: 662795136.0000\n",
            "Epoch 12869/20000\n",
            "1/1 - 0s - 66ms/step - loss: 473903552.0000 - val_loss: 660029568.0000\n",
            "Epoch 12870/20000\n",
            "1/1 - 0s - 70ms/step - loss: 473908160.0000 - val_loss: 665935872.0000\n",
            "Epoch 12871/20000\n",
            "1/1 - 0s - 63ms/step - loss: 473827776.0000 - val_loss: 663271424.0000\n",
            "Epoch 12872/20000\n",
            "1/1 - 0s - 60ms/step - loss: 473632128.0000 - val_loss: 658662592.0000\n",
            "Epoch 12873/20000\n",
            "1/1 - 0s - 67ms/step - loss: 473745376.0000 - val_loss: 665374144.0000\n",
            "Epoch 12874/20000\n",
            "1/1 - 0s - 63ms/step - loss: 473553408.0000 - val_loss: 664276032.0000\n",
            "Epoch 12875/20000\n",
            "1/1 - 0s - 60ms/step - loss: 473404320.0000 - val_loss: 658868608.0000\n",
            "Epoch 12876/20000\n",
            "1/1 - 0s - 59ms/step - loss: 473451968.0000 - val_loss: 663269248.0000\n",
            "Epoch 12877/20000\n",
            "1/1 - 0s - 64ms/step - loss: 473211552.0000 - val_loss: 665128064.0000\n",
            "Epoch 12878/20000\n",
            "1/1 - 0s - 65ms/step - loss: 473209056.0000 - val_loss: 659659904.0000\n",
            "Epoch 12879/20000\n",
            "1/1 - 0s - 144ms/step - loss: 473084928.0000 - val_loss: 661269760.0000\n",
            "Epoch 12880/20000\n",
            "1/1 - 0s - 138ms/step - loss: 472927328.0000 - val_loss: 663457216.0000\n",
            "Epoch 12881/20000\n",
            "1/1 - 0s - 136ms/step - loss: 472880640.0000 - val_loss: 661125376.0000\n",
            "Epoch 12882/20000\n",
            "1/1 - 0s - 133ms/step - loss: 472758112.0000 - val_loss: 660210048.0000\n",
            "Epoch 12883/20000\n",
            "1/1 - 0s - 141ms/step - loss: 472703616.0000 - val_loss: 664703168.0000\n",
            "Epoch 12884/20000\n",
            "1/1 - 0s - 60ms/step - loss: 472684480.0000 - val_loss: 661231680.0000\n",
            "Epoch 12885/20000\n",
            "1/1 - 0s - 61ms/step - loss: 472490912.0000 - val_loss: 658836160.0000\n",
            "Epoch 12886/20000\n",
            "1/1 - 0s - 65ms/step - loss: 472498176.0000 - val_loss: 665110336.0000\n",
            "Epoch 12887/20000\n",
            "1/1 - 0s - 64ms/step - loss: 472478464.0000 - val_loss: 661796672.0000\n",
            "Epoch 12888/20000\n",
            "1/1 - 0s - 59ms/step - loss: 472253856.0000 - val_loss: 657438528.0000\n",
            "Epoch 12889/20000\n",
            "1/1 - 0s - 68ms/step - loss: 472334784.0000 - val_loss: 664001472.0000\n",
            "Epoch 12890/20000\n",
            "1/1 - 0s - 132ms/step - loss: 472172224.0000 - val_loss: 662960128.0000\n",
            "Epoch 12891/20000\n",
            "1/1 - 0s - 138ms/step - loss: 472031424.0000 - val_loss: 657225472.0000\n",
            "Epoch 12892/20000\n",
            "1/1 - 0s - 85ms/step - loss: 472067328.0000 - val_loss: 662567296.0000\n",
            "Epoch 12893/20000\n",
            "1/1 - 0s - 60ms/step - loss: 471852896.0000 - val_loss: 663208832.0000\n",
            "Epoch 12894/20000\n",
            "1/1 - 0s - 61ms/step - loss: 471800128.0000 - val_loss: 658050688.0000\n",
            "Epoch 12895/20000\n",
            "1/1 - 0s - 61ms/step - loss: 471721248.0000 - val_loss: 659611008.0000\n",
            "Epoch 12896/20000\n",
            "1/1 - 0s - 58ms/step - loss: 471565728.0000 - val_loss: 662880448.0000\n",
            "Epoch 12897/20000\n",
            "1/1 - 0s - 140ms/step - loss: 471521056.0000 - val_loss: 660044928.0000\n",
            "Epoch 12898/20000\n",
            "1/1 - 0s - 60ms/step - loss: 471390560.0000 - val_loss: 659228096.0000\n",
            "Epoch 12899/20000\n",
            "1/1 - 0s - 66ms/step - loss: 471327552.0000 - val_loss: 662536000.0000\n",
            "Epoch 12900/20000\n",
            "1/1 - 0s - 67ms/step - loss: 471252800.0000 - val_loss: 660456896.0000\n",
            "Epoch 12901/20000\n",
            "1/1 - 0s - 132ms/step - loss: 471126400.0000 - val_loss: 658906368.0000\n",
            "Epoch 12902/20000\n",
            "1/1 - 0s - 63ms/step - loss: 471069216.0000 - val_loss: 661047296.0000\n",
            "Epoch 12903/20000\n",
            "1/1 - 0s - 58ms/step - loss: 470978496.0000 - val_loss: 660322816.0000\n",
            "Epoch 12904/20000\n",
            "1/1 - 0s - 59ms/step - loss: 470881952.0000 - val_loss: 658368256.0000\n",
            "Epoch 12905/20000\n",
            "1/1 - 0s - 60ms/step - loss: 470814624.0000 - val_loss: 660875968.0000\n",
            "Epoch 12906/20000\n",
            "1/1 - 0s - 139ms/step - loss: 470722624.0000 - val_loss: 659888704.0000\n",
            "Epoch 12907/20000\n",
            "1/1 - 0s - 93ms/step - loss: 470609984.0000 - val_loss: 657821376.0000\n",
            "Epoch 12908/20000\n",
            "1/1 - 0s - 153ms/step - loss: 470581504.0000 - val_loss: 661633600.0000\n",
            "Epoch 12909/20000\n",
            "1/1 - 0s - 93ms/step - loss: 470492960.0000 - val_loss: 660516224.0000\n",
            "Epoch 12910/20000\n",
            "1/1 - 0s - 93ms/step - loss: 470373088.0000 - val_loss: 657545728.0000\n",
            "Epoch 12911/20000\n",
            "1/1 - 0s - 158ms/step - loss: 470327840.0000 - val_loss: 660580416.0000\n",
            "Epoch 12912/20000\n",
            "1/1 - 0s - 123ms/step - loss: 470206336.0000 - val_loss: 660075456.0000\n",
            "Epoch 12913/20000\n",
            "1/1 - 0s - 136ms/step - loss: 470113280.0000 - val_loss: 656939456.0000\n",
            "Epoch 12914/20000\n",
            "1/1 - 0s - 140ms/step - loss: 470088576.0000 - val_loss: 661081408.0000\n",
            "Epoch 12915/20000\n",
            "1/1 - 0s - 87ms/step - loss: 469988640.0000 - val_loss: 659682560.0000\n",
            "Epoch 12916/20000\n",
            "1/1 - 0s - 145ms/step - loss: 469860288.0000 - val_loss: 656344064.0000\n",
            "Epoch 12917/20000\n",
            "1/1 - 0s - 149ms/step - loss: 469860096.0000 - val_loss: 661020992.0000\n",
            "Epoch 12918/20000\n",
            "1/1 - 0s - 119ms/step - loss: 469746624.0000 - val_loss: 659689920.0000\n",
            "Epoch 12919/20000\n",
            "1/1 - 0s - 138ms/step - loss: 469610848.0000 - val_loss: 655944000.0000\n",
            "Epoch 12920/20000\n",
            "1/1 - 0s - 76ms/step - loss: 469611616.0000 - val_loss: 660655808.0000\n",
            "Epoch 12921/20000\n",
            "1/1 - 0s - 140ms/step - loss: 469485632.0000 - val_loss: 659875712.0000\n",
            "Epoch 12922/20000\n",
            "1/1 - 0s - 142ms/step - loss: 469366816.0000 - val_loss: 656011968.0000\n",
            "Epoch 12923/20000\n",
            "1/1 - 0s - 134ms/step - loss: 469344000.0000 - val_loss: 659185472.0000\n",
            "Epoch 12924/20000\n",
            "1/1 - 0s - 137ms/step - loss: 469191008.0000 - val_loss: 659971136.0000\n",
            "Epoch 12925/20000\n",
            "1/1 - 0s - 167ms/step - loss: 469129344.0000 - val_loss: 656135040.0000\n",
            "Epoch 12926/20000\n",
            "1/1 - 0s - 114ms/step - loss: 469091648.0000 - val_loss: 659505920.0000\n",
            "Epoch 12927/20000\n",
            "1/1 - 0s - 139ms/step - loss: 468947584.0000 - val_loss: 659252992.0000\n",
            "Epoch 12928/20000\n",
            "1/1 - 0s - 147ms/step - loss: 468859872.0000 - val_loss: 656414528.0000\n",
            "Epoch 12929/20000\n",
            "1/1 - 0s - 105ms/step - loss: 468798752.0000 - val_loss: 658598784.0000\n",
            "Epoch 12930/20000\n",
            "1/1 - 0s - 122ms/step - loss: 468693248.0000 - val_loss: 658041984.0000\n",
            "Epoch 12931/20000\n",
            "1/1 - 0s - 102ms/step - loss: 468599456.0000 - val_loss: 656479808.0000\n",
            "Epoch 12932/20000\n",
            "1/1 - 0s - 103ms/step - loss: 468535712.0000 - val_loss: 658761984.0000\n",
            "Epoch 12933/20000\n",
            "1/1 - 0s - 187ms/step - loss: 468445600.0000 - val_loss: 657642944.0000\n",
            "Epoch 12934/20000\n",
            "1/1 - 0s - 271ms/step - loss: 468350720.0000 - val_loss: 656919104.0000\n",
            "Epoch 12935/20000\n",
            "1/1 - 0s - 82ms/step - loss: 468274432.0000 - val_loss: 659792832.0000\n",
            "Epoch 12936/20000\n",
            "1/1 - 0s - 67ms/step - loss: 468242784.0000 - val_loss: 656424832.0000\n",
            "Epoch 12937/20000\n",
            "1/1 - 0s - 74ms/step - loss: 468110976.0000 - val_loss: 656322368.0000\n",
            "Epoch 12938/20000\n",
            "1/1 - 0s - 127ms/step - loss: 468024672.0000 - val_loss: 659228288.0000\n",
            "Epoch 12939/20000\n",
            "1/1 - 0s - 140ms/step - loss: 467989792.0000 - val_loss: 656494592.0000\n",
            "Epoch 12940/20000\n",
            "1/1 - 0s - 62ms/step - loss: 467849056.0000 - val_loss: 655000960.0000\n",
            "Epoch 12941/20000\n",
            "1/1 - 0s - 136ms/step - loss: 467818240.0000 - val_loss: 659502656.0000\n",
            "Epoch 12942/20000\n",
            "1/1 - 0s - 59ms/step - loss: 467761216.0000 - val_loss: 656926912.0000\n",
            "Epoch 12943/20000\n",
            "1/1 - 0s - 63ms/step - loss: 467590016.0000 - val_loss: 654337536.0000\n",
            "Epoch 12944/20000\n",
            "1/1 - 0s - 141ms/step - loss: 467623072.0000 - val_loss: 659793408.0000\n",
            "Epoch 12945/20000\n",
            "1/1 - 0s - 61ms/step - loss: 467530112.0000 - val_loss: 657084864.0000\n",
            "Epoch 12946/20000\n",
            "1/1 - 0s - 63ms/step - loss: 467355104.0000 - val_loss: 653784960.0000\n",
            "Epoch 12947/20000\n",
            "1/1 - 0s - 68ms/step - loss: 467376096.0000 - val_loss: 658695424.0000\n",
            "Epoch 12948/20000\n",
            "1/1 - 0s - 138ms/step - loss: 467247584.0000 - val_loss: 657590272.0000\n",
            "Epoch 12949/20000\n",
            "1/1 - 0s - 61ms/step - loss: 467122688.0000 - val_loss: 653465856.0000\n",
            "Epoch 12950/20000\n",
            "1/1 - 0s - 59ms/step - loss: 467129344.0000 - val_loss: 657020352.0000\n",
            "Epoch 12951/20000\n",
            "1/1 - 0s - 62ms/step - loss: 466951264.0000 - val_loss: 658041600.0000\n",
            "Epoch 12952/20000\n",
            "1/1 - 0s - 146ms/step - loss: 466905248.0000 - val_loss: 653699840.0000\n",
            "Epoch 12953/20000\n",
            "1/1 - 0s - 64ms/step - loss: 466862176.0000 - val_loss: 657012352.0000\n",
            "Epoch 12954/20000\n",
            "1/1 - 0s - 63ms/step - loss: 466707584.0000 - val_loss: 657604928.0000\n",
            "Epoch 12955/20000\n",
            "1/1 - 0s - 64ms/step - loss: 466651360.0000 - val_loss: 653545984.0000\n",
            "Epoch 12956/20000\n",
            "1/1 - 0s - 64ms/step - loss: 466613152.0000 - val_loss: 657235008.0000\n",
            "Epoch 12957/20000\n",
            "1/1 - 0s - 63ms/step - loss: 466477440.0000 - val_loss: 657083008.0000\n",
            "Epoch 12958/20000\n",
            "1/1 - 0s - 59ms/step - loss: 466400256.0000 - val_loss: 653398784.0000\n",
            "Epoch 12959/20000\n",
            "1/1 - 0s - 62ms/step - loss: 466368288.0000 - val_loss: 656166016.0000\n",
            "Epoch 12960/20000\n",
            "1/1 - 0s - 67ms/step - loss: 466225696.0000 - val_loss: 656977792.0000\n",
            "Epoch 12961/20000\n",
            "1/1 - 0s - 138ms/step - loss: 466166976.0000 - val_loss: 653828032.0000\n",
            "Epoch 12962/20000\n",
            "1/1 - 0s - 61ms/step - loss: 466079392.0000 - val_loss: 655328896.0000\n",
            "Epoch 12963/20000\n",
            "1/1 - 0s - 61ms/step - loss: 465970400.0000 - val_loss: 656464064.0000\n",
            "Epoch 12964/20000\n",
            "1/1 - 0s - 68ms/step - loss: 465914400.0000 - val_loss: 654349376.0000\n",
            "Epoch 12965/20000\n",
            "1/1 - 0s - 143ms/step - loss: 465819520.0000 - val_loss: 654533504.0000\n",
            "Epoch 12966/20000\n",
            "1/1 - 0s - 65ms/step - loss: 465727680.0000 - val_loss: 656561216.0000\n",
            "Epoch 12967/20000\n",
            "1/1 - 0s - 61ms/step - loss: 465673568.0000 - val_loss: 654333056.0000\n",
            "Epoch 12968/20000\n",
            "1/1 - 0s - 63ms/step - loss: 465581632.0000 - val_loss: 654286912.0000\n",
            "Epoch 12969/20000\n",
            "1/1 - 0s - 65ms/step - loss: 465502016.0000 - val_loss: 656037824.0000\n",
            "Epoch 12970/20000\n",
            "1/1 - 0s - 61ms/step - loss: 465430560.0000 - val_loss: 654593408.0000\n",
            "Epoch 12971/20000\n",
            "1/1 - 0s - 59ms/step - loss: 465330016.0000 - val_loss: 654456704.0000\n",
            "Epoch 12972/20000\n",
            "1/1 - 0s - 142ms/step - loss: 465251520.0000 - val_loss: 655060864.0000\n",
            "Epoch 12973/20000\n",
            "1/1 - 0s - 74ms/step - loss: 465170528.0000 - val_loss: 653600064.0000\n",
            "Epoch 12974/20000\n",
            "1/1 - 0s - 61ms/step - loss: 465095840.0000 - val_loss: 654881472.0000\n",
            "Epoch 12975/20000\n",
            "1/1 - 0s - 64ms/step - loss: 465012096.0000 - val_loss: 654333376.0000\n",
            "Epoch 12976/20000\n",
            "1/1 - 0s - 59ms/step - loss: 464931328.0000 - val_loss: 654031808.0000\n",
            "Epoch 12977/20000\n",
            "1/1 - 0s - 71ms/step - loss: 464852832.0000 - val_loss: 655131904.0000\n",
            "Epoch 12978/20000\n",
            "1/1 - 0s - 73ms/step - loss: 464779200.0000 - val_loss: 653117504.0000\n",
            "Epoch 12979/20000\n",
            "1/1 - 0s - 136ms/step - loss: 464695232.0000 - val_loss: 653911296.0000\n",
            "Epoch 12980/20000\n",
            "1/1 - 0s - 62ms/step - loss: 464600800.0000 - val_loss: 654734144.0000\n",
            "Epoch 12981/20000\n",
            "1/1 - 0s - 140ms/step - loss: 464532480.0000 - val_loss: 653067072.0000\n",
            "Epoch 12982/20000\n",
            "1/1 - 0s - 62ms/step - loss: 464460672.0000 - val_loss: 654355008.0000\n",
            "Epoch 12983/20000\n",
            "1/1 - 0s - 61ms/step - loss: 464376672.0000 - val_loss: 653919552.0000\n",
            "Epoch 12984/20000\n",
            "1/1 - 0s - 63ms/step - loss: 464292192.0000 - val_loss: 653157440.0000\n",
            "Epoch 12985/20000\n",
            "1/1 - 0s - 140ms/step - loss: 464213088.0000 - val_loss: 654935168.0000\n",
            "Epoch 12986/20000\n",
            "1/1 - 0s - 64ms/step - loss: 464156032.0000 - val_loss: 652582080.0000\n",
            "Epoch 12987/20000\n",
            "1/1 - 0s - 62ms/step - loss: 464061376.0000 - val_loss: 652990080.0000\n",
            "Epoch 12988/20000\n",
            "1/1 - 0s - 144ms/step - loss: 463972416.0000 - val_loss: 653992064.0000\n",
            "Epoch 12989/20000\n",
            "1/1 - 0s - 64ms/step - loss: 463902464.0000 - val_loss: 652382208.0000\n",
            "Epoch 12990/20000\n",
            "1/1 - 0s - 139ms/step - loss: 463825152.0000 - val_loss: 654256704.0000\n",
            "Epoch 12991/20000\n",
            "1/1 - 0s - 63ms/step - loss: 463754080.0000 - val_loss: 652832320.0000\n",
            "Epoch 12992/20000\n",
            "1/1 - 0s - 64ms/step - loss: 463656832.0000 - val_loss: 652080320.0000\n",
            "Epoch 12993/20000\n",
            "1/1 - 0s - 63ms/step - loss: 463586240.0000 - val_loss: 653939200.0000\n",
            "Epoch 12994/20000\n",
            "1/1 - 0s - 67ms/step - loss: 463521088.0000 - val_loss: 652341504.0000\n",
            "Epoch 12995/20000\n",
            "1/1 - 0s - 69ms/step - loss: 463427136.0000 - val_loss: 652304832.0000\n",
            "Epoch 12996/20000\n",
            "1/1 - 0s - 145ms/step - loss: 463350304.0000 - val_loss: 653727616.0000\n",
            "Epoch 12997/20000\n",
            "1/1 - 0s - 59ms/step - loss: 463277056.0000 - val_loss: 652330368.0000\n",
            "Epoch 12998/20000\n",
            "1/1 - 0s - 63ms/step - loss: 463182144.0000 - val_loss: 652404288.0000\n",
            "Epoch 12999/20000\n",
            "1/1 - 0s - 61ms/step - loss: 463103872.0000 - val_loss: 653073088.0000\n",
            "Epoch 13000/20000\n",
            "1/1 - 0s - 75ms/step - loss: 463038208.0000 - val_loss: 651236160.0000\n",
            "Epoch 13001/20000\n",
            "1/1 - 0s - 76ms/step - loss: 462980320.0000 - val_loss: 653684608.0000\n",
            "Epoch 13002/20000\n",
            "1/1 - 0s - 132ms/step - loss: 462906176.0000 - val_loss: 651777984.0000\n",
            "Epoch 13003/20000\n",
            "1/1 - 0s - 137ms/step - loss: 462803488.0000 - val_loss: 650967616.0000\n",
            "Epoch 13004/20000\n",
            "1/1 - 0s - 64ms/step - loss: 462736384.0000 - val_loss: 653642368.0000\n",
            "Epoch 13005/20000\n",
            "1/1 - 0s - 137ms/step - loss: 462681120.0000 - val_loss: 651455488.0000\n",
            "Epoch 13006/20000\n",
            "1/1 - 0s - 62ms/step - loss: 462566592.0000 - val_loss: 650865024.0000\n",
            "Epoch 13007/20000\n",
            "1/1 - 0s - 58ms/step - loss: 462497568.0000 - val_loss: 653177856.0000\n",
            "Epoch 13008/20000\n",
            "1/1 - 0s - 139ms/step - loss: 462433664.0000 - val_loss: 651307200.0000\n",
            "Epoch 13009/20000\n",
            "1/1 - 0s - 60ms/step - loss: 462339808.0000 - val_loss: 650661760.0000\n",
            "Epoch 13010/20000\n",
            "1/1 - 0s - 73ms/step - loss: 462272544.0000 - val_loss: 653351552.0000\n",
            "Epoch 13011/20000\n",
            "1/1 - 0s - 67ms/step - loss: 462228288.0000 - val_loss: 651055872.0000\n",
            "Epoch 13012/20000\n",
            "1/1 - 0s - 61ms/step - loss: 462114144.0000 - val_loss: 650135744.0000\n",
            "Epoch 13013/20000\n",
            "1/1 - 0s - 61ms/step - loss: 462054112.0000 - val_loss: 653483840.0000\n",
            "Epoch 13014/20000\n",
            "1/1 - 0s - 60ms/step - loss: 462017280.0000 - val_loss: 651072576.0000\n",
            "Epoch 13015/20000\n",
            "1/1 - 0s - 141ms/step - loss: 461879616.0000 - val_loss: 649512448.0000\n",
            "Epoch 13016/20000\n",
            "1/1 - 0s - 63ms/step - loss: 461824576.0000 - val_loss: 652211200.0000\n",
            "Epoch 13017/20000\n",
            "1/1 - 0s - 65ms/step - loss: 461747136.0000 - val_loss: 651314816.0000\n",
            "Epoch 13018/20000\n",
            "1/1 - 0s - 60ms/step - loss: 461655680.0000 - val_loss: 649624896.0000\n",
            "Epoch 13019/20000\n",
            "1/1 - 0s - 59ms/step - loss: 461611584.0000 - val_loss: 651587136.0000\n",
            "Epoch 13020/20000\n",
            "1/1 - 0s - 138ms/step - loss: 461506304.0000 - val_loss: 651655488.0000\n",
            "Epoch 13021/20000\n",
            "1/1 - 0s - 63ms/step - loss: 461430592.0000 - val_loss: 649066304.0000\n",
            "Epoch 13022/20000\n",
            "1/1 - 0s - 60ms/step - loss: 461372480.0000 - val_loss: 651054720.0000\n",
            "Epoch 13023/20000\n",
            "1/1 - 0s - 141ms/step - loss: 461263328.0000 - val_loss: 651355584.0000\n",
            "Epoch 13024/20000\n",
            "1/1 - 0s - 62ms/step - loss: 461197152.0000 - val_loss: 650248960.0000\n",
            "Epoch 13025/20000\n",
            "1/1 - 0s - 64ms/step - loss: 461130752.0000 - val_loss: 650975424.0000\n",
            "Epoch 13026/20000\n",
            "1/1 - 0s - 60ms/step - loss: 461044992.0000 - val_loss: 650516288.0000\n",
            "Epoch 13027/20000\n",
            "1/1 - 0s - 65ms/step - loss: 460965472.0000 - val_loss: 649787776.0000\n",
            "Epoch 13028/20000\n",
            "1/1 - 0s - 63ms/step - loss: 460890272.0000 - val_loss: 650254336.0000\n",
            "Epoch 13029/20000\n",
            "1/1 - 0s - 137ms/step - loss: 460811552.0000 - val_loss: 649905600.0000\n",
            "Epoch 13030/20000\n",
            "1/1 - 0s - 58ms/step - loss: 460737440.0000 - val_loss: 649608704.0000\n",
            "Epoch 13031/20000\n",
            "1/1 - 0s - 61ms/step - loss: 460663680.0000 - val_loss: 650314048.0000\n",
            "Epoch 13032/20000\n",
            "1/1 - 0s - 139ms/step - loss: 460586176.0000 - val_loss: 649225088.0000\n",
            "Epoch 13033/20000\n",
            "1/1 - 0s - 65ms/step - loss: 460509248.0000 - val_loss: 649939648.0000\n",
            "Epoch 13034/20000\n",
            "1/1 - 0s - 145ms/step - loss: 460430880.0000 - val_loss: 649650624.0000\n",
            "Epoch 13035/20000\n",
            "1/1 - 0s - 65ms/step - loss: 460359264.0000 - val_loss: 649074304.0000\n",
            "Epoch 13036/20000\n",
            "1/1 - 0s - 62ms/step - loss: 460285984.0000 - val_loss: 650723200.0000\n",
            "Epoch 13037/20000\n",
            "1/1 - 0s - 140ms/step - loss: 460213344.0000 - val_loss: 649134208.0000\n",
            "Epoch 13038/20000\n",
            "1/1 - 0s - 61ms/step - loss: 460123680.0000 - val_loss: 648687936.0000\n",
            "Epoch 13039/20000\n",
            "1/1 - 0s - 62ms/step - loss: 460051456.0000 - val_loss: 650193792.0000\n",
            "Epoch 13040/20000\n",
            "1/1 - 0s - 62ms/step - loss: 459979584.0000 - val_loss: 648791296.0000\n",
            "Epoch 13041/20000\n",
            "1/1 - 0s - 63ms/step - loss: 459903040.0000 - val_loss: 649350528.0000\n",
            "Epoch 13042/20000\n",
            "1/1 - 0s - 60ms/step - loss: 459824864.0000 - val_loss: 648980672.0000\n",
            "Epoch 13043/20000\n",
            "1/1 - 0s - 63ms/step - loss: 459747712.0000 - val_loss: 648272640.0000\n",
            "Epoch 13044/20000\n",
            "1/1 - 0s - 84ms/step - loss: 459679008.0000 - val_loss: 649498880.0000\n",
            "Epoch 13045/20000\n",
            "1/1 - 0s - 124ms/step - loss: 459604448.0000 - val_loss: 648318848.0000\n",
            "Epoch 13046/20000\n",
            "1/1 - 0s - 79ms/step - loss: 459530688.0000 - val_loss: 648536576.0000\n",
            "Epoch 13047/20000\n",
            "1/1 - 0s - 134ms/step - loss: 459445664.0000 - val_loss: 648856448.0000\n",
            "Epoch 13048/20000\n",
            "1/1 - 0s - 61ms/step - loss: 459373344.0000 - val_loss: 648352832.0000\n",
            "Epoch 13049/20000\n",
            "1/1 - 0s - 61ms/step - loss: 459302272.0000 - val_loss: 648382464.0000\n",
            "Epoch 13050/20000\n",
            "1/1 - 0s - 63ms/step - loss: 459225312.0000 - val_loss: 649287872.0000\n",
            "Epoch 13051/20000\n",
            "1/1 - 0s - 59ms/step - loss: 459156448.0000 - val_loss: 647488960.0000\n",
            "Epoch 13052/20000\n",
            "1/1 - 0s - 105ms/step - loss: 459083712.0000 - val_loss: 648989376.0000\n",
            "Epoch 13053/20000\n",
            "1/1 - 0s - 99ms/step - loss: 459004608.0000 - val_loss: 648411392.0000\n",
            "Epoch 13054/20000\n",
            "1/1 - 0s - 97ms/step - loss: 458926720.0000 - val_loss: 647212928.0000\n",
            "Epoch 13055/20000\n",
            "1/1 - 0s - 100ms/step - loss: 458866784.0000 - val_loss: 648479424.0000\n",
            "Epoch 13056/20000\n",
            "1/1 - 0s - 147ms/step - loss: 458792384.0000 - val_loss: 647800000.0000\n",
            "Epoch 13057/20000\n",
            "1/1 - 0s - 117ms/step - loss: 458707392.0000 - val_loss: 647315200.0000\n",
            "Epoch 13058/20000\n",
            "1/1 - 0s - 141ms/step - loss: 458634432.0000 - val_loss: 648255104.0000\n",
            "Epoch 13059/20000\n",
            "1/1 - 0s - 132ms/step - loss: 458554944.0000 - val_loss: 647306752.0000\n",
            "Epoch 13060/20000\n",
            "1/1 - 0s - 141ms/step - loss: 458486592.0000 - val_loss: 647929728.0000\n",
            "Epoch 13061/20000\n",
            "1/1 - 0s - 165ms/step - loss: 458410688.0000 - val_loss: 647715072.0000\n",
            "Epoch 13062/20000\n",
            "1/1 - 0s - 138ms/step - loss: 458336320.0000 - val_loss: 647883200.0000\n",
            "Epoch 13063/20000\n",
            "1/1 - 0s - 105ms/step - loss: 458257600.0000 - val_loss: 646606400.0000\n",
            "Epoch 13064/20000\n",
            "1/1 - 0s - 132ms/step - loss: 458193888.0000 - val_loss: 648075072.0000\n",
            "Epoch 13065/20000\n",
            "1/1 - 0s - 133ms/step - loss: 458114752.0000 - val_loss: 646906752.0000\n",
            "Epoch 13066/20000\n",
            "1/1 - 0s - 103ms/step - loss: 458036320.0000 - val_loss: 646876096.0000\n",
            "Epoch 13067/20000\n",
            "1/1 - 0s - 105ms/step - loss: 457960544.0000 - val_loss: 647444160.0000\n",
            "Epoch 13068/20000\n",
            "1/1 - 0s - 100ms/step - loss: 457885088.0000 - val_loss: 646807296.0000\n",
            "Epoch 13069/20000\n",
            "1/1 - 0s - 136ms/step - loss: 457808672.0000 - val_loss: 646996800.0000\n",
            "Epoch 13070/20000\n",
            "1/1 - 0s - 141ms/step - loss: 457739040.0000 - val_loss: 647461632.0000\n",
            "Epoch 13071/20000\n",
            "1/1 - 0s - 148ms/step - loss: 457666304.0000 - val_loss: 646188480.0000\n",
            "Epoch 13072/20000\n",
            "1/1 - 0s - 112ms/step - loss: 457595296.0000 - val_loss: 646956288.0000\n",
            "Epoch 13073/20000\n",
            "1/1 - 0s - 97ms/step - loss: 457516736.0000 - val_loss: 646397120.0000\n",
            "Epoch 13074/20000\n",
            "1/1 - 0s - 132ms/step - loss: 457452256.0000 - val_loss: 647498496.0000\n",
            "Epoch 13075/20000\n",
            "1/1 - 0s - 142ms/step - loss: 457384576.0000 - val_loss: 645703808.0000\n",
            "Epoch 13076/20000\n",
            "1/1 - 0s - 131ms/step - loss: 457307840.0000 - val_loss: 646986688.0000\n",
            "Epoch 13077/20000\n",
            "1/1 - 0s - 138ms/step - loss: 457227904.0000 - val_loss: 646010304.0000\n",
            "Epoch 13078/20000\n",
            "1/1 - 0s - 174ms/step - loss: 457159584.0000 - val_loss: 646291776.0000\n",
            "Epoch 13079/20000\n",
            "1/1 - 0s - 112ms/step - loss: 457080640.0000 - val_loss: 646540032.0000\n",
            "Epoch 13080/20000\n",
            "1/1 - 0s - 110ms/step - loss: 457007776.0000 - val_loss: 645798784.0000\n",
            "Epoch 13081/20000\n",
            "1/1 - 0s - 125ms/step - loss: 456942240.0000 - val_loss: 647184896.0000\n",
            "Epoch 13082/20000\n",
            "1/1 - 0s - 137ms/step - loss: 456880192.0000 - val_loss: 645330304.0000\n",
            "Epoch 13083/20000\n",
            "1/1 - 0s - 122ms/step - loss: 456802848.0000 - val_loss: 646280448.0000\n",
            "Epoch 13084/20000\n",
            "1/1 - 0s - 64ms/step - loss: 456727040.0000 - val_loss: 645682176.0000\n",
            "Epoch 13085/20000\n",
            "1/1 - 0s - 62ms/step - loss: 456650272.0000 - val_loss: 645521088.0000\n",
            "Epoch 13086/20000\n",
            "1/1 - 0s - 59ms/step - loss: 456578048.0000 - val_loss: 645894400.0000\n",
            "Epoch 13087/20000\n",
            "1/1 - 0s - 86ms/step - loss: 456498048.0000 - val_loss: 645696256.0000\n",
            "Epoch 13088/20000\n",
            "1/1 - 0s - 122ms/step - loss: 456421536.0000 - val_loss: 645866624.0000\n",
            "Epoch 13089/20000\n",
            "1/1 - 0s - 145ms/step - loss: 456353472.0000 - val_loss: 645388672.0000\n",
            "Epoch 13090/20000\n",
            "1/1 - 0s - 121ms/step - loss: 456276128.0000 - val_loss: 645903744.0000\n",
            "Epoch 13091/20000\n",
            "1/1 - 0s - 61ms/step - loss: 456199456.0000 - val_loss: 644735872.0000\n",
            "Epoch 13092/20000\n",
            "1/1 - 0s - 63ms/step - loss: 456126272.0000 - val_loss: 645549312.0000\n",
            "Epoch 13093/20000\n",
            "1/1 - 0s - 63ms/step - loss: 456040928.0000 - val_loss: 645232448.0000\n",
            "Epoch 13094/20000\n",
            "1/1 - 0s - 61ms/step - loss: 455971872.0000 - val_loss: 645462080.0000\n",
            "Epoch 13095/20000\n",
            "1/1 - 0s - 62ms/step - loss: 455898048.0000 - val_loss: 645406848.0000\n",
            "Epoch 13096/20000\n",
            "1/1 - 0s - 64ms/step - loss: 455811360.0000 - val_loss: 644737600.0000\n",
            "Epoch 13097/20000\n",
            "1/1 - 0s - 62ms/step - loss: 455729632.0000 - val_loss: 645146560.0000\n",
            "Epoch 13098/20000\n",
            "1/1 - 0s - 63ms/step - loss: 455656096.0000 - val_loss: 645177920.0000\n",
            "Epoch 13099/20000\n",
            "1/1 - 0s - 143ms/step - loss: 455580832.0000 - val_loss: 644460096.0000\n",
            "Epoch 13100/20000\n",
            "1/1 - 0s - 142ms/step - loss: 455505600.0000 - val_loss: 645325376.0000\n",
            "Epoch 13101/20000\n",
            "1/1 - 0s - 131ms/step - loss: 455428704.0000 - val_loss: 644555136.0000\n",
            "Epoch 13102/20000\n",
            "1/1 - 0s - 60ms/step - loss: 455345856.0000 - val_loss: 644872576.0000\n",
            "Epoch 13103/20000\n",
            "1/1 - 0s - 61ms/step - loss: 455268704.0000 - val_loss: 644770560.0000\n",
            "Epoch 13104/20000\n",
            "1/1 - 0s - 140ms/step - loss: 455192768.0000 - val_loss: 645348352.0000\n",
            "Epoch 13105/20000\n",
            "1/1 - 0s - 60ms/step - loss: 455126368.0000 - val_loss: 643539328.0000\n",
            "Epoch 13106/20000\n",
            "1/1 - 0s - 142ms/step - loss: 455074656.0000 - val_loss: 646635520.0000\n",
            "Epoch 13107/20000\n",
            "1/1 - 0s - 59ms/step - loss: 455031872.0000 - val_loss: 643977664.0000\n",
            "Epoch 13108/20000\n",
            "1/1 - 0s - 59ms/step - loss: 454901920.0000 - val_loss: 642717376.0000\n",
            "Epoch 13109/20000\n",
            "1/1 - 0s - 74ms/step - loss: 454857376.0000 - val_loss: 646930880.0000\n",
            "Epoch 13110/20000\n",
            "1/1 - 0s - 81ms/step - loss: 454846144.0000 - val_loss: 644212160.0000\n",
            "Epoch 13111/20000\n",
            "1/1 - 0s - 136ms/step - loss: 454684960.0000 - val_loss: 641665024.0000\n",
            "Epoch 13112/20000\n",
            "1/1 - 0s - 60ms/step - loss: 454690400.0000 - val_loss: 645777664.0000\n",
            "Epoch 13113/20000\n",
            "1/1 - 0s - 138ms/step - loss: 454558272.0000 - val_loss: 645503168.0000\n",
            "Epoch 13114/20000\n",
            "1/1 - 0s - 61ms/step - loss: 454460928.0000 - val_loss: 641474432.0000\n",
            "Epoch 13115/20000\n",
            "1/1 - 0s - 59ms/step - loss: 454460320.0000 - val_loss: 645943936.0000\n",
            "Epoch 13116/20000\n",
            "1/1 - 0s - 60ms/step - loss: 454324960.0000 - val_loss: 644827456.0000\n",
            "Epoch 13117/20000\n",
            "1/1 - 0s - 64ms/step - loss: 454208192.0000 - val_loss: 641325696.0000\n",
            "Epoch 13118/20000\n",
            "1/1 - 0s - 134ms/step - loss: 454214976.0000 - val_loss: 645555520.0000\n",
            "Epoch 13119/20000\n",
            "1/1 - 0s - 62ms/step - loss: 454068864.0000 - val_loss: 644689664.0000\n",
            "Epoch 13120/20000\n",
            "1/1 - 0s - 71ms/step - loss: 453952544.0000 - val_loss: 641478592.0000\n",
            "Epoch 13121/20000\n",
            "1/1 - 0s - 134ms/step - loss: 453951936.0000 - val_loss: 645169216.0000\n",
            "Epoch 13122/20000\n",
            "1/1 - 0s - 134ms/step - loss: 453806912.0000 - val_loss: 644804608.0000\n",
            "Epoch 13123/20000\n",
            "1/1 - 0s - 64ms/step - loss: 453711744.0000 - val_loss: 641674624.0000\n",
            "Epoch 13124/20000\n",
            "1/1 - 0s - 61ms/step - loss: 453679360.0000 - val_loss: 644490624.0000\n",
            "Epoch 13125/20000\n",
            "1/1 - 0s - 60ms/step - loss: 453559712.0000 - val_loss: 644448384.0000\n",
            "Epoch 13126/20000\n",
            "1/1 - 0s - 62ms/step - loss: 453488256.0000 - val_loss: 641405504.0000\n",
            "Epoch 13127/20000\n",
            "1/1 - 0s - 62ms/step - loss: 453438080.0000 - val_loss: 644016960.0000\n",
            "Epoch 13128/20000\n",
            "1/1 - 0s - 66ms/step - loss: 453327552.0000 - val_loss: 644161408.0000\n",
            "Epoch 13129/20000\n",
            "1/1 - 0s - 62ms/step - loss: 453259328.0000 - val_loss: 641121408.0000\n",
            "Epoch 13130/20000\n",
            "1/1 - 0s - 61ms/step - loss: 453221056.0000 - val_loss: 643347712.0000\n",
            "Epoch 13131/20000\n",
            "1/1 - 0s - 61ms/step - loss: 453101600.0000 - val_loss: 643881472.0000\n",
            "Epoch 13132/20000\n",
            "1/1 - 0s - 60ms/step - loss: 453035232.0000 - val_loss: 641616448.0000\n",
            "Epoch 13133/20000\n",
            "1/1 - 0s - 73ms/step - loss: 452988288.0000 - val_loss: 644363776.0000\n",
            "Epoch 13134/20000\n",
            "1/1 - 0s - 73ms/step - loss: 452908128.0000 - val_loss: 643239616.0000\n",
            "Epoch 13135/20000\n",
            "1/1 - 0s - 134ms/step - loss: 452815776.0000 - val_loss: 641174848.0000\n",
            "Epoch 13136/20000\n",
            "1/1 - 0s - 134ms/step - loss: 452766720.0000 - val_loss: 643690048.0000\n",
            "Epoch 13137/20000\n",
            "1/1 - 0s - 62ms/step - loss: 452683552.0000 - val_loss: 643119808.0000\n",
            "Epoch 13138/20000\n",
            "1/1 - 0s - 61ms/step - loss: 452604352.0000 - val_loss: 641362816.0000\n",
            "Epoch 13139/20000\n",
            "1/1 - 0s - 58ms/step - loss: 452558560.0000 - val_loss: 643068352.0000\n",
            "Epoch 13140/20000\n",
            "1/1 - 0s - 138ms/step - loss: 452467584.0000 - val_loss: 643108352.0000\n",
            "Epoch 13141/20000\n",
            "1/1 - 0s - 61ms/step - loss: 452388448.0000 - val_loss: 641399296.0000\n",
            "Epoch 13142/20000\n",
            "1/1 - 0s - 60ms/step - loss: 452331648.0000 - val_loss: 643436864.0000\n",
            "Epoch 13143/20000\n",
            "1/1 - 0s - 58ms/step - loss: 452247648.0000 - val_loss: 642205760.0000\n",
            "Epoch 13144/20000\n",
            "1/1 - 0s - 73ms/step - loss: 452164032.0000 - val_loss: 642422208.0000\n",
            "Epoch 13145/20000\n",
            "1/1 - 0s - 132ms/step - loss: 452085824.0000 - val_loss: 641940288.0000\n",
            "Epoch 13146/20000\n",
            "1/1 - 0s - 62ms/step - loss: 452025024.0000 - val_loss: 642899072.0000\n",
            "Epoch 13147/20000\n",
            "1/1 - 0s - 62ms/step - loss: 451963328.0000 - val_loss: 641731520.0000\n",
            "Epoch 13148/20000\n",
            "1/1 - 0s - 70ms/step - loss: 451889568.0000 - val_loss: 642068864.0000\n",
            "Epoch 13149/20000\n",
            "1/1 - 0s - 136ms/step - loss: 451809536.0000 - val_loss: 642690048.0000\n",
            "Epoch 13150/20000\n",
            "1/1 - 0s - 140ms/step - loss: 451742720.0000 - val_loss: 640995264.0000\n",
            "Epoch 13151/20000\n",
            "1/1 - 0s - 137ms/step - loss: 451675552.0000 - val_loss: 642679680.0000\n",
            "Epoch 13152/20000\n",
            "1/1 - 0s - 59ms/step - loss: 451599904.0000 - val_loss: 642316672.0000\n",
            "Epoch 13153/20000\n",
            "1/1 - 0s - 58ms/step - loss: 451530016.0000 - val_loss: 641255552.0000\n",
            "Epoch 13154/20000\n",
            "1/1 - 0s - 62ms/step - loss: 451467776.0000 - val_loss: 642806848.0000\n",
            "Epoch 13155/20000\n",
            "1/1 - 0s - 74ms/step - loss: 451396224.0000 - val_loss: 641494144.0000\n",
            "Epoch 13156/20000\n",
            "1/1 - 0s - 131ms/step - loss: 451311840.0000 - val_loss: 641185856.0000\n",
            "Epoch 13157/20000\n",
            "1/1 - 0s - 62ms/step - loss: 451247264.0000 - val_loss: 642175680.0000\n",
            "Epoch 13158/20000\n",
            "1/1 - 0s - 61ms/step - loss: 451174336.0000 - val_loss: 641581952.0000\n",
            "Epoch 13159/20000\n",
            "1/1 - 0s - 138ms/step - loss: 451103264.0000 - val_loss: 642331904.0000\n",
            "Epoch 13160/20000\n",
            "1/1 - 0s - 62ms/step - loss: 451034272.0000 - val_loss: 641129728.0000\n",
            "Epoch 13161/20000\n",
            "1/1 - 0s - 61ms/step - loss: 450967808.0000 - val_loss: 642109952.0000\n",
            "Epoch 13162/20000\n",
            "1/1 - 0s - 62ms/step - loss: 450894752.0000 - val_loss: 641267712.0000\n",
            "Epoch 13163/20000\n",
            "1/1 - 0s - 61ms/step - loss: 450821024.0000 - val_loss: 641877248.0000\n",
            "Epoch 13164/20000\n",
            "1/1 - 0s - 58ms/step - loss: 450753728.0000 - val_loss: 641619520.0000\n",
            "Epoch 13165/20000\n",
            "1/1 - 0s - 63ms/step - loss: 450685472.0000 - val_loss: 640381376.0000\n",
            "Epoch 13166/20000\n",
            "1/1 - 0s - 60ms/step - loss: 450632768.0000 - val_loss: 642208064.0000\n",
            "Epoch 13167/20000\n",
            "1/1 - 0s - 63ms/step - loss: 450560928.0000 - val_loss: 641223808.0000\n",
            "Epoch 13168/20000\n",
            "1/1 - 0s - 70ms/step - loss: 450471904.0000 - val_loss: 640187328.0000\n",
            "Epoch 13169/20000\n",
            "1/1 - 0s - 75ms/step - loss: 450429216.0000 - val_loss: 642765504.0000\n",
            "Epoch 13170/20000\n",
            "1/1 - 0s - 129ms/step - loss: 450367648.0000 - val_loss: 640634496.0000\n",
            "Epoch 13171/20000\n",
            "1/1 - 0s - 60ms/step - loss: 450283744.0000 - val_loss: 640836352.0000\n",
            "Epoch 13172/20000\n",
            "1/1 - 0s - 72ms/step - loss: 450205184.0000 - val_loss: 641711104.0000\n",
            "Epoch 13173/20000\n",
            "1/1 - 0s - 64ms/step - loss: 450132224.0000 - val_loss: 640805568.0000\n",
            "Epoch 13174/20000\n",
            "1/1 - 0s - 62ms/step - loss: 450066080.0000 - val_loss: 640473344.0000\n",
            "Epoch 13175/20000\n",
            "1/1 - 0s - 63ms/step - loss: 450001792.0000 - val_loss: 641751680.0000\n",
            "Epoch 13176/20000\n",
            "1/1 - 0s - 61ms/step - loss: 449939424.0000 - val_loss: 640313664.0000\n",
            "Epoch 13177/20000\n",
            "1/1 - 0s - 58ms/step - loss: 449858464.0000 - val_loss: 640828032.0000\n",
            "Epoch 13178/20000\n",
            "1/1 - 0s - 60ms/step - loss: 449781024.0000 - val_loss: 640748672.0000\n",
            "Epoch 13179/20000\n",
            "1/1 - 0s - 66ms/step - loss: 449713856.0000 - val_loss: 640344192.0000\n",
            "Epoch 13180/20000\n",
            "1/1 - 0s - 144ms/step - loss: 449648416.0000 - val_loss: 640470784.0000\n",
            "Epoch 13181/20000\n",
            "1/1 - 0s - 74ms/step - loss: 449575520.0000 - val_loss: 640656768.0000\n",
            "Epoch 13182/20000\n",
            "1/1 - 0s - 68ms/step - loss: 449497920.0000 - val_loss: 639439808.0000\n",
            "Epoch 13183/20000\n",
            "1/1 - 0s - 139ms/step - loss: 449452640.0000 - val_loss: 643231744.0000\n",
            "Epoch 13184/20000\n",
            "1/1 - 0s - 62ms/step - loss: 449459392.0000 - val_loss: 639501760.0000\n",
            "Epoch 13185/20000\n",
            "1/1 - 0s - 78ms/step - loss: 449316832.0000 - val_loss: 638981376.0000\n",
            "Epoch 13186/20000\n",
            "1/1 - 0s - 142ms/step - loss: 449258720.0000 - val_loss: 642426688.0000\n",
            "Epoch 13187/20000\n",
            "1/1 - 0s - 63ms/step - loss: 449226048.0000 - val_loss: 640059520.0000\n",
            "Epoch 13188/20000\n",
            "1/1 - 0s - 57ms/step - loss: 449095072.0000 - val_loss: 638292160.0000\n",
            "Epoch 13189/20000\n",
            "1/1 - 0s - 58ms/step - loss: 449063136.0000 - val_loss: 641411520.0000\n",
            "Epoch 13190/20000\n",
            "1/1 - 0s - 140ms/step - loss: 448988768.0000 - val_loss: 640317824.0000\n",
            "Epoch 13191/20000\n",
            "1/1 - 0s - 65ms/step - loss: 448891968.0000 - val_loss: 637846976.0000\n",
            "Epoch 13192/20000\n",
            "1/1 - 0s - 134ms/step - loss: 448882944.0000 - val_loss: 641809152.0000\n",
            "Epoch 13193/20000\n",
            "1/1 - 0s - 59ms/step - loss: 448797024.0000 - val_loss: 640368768.0000\n",
            "Epoch 13194/20000\n",
            "1/1 - 0s - 62ms/step - loss: 448691936.0000 - val_loss: 637958848.0000\n",
            "Epoch 13195/20000\n",
            "1/1 - 0s - 61ms/step - loss: 448662432.0000 - val_loss: 640744576.0000\n",
            "Epoch 13196/20000\n",
            "1/1 - 0s - 137ms/step - loss: 448559808.0000 - val_loss: 640338880.0000\n",
            "Epoch 13197/20000\n",
            "1/1 - 0s - 62ms/step - loss: 448478496.0000 - val_loss: 637969472.0000\n",
            "Epoch 13198/20000\n",
            "1/1 - 0s - 60ms/step - loss: 448436064.0000 - val_loss: 640685312.0000\n",
            "Epoch 13199/20000\n",
            "1/1 - 0s - 73ms/step - loss: 448350816.0000 - val_loss: 640037632.0000\n",
            "Epoch 13200/20000\n",
            "1/1 - 0s - 89ms/step - loss: 448267872.0000 - val_loss: 638255232.0000\n",
            "Epoch 13201/20000\n",
            "1/1 - 0s - 138ms/step - loss: 448218976.0000 - val_loss: 640614016.0000\n",
            "Epoch 13202/20000\n",
            "1/1 - 0s - 99ms/step - loss: 448147072.0000 - val_loss: 639549696.0000\n",
            "Epoch 13203/20000\n",
            "1/1 - 0s - 116ms/step - loss: 448060768.0000 - val_loss: 637560320.0000\n",
            "Epoch 13204/20000\n",
            "1/1 - 0s - 121ms/step - loss: 448055744.0000 - val_loss: 641715136.0000\n",
            "Epoch 13205/20000\n",
            "1/1 - 0s - 162ms/step - loss: 448001888.0000 - val_loss: 639128384.0000\n",
            "Epoch 13206/20000\n",
            "1/1 - 0s - 123ms/step - loss: 447860064.0000 - val_loss: 636986624.0000\n",
            "Epoch 13207/20000\n",
            "1/1 - 0s - 89ms/step - loss: 447855200.0000 - val_loss: 641437632.0000\n",
            "Epoch 13208/20000\n",
            "1/1 - 0s - 145ms/step - loss: 447782720.0000 - val_loss: 639668288.0000\n",
            "Epoch 13209/20000\n",
            "1/1 - 0s - 111ms/step - loss: 447650592.0000 - val_loss: 636259840.0000\n",
            "Epoch 13210/20000\n",
            "1/1 - 0s - 113ms/step - loss: 447678112.0000 - val_loss: 640572800.0000\n",
            "Epoch 13211/20000\n",
            "1/1 - 0s - 145ms/step - loss: 447544704.0000 - val_loss: 640703552.0000\n",
            "Epoch 13212/20000\n",
            "1/1 - 0s - 117ms/step - loss: 447479520.0000 - val_loss: 636630976.0000\n",
            "Epoch 13213/20000\n",
            "1/1 - 0s - 135ms/step - loss: 447456352.0000 - val_loss: 639418880.0000\n",
            "Epoch 13214/20000\n",
            "1/1 - 0s - 138ms/step - loss: 447302528.0000 - val_loss: 641117056.0000\n",
            "Epoch 13215/20000\n",
            "1/1 - 0s - 89ms/step - loss: 447280256.0000 - val_loss: 637266880.0000\n",
            "Epoch 13216/20000\n",
            "1/1 - 0s - 78ms/step - loss: 447188064.0000 - val_loss: 638273408.0000\n",
            "Epoch 13217/20000\n",
            "1/1 - 0s - 93ms/step - loss: 447084224.0000 - val_loss: 640091520.0000\n",
            "Epoch 13218/20000\n",
            "1/1 - 0s - 78ms/step - loss: 447043520.0000 - val_loss: 638163200.0000\n",
            "Epoch 13219/20000\n",
            "1/1 - 0s - 145ms/step - loss: 446948064.0000 - val_loss: 637694272.0000\n",
            "Epoch 13220/20000\n",
            "1/1 - 0s - 100ms/step - loss: 446892000.0000 - val_loss: 640694080.0000\n",
            "Epoch 13221/20000\n",
            "1/1 - 0s - 93ms/step - loss: 446853280.0000 - val_loss: 638075584.0000\n",
            "Epoch 13222/20000\n",
            "1/1 - 0s - 161ms/step - loss: 446733280.0000 - val_loss: 637384896.0000\n",
            "Epoch 13223/20000\n",
            "1/1 - 0s - 98ms/step - loss: 446683872.0000 - val_loss: 641202176.0000\n",
            "Epoch 13224/20000\n",
            "1/1 - 0s - 77ms/step - loss: 446690336.0000 - val_loss: 637937984.0000\n",
            "Epoch 13225/20000\n",
            "1/1 - 0s - 149ms/step - loss: 446534944.0000 - val_loss: 635823744.0000\n",
            "Epoch 13226/20000\n",
            "1/1 - 0s - 158ms/step - loss: 446558976.0000 - val_loss: 642395264.0000\n",
            "Epoch 13227/20000\n",
            "1/1 - 0s - 122ms/step - loss: 446584960.0000 - val_loss: 638979136.0000\n",
            "Epoch 13228/20000\n",
            "1/1 - 0s - 137ms/step - loss: 446338848.0000 - val_loss: 634248768.0000\n",
            "Epoch 13229/20000\n",
            "1/1 - 0s - 100ms/step - loss: 446481152.0000 - val_loss: 640124160.0000\n",
            "Epoch 13230/20000\n",
            "1/1 - 0s - 161ms/step - loss: 446244448.0000 - val_loss: 641420672.0000\n",
            "Epoch 13231/20000\n",
            "1/1 - 0s - 126ms/step - loss: 446253888.0000 - val_loss: 635099072.0000\n",
            "Epoch 13232/20000\n",
            "1/1 - 0s - 89ms/step - loss: 446184608.0000 - val_loss: 636783296.0000\n",
            "Epoch 13233/20000\n",
            "1/1 - 0s - 127ms/step - loss: 446008800.0000 - val_loss: 641255616.0000\n",
            "Epoch 13234/20000\n",
            "1/1 - 0s - 139ms/step - loss: 446045920.0000 - val_loss: 637736448.0000\n",
            "Epoch 13235/20000\n",
            "1/1 - 0s - 131ms/step - loss: 445854816.0000 - val_loss: 635128128.0000\n",
            "Epoch 13236/20000\n",
            "1/1 - 0s - 138ms/step - loss: 445889728.0000 - val_loss: 640219264.0000\n",
            "Epoch 13237/20000\n",
            "1/1 - 0s - 65ms/step - loss: 445785312.0000 - val_loss: 639069824.0000\n",
            "Epoch 13238/20000\n",
            "1/1 - 0s - 60ms/step - loss: 445662688.0000 - val_loss: 635066560.0000\n",
            "Epoch 13239/20000\n",
            "1/1 - 0s - 67ms/step - loss: 445687104.0000 - val_loss: 638892608.0000\n",
            "Epoch 13240/20000\n",
            "1/1 - 0s - 63ms/step - loss: 445534016.0000 - val_loss: 639771712.0000\n",
            "Epoch 13241/20000\n",
            "1/1 - 0s - 60ms/step - loss: 445500416.0000 - val_loss: 635560128.0000\n",
            "Epoch 13242/20000\n",
            "1/1 - 0s - 135ms/step - loss: 445445504.0000 - val_loss: 637980544.0000\n",
            "Epoch 13243/20000\n",
            "1/1 - 0s - 61ms/step - loss: 445309184.0000 - val_loss: 638889664.0000\n",
            "Epoch 13244/20000\n",
            "1/1 - 0s - 146ms/step - loss: 445267520.0000 - val_loss: 635970240.0000\n",
            "Epoch 13245/20000\n",
            "1/1 - 0s - 79ms/step - loss: 445192416.0000 - val_loss: 637085120.0000\n",
            "Epoch 13246/20000\n",
            "1/1 - 0s - 60ms/step - loss: 445092864.0000 - val_loss: 637885952.0000\n",
            "Epoch 13247/20000\n",
            "1/1 - 0s - 60ms/step - loss: 445037440.0000 - val_loss: 637148736.0000\n",
            "Epoch 13248/20000\n",
            "1/1 - 0s - 64ms/step - loss: 444976416.0000 - val_loss: 637734400.0000\n",
            "Epoch 13249/20000\n",
            "1/1 - 0s - 137ms/step - loss: 444900672.0000 - val_loss: 637567616.0000\n",
            "Epoch 13250/20000\n",
            "1/1 - 0s - 60ms/step - loss: 444826848.0000 - val_loss: 636320704.0000\n",
            "Epoch 13251/20000\n",
            "1/1 - 0s - 140ms/step - loss: 444771328.0000 - val_loss: 637760960.0000\n",
            "Epoch 13252/20000\n",
            "1/1 - 0s - 84ms/step - loss: 444706272.0000 - val_loss: 637212416.0000\n",
            "Epoch 13253/20000\n",
            "1/1 - 0s - 117ms/step - loss: 444628352.0000 - val_loss: 636189824.0000\n",
            "Epoch 13254/20000\n",
            "1/1 - 0s - 60ms/step - loss: 444579264.0000 - val_loss: 638108736.0000\n",
            "Epoch 13255/20000\n",
            "1/1 - 0s - 82ms/step - loss: 444513728.0000 - val_loss: 637249664.0000\n",
            "Epoch 13256/20000\n",
            "1/1 - 0s - 73ms/step - loss: 444431808.0000 - val_loss: 636529984.0000\n",
            "Epoch 13257/20000\n",
            "1/1 - 0s - 134ms/step - loss: 444364928.0000 - val_loss: 636548096.0000\n",
            "Epoch 13258/20000\n",
            "1/1 - 0s - 62ms/step - loss: 444302048.0000 - val_loss: 636697920.0000\n",
            "Epoch 13259/20000\n",
            "1/1 - 0s - 62ms/step - loss: 444233280.0000 - val_loss: 637115200.0000\n",
            "Epoch 13260/20000\n",
            "1/1 - 0s - 60ms/step - loss: 444166784.0000 - val_loss: 635831744.0000\n",
            "Epoch 13261/20000\n",
            "1/1 - 0s - 63ms/step - loss: 444117824.0000 - val_loss: 637578112.0000\n",
            "Epoch 13262/20000\n",
            "1/1 - 0s - 60ms/step - loss: 444038112.0000 - val_loss: 636666176.0000\n",
            "Epoch 13263/20000\n",
            "1/1 - 0s - 139ms/step - loss: 443955328.0000 - val_loss: 636011008.0000\n",
            "Epoch 13264/20000\n",
            "1/1 - 0s - 70ms/step - loss: 443898112.0000 - val_loss: 637265152.0000\n",
            "Epoch 13265/20000\n",
            "1/1 - 0s - 63ms/step - loss: 443830208.0000 - val_loss: 637083648.0000\n",
            "Epoch 13266/20000\n",
            "1/1 - 0s - 60ms/step - loss: 443763424.0000 - val_loss: 635663680.0000\n",
            "Epoch 13267/20000\n",
            "1/1 - 0s - 60ms/step - loss: 443700512.0000 - val_loss: 636590464.0000\n",
            "Epoch 13268/20000\n",
            "1/1 - 0s - 70ms/step - loss: 443625024.0000 - val_loss: 636595008.0000\n",
            "Epoch 13269/20000\n",
            "1/1 - 0s - 69ms/step - loss: 443556800.0000 - val_loss: 635564544.0000\n",
            "Epoch 13270/20000\n",
            "1/1 - 0s - 64ms/step - loss: 443497280.0000 - val_loss: 636977472.0000\n",
            "Epoch 13271/20000\n",
            "1/1 - 0s - 141ms/step - loss: 443425824.0000 - val_loss: 635734080.0000\n",
            "Epoch 13272/20000\n",
            "1/1 - 0s - 64ms/step - loss: 443364576.0000 - val_loss: 636340032.0000\n",
            "Epoch 13273/20000\n",
            "1/1 - 0s - 58ms/step - loss: 443284672.0000 - val_loss: 636540544.0000\n",
            "Epoch 13274/20000\n",
            "1/1 - 0s - 61ms/step - loss: 443218656.0000 - val_loss: 635536896.0000\n",
            "Epoch 13275/20000\n",
            "1/1 - 0s - 60ms/step - loss: 443149568.0000 - val_loss: 636090368.0000\n",
            "Epoch 13276/20000\n",
            "1/1 - 0s - 63ms/step - loss: 443084448.0000 - val_loss: 636393792.0000\n",
            "Epoch 13277/20000\n",
            "1/1 - 0s - 58ms/step - loss: 443022400.0000 - val_loss: 635082368.0000\n",
            "Epoch 13278/20000\n",
            "1/1 - 0s - 74ms/step - loss: 442961280.0000 - val_loss: 635955712.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79ef3235bc10>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m2preds_full = m2.predict(x=split_features(mydat_emb))\n",
        "m2preds_val = m2.predict(x=split_features(mydat_val_emb))\n",
        "m2preds = m2.predict(x=split_features(mydat_train_emb))\n",
        "\n",
        "errs2 = m2preds_full.flatten() - mydat_y.flatten()\n",
        "\n",
        "print(np.mean(np.square(errs2))/1000000)\n",
        "print(np.mean(np.absolute(errs2))/1000)\n",
        "print(np.sqrt(np.mean(np.square(errs2)))/mave*100)\n",
        "print(np.mean(np.absolute(errs2))/mave*100)\n",
        "print(np.mean(errs2)/mydat_y.mean())"
      ],
      "metadata": {
        "id": "CBvMMxrl18Zg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06823fd-e485-4a7d-84db-3177dd88d600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5938/5938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
            "699.4219014133438\n",
            "14.932977704436835\n",
            "26.143891565186976\n",
            "14.76206239025223\n",
            "0.010636112906442091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(19):\n",
        "  print(\"Product type:\" + str(i+1))\n",
        "  currenterrs = errs[10000*i:(10000*i+10000)]\n",
        "  currenty = mydat_y[10000*i:(10000*i+10000)]\n",
        "  currentmave = np.mean(np.abs(currenty))\n",
        "  currentmse = np.mean(np.square(currenterrs))\n",
        "  print(currentmse/1000000)\n",
        "  print(np.sqrt(currentmse)/currentmave)\n",
        "  print(np.mean(currenterrs)/np.mean(currenty))"
      ],
      "metadata": {
        "id": "kCpUnHzj2Gph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fb92000-c6ce-41c7-b6eb-a75f73e759ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product type:1\n",
            "183.47643966549677\n",
            "0.39129161773743193\n",
            "-0.0006935991460613565\n",
            "Product type:2\n",
            "1878.589386982343\n",
            "0.1371328665561452\n",
            "0.013118943047495061\n",
            "Product type:3\n",
            "188.66827908947027\n",
            "0.3830771982497577\n",
            "0.14106062344897866\n",
            "Product type:4\n",
            "641.3151089578128\n",
            "0.36024020928790657\n",
            "0.07953607974576944\n",
            "Product type:5\n",
            "221.77844110206675\n",
            "0.3178048978418479\n",
            "0.06633715109723426\n",
            "Product type:6\n",
            "463.2954285027623\n",
            "0.45531886897754154\n",
            "-0.015906215684053884\n",
            "Product type:7\n",
            "1388.5828105684916\n",
            "0.7568603651594896\n",
            "-0.10495115957853395\n",
            "Product type:8\n",
            "42.78755926014768\n",
            "1.245956612534637\n",
            "-0.0843358182144588\n",
            "Product type:9\n",
            "230.88142312797999\n",
            "0.31219606282831136\n",
            "0.059076928572447764\n",
            "Product type:10\n",
            "295.742945999504\n",
            "0.35115688110038684\n",
            "0.044727075148855745\n",
            "Product type:11\n",
            "489.4722456226593\n",
            "0.4385905701341235\n",
            "0.1087630761411987\n",
            "Product type:12\n",
            "412.06265777118335\n",
            "0.27608141629827326\n",
            "0.049636163616487136\n",
            "Product type:13\n",
            "58.142876523614525\n",
            "0.7497570228243251\n",
            "-0.03865545344881798\n",
            "Product type:14\n",
            "48.90567541993699\n",
            "0.6931262606902503\n",
            "-0.012948422917951743\n",
            "Product type:15\n",
            "1138.9611152619689\n",
            "0.546599796068813\n",
            "0.17843532211921273\n",
            "Product type:16\n",
            "1608.3407645737789\n",
            "0.11076912219148993\n",
            "-0.0029470388189073364\n",
            "Product type:17\n",
            "214.05623490422911\n",
            "0.27337387925016593\n",
            "-0.015614558928290197\n",
            "Product type:18\n",
            "3267.4000466033976\n",
            "0.11786974593559066\n",
            "0.028660097816756228\n",
            "Product type:19\n",
            "418.7432655556251\n",
            "0.18204404218602285\n",
            "-0.004577504081598014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxAfkOqdki1q"
      },
      "source": [
        "## Hejazi methodology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWbqR-f1kmkR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "mydat_trainr, mydat_rep, mydat_trainr_y, mydat_rep_y = train_test_split(mydat_train_emb, mydat_train_y, test_size = 0.5, train_size = 0.5, random_state = random_state)\n",
        "#print(len(mydat_rep), len(mydat_trainr), len(mydat_val))\n",
        "\n",
        "def getcatdist(catvec2, catvec1):\n",
        "  return 1*np.array(catvec2 != catvec1)\n",
        "\n",
        "def getmaxdist(numvec2, numvec1):\n",
        "  numvec2 = np.array(numvec2)\n",
        "  numvec1 = np.array(numvec1)\n",
        "  return 1*np.maximum(numvec2-numvec1, 0)\n",
        "\n",
        "def getmindist(numvec2, numvec1):\n",
        "  numvec2 = np.array(numvec2)\n",
        "  numvec1 = np.array(numvec1)\n",
        "  return 1*np.maximum(numvec1-numvec2, 0)\n",
        "\n",
        "def getdist (list_rep, list_pred):\n",
        "  datcat_rep = list_rep[0]\n",
        "  datnum_rep = list_rep[1]\n",
        "  datcat_pred = list_pred[0]\n",
        "  datnum_pred = list_pred[1]\n",
        "  distlist = []\n",
        "  for i in range(len(list_rep[0])):\n",
        "    currentcat = getcatdist(datcat_rep.iloc[i], datcat_pred)\n",
        "    currentmax = getmaxdist(datnum_rep.iloc[i], datnum_pred)\n",
        "    currentmin = getmindist(datnum_rep.iloc[i], datnum_pred)\n",
        "    currentdist = np.concatenate([currentcat, currentmax, currentmin], axis=1)\n",
        "    distlist.append(currentdist)\n",
        "  return distlist\n",
        "\n",
        "mydat_trainr_cat = mydat_trainr[['gender', 'productType']]\n",
        "mydat_trainr_num = mydat_trainr.loc[:, ~mydat_trainr.columns.isin(['gender', 'productType'])]\n",
        "mydat_trainr_list = [mydat_trainr_cat, mydat_trainr_num]\n",
        "mydat_rep_cat = mydat_rep[['gender', 'productType']]\n",
        "mydat_rep_num = mydat_rep.loc[:, ~mydat_rep.columns.isin(['gender', 'productType'])]\n",
        "mydat_rep_list = [mydat_rep_cat, mydat_rep_num]\n",
        "mydat_val_cat = mydat_val_emb[['gender', 'productType']]\n",
        "mydat_val_num = mydat_val_emb.loc[:, ~mydat_val_emb.columns.isin(['gender', 'productType'])]\n",
        "mydat_val_list = [mydat_val_cat, mydat_val_num]\n",
        "mydat_cat = mydat_emb[['gender', 'productType']]\n",
        "mydat_num = mydat_emb.loc[:, ~mydat_emb.columns.isin(['gender', 'productType'])]\n",
        "mydat_list = [mydat_cat, mydat_num]\n",
        "\n",
        "\n",
        "K.clear_session()\n",
        "set_random_seed(1)\n",
        "inputlist = []\n",
        "denselist = []\n",
        "nrep = len(mydat_rep)\n",
        "for i in range(nrep):\n",
        "  inputlist.append(Input((30, ), name = 'input'+str(i)))\n",
        "  denselist.append(Dense(1, name = 'dense'+str(i))(inputlist[i]))\n",
        "\n",
        "weights = Concatenate()(denselist)\n",
        "weights = Softmax()(weights)\n",
        "output = Dense(1, use_bias = False, name = 'weightedval', trainable=False)(weights)\n",
        "m = KerasModel(inputs = inputlist, outputs = output)\n",
        "m.get_layer('weightedval').set_weights([np.reshape(np.array(mydat_rep_y), (len(mydat_rep_y), 1))])\n",
        "#this forces the weights to each multiply by the representative portfolio values\n",
        "m.compile(optimizer=Adam(), loss='mse')\n",
        "#m.summary()\n",
        "callback = EarlyStopping(monitor='val_loss', patience=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6_tkufpidvP",
        "outputId": "a2a888d1-261f-4aef-d5a5-9c4f24a9b951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 342ms/step - loss: 26157467648.0000 - val_loss: 20706230272.0000\n",
            "Epoch 2/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 26045372416.0000 - val_loss: 20628797440.0000\n",
            "Epoch 3/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 25959151616.0000 - val_loss: 20557805568.0000\n",
            "Epoch 4/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 25876733952.0000 - val_loss: 20490952704.0000\n",
            "Epoch 5/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 25796290560.0000 - val_loss: 20426905600.0000\n",
            "Epoch 6/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - loss: 25716844544.0000 - val_loss: 20364736512.0000\n",
            "Epoch 7/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 25637732352.0000 - val_loss: 20303800320.0000\n",
            "Epoch 8/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - loss: 25558499328.0000 - val_loss: 20243615744.0000\n",
            "Epoch 9/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 25478830080.0000 - val_loss: 20183824384.0000\n",
            "Epoch 10/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - loss: 25398493184.0000 - val_loss: 20124178432.0000\n",
            "Epoch 11/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 25317345280.0000 - val_loss: 20064460800.0000\n",
            "Epoch 12/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 25235261440.0000 - val_loss: 20004538368.0000\n",
            "Epoch 13/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 25152182272.0000 - val_loss: 19944290304.0000\n",
            "Epoch 14/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - loss: 25068056576.0000 - val_loss: 19883630592.0000\n",
            "Epoch 15/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 24982861824.0000 - val_loss: 19822493696.0000\n",
            "Epoch 16/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 24896575488.0000 - val_loss: 19760820224.0000\n",
            "Epoch 17/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 24809191424.0000 - val_loss: 19698581504.0000\n",
            "Epoch 18/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 24720709632.0000 - val_loss: 19635734528.0000\n",
            "Epoch 19/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 24631134208.0000 - val_loss: 19572271104.0000\n",
            "Epoch 20/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - loss: 24540473344.0000 - val_loss: 19508164608.0000\n",
            "Epoch 21/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 24448731136.0000 - val_loss: 19443398656.0000\n",
            "Epoch 22/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 24355928064.0000 - val_loss: 19377983488.0000\n",
            "Epoch 23/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 24262074368.0000 - val_loss: 19311908864.0000\n",
            "Epoch 24/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 24167186432.0000 - val_loss: 19245168640.0000\n",
            "Epoch 25/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 24071288832.0000 - val_loss: 19177773056.0000\n",
            "Epoch 26/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 23974397952.0000 - val_loss: 19109724160.0000\n",
            "Epoch 27/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 187ms/step - loss: 23876542464.0000 - val_loss: 19041028096.0000\n",
            "Epoch 28/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 196ms/step - loss: 23777738752.0000 - val_loss: 18971695104.0000\n",
            "Epoch 29/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 23678017536.0000 - val_loss: 18901723136.0000\n",
            "Epoch 30/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 23577397248.0000 - val_loss: 18831128576.0000\n",
            "Epoch 31/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 23475904512.0000 - val_loss: 18759913472.0000\n",
            "Epoch 32/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 23373561856.0000 - val_loss: 18688079872.0000\n",
            "Epoch 33/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - loss: 23270393856.0000 - val_loss: 18615644160.0000\n",
            "Epoch 34/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 23166416896.0000 - val_loss: 18542602240.0000\n",
            "Epoch 35/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 23061657600.0000 - val_loss: 18468966400.0000\n",
            "Epoch 36/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 22956132352.0000 - val_loss: 18394734592.0000\n",
            "Epoch 37/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 22849865728.0000 - val_loss: 18319917056.0000\n",
            "Epoch 38/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - loss: 22742867968.0000 - val_loss: 18244501504.0000\n",
            "Epoch 39/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 22635163648.0000 - val_loss: 18168502272.0000\n",
            "Epoch 40/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 22526771200.0000 - val_loss: 18091915264.0000\n",
            "Epoch 41/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 22417700864.0000 - val_loss: 18014732288.0000\n",
            "Epoch 42/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - loss: 22307969024.0000 - val_loss: 17936959488.0000\n",
            "Epoch 43/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - loss: 22197598208.0000 - val_loss: 17858592768.0000\n",
            "Epoch 44/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 22086594560.0000 - val_loss: 17779619840.0000\n",
            "Epoch 45/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 21974976512.0000 - val_loss: 17700050944.0000\n",
            "Epoch 46/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 21862756352.0000 - val_loss: 17619863552.0000\n",
            "Epoch 47/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - loss: 21749944320.0000 - val_loss: 17539057664.0000\n",
            "Epoch 48/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 21636552704.0000 - val_loss: 17457627136.0000\n",
            "Epoch 49/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 21522599936.0000 - val_loss: 17375563776.0000\n",
            "Epoch 50/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - loss: 21408086016.0000 - val_loss: 17292863488.0000\n",
            "Epoch 51/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 133ms/step - loss: 21293029376.0000 - val_loss: 17209509888.0000\n",
            "Epoch 52/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - loss: 21177434112.0000 - val_loss: 17125497856.0000\n",
            "Epoch 53/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 21061312512.0000 - val_loss: 17040820224.0000\n",
            "Epoch 54/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 20944666624.0000 - val_loss: 16955470848.0000\n",
            "Epoch 55/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 20827508736.0000 - val_loss: 16869435392.0000\n",
            "Epoch 56/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 20709842944.0000 - val_loss: 16782709760.0000\n",
            "Epoch 57/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 20591673344.0000 - val_loss: 16695280640.0000\n",
            "Epoch 58/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 20473004032.0000 - val_loss: 16607148032.0000\n",
            "Epoch 59/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - loss: 20353841152.0000 - val_loss: 16518300672.0000\n",
            "Epoch 60/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - loss: 20234190848.0000 - val_loss: 16428734464.0000\n",
            "Epoch 61/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 20114049024.0000 - val_loss: 16338442240.0000\n",
            "Epoch 62/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 19993425920.0000 - val_loss: 16247428096.0000\n",
            "Epoch 63/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - loss: 19872319488.0000 - val_loss: 16155674624.0000\n",
            "Epoch 64/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 19750737920.0000 - val_loss: 16063191040.0000\n",
            "Epoch 65/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - loss: 19628679168.0000 - val_loss: 15969973248.0000\n",
            "Epoch 66/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 95ms/step - loss: 19506151424.0000 - val_loss: 15876028416.0000\n",
            "Epoch 67/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - loss: 19383154688.0000 - val_loss: 15781352448.0000\n",
            "Epoch 68/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 19259695104.0000 - val_loss: 15685955584.0000\n",
            "Epoch 69/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 19135776768.0000 - val_loss: 15589842944.0000\n",
            "Epoch 70/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 19011407872.0000 - val_loss: 15493023744.0000\n",
            "Epoch 71/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 18886594560.0000 - val_loss: 15395507200.0000\n",
            "Epoch 72/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 18761347072.0000 - val_loss: 15297312768.0000\n",
            "Epoch 73/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 18635677696.0000 - val_loss: 15198445568.0000\n",
            "Epoch 74/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 18509600768.0000 - val_loss: 15098930176.0000\n",
            "Epoch 75/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 18383136768.0000 - val_loss: 14998789120.0000\n",
            "Epoch 76/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 149ms/step - loss: 18256302080.0000 - val_loss: 14898045952.0000\n",
            "Epoch 77/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 18129129472.0000 - val_loss: 14796729344.0000\n",
            "Epoch 78/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 18001649664.0000 - val_loss: 14694870016.0000\n",
            "Epoch 79/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 17873901568.0000 - val_loss: 14592522240.0000\n",
            "Epoch 80/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 17745934336.0000 - val_loss: 14489711616.0000\n",
            "Epoch 81/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - loss: 17617788928.0000 - val_loss: 14386499584.0000\n",
            "Epoch 82/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 17489534976.0000 - val_loss: 14282942464.0000\n",
            "Epoch 83/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 17361225728.0000 - val_loss: 14179108864.0000\n",
            "Epoch 84/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 17232939008.0000 - val_loss: 14075056128.0000\n",
            "Epoch 85/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - loss: 17104742400.0000 - val_loss: 13970861056.0000\n",
            "Epoch 86/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 16976713728.0000 - val_loss: 13866610688.0000\n",
            "Epoch 87/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 16848933888.0000 - val_loss: 13762374656.0000\n",
            "Epoch 88/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 16721481728.0000 - val_loss: 13658245120.0000\n",
            "Epoch 89/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 16594441216.0000 - val_loss: 13554307072.0000\n",
            "Epoch 90/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 150ms/step - loss: 16467900416.0000 - val_loss: 13450645504.0000\n",
            "Epoch 91/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - loss: 16341934080.0000 - val_loss: 13347348480.0000\n",
            "Epoch 92/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 16216622080.0000 - val_loss: 13244502016.0000\n",
            "Epoch 93/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 16092042240.0000 - val_loss: 13142187008.0000\n",
            "Epoch 94/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - loss: 15968268288.0000 - val_loss: 13040487424.0000\n",
            "Epoch 95/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - loss: 15845364736.0000 - val_loss: 12939470848.0000\n",
            "Epoch 96/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 15723394048.0000 - val_loss: 12839215104.0000\n",
            "Epoch 97/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 15602411520.0000 - val_loss: 12739775488.0000\n",
            "Epoch 98/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 15482465280.0000 - val_loss: 12641220608.0000\n",
            "Epoch 99/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - loss: 15363601408.0000 - val_loss: 12543596544.0000\n",
            "Epoch 100/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 15245849600.0000 - val_loss: 12446943232.0000\n",
            "Epoch 101/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 15129240576.0000 - val_loss: 12351303680.0000\n",
            "Epoch 102/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 15013790720.0000 - val_loss: 12256702464.0000\n",
            "Epoch 103/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 14899516416.0000 - val_loss: 12163162112.0000\n",
            "Epoch 104/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 14786421760.0000 - val_loss: 12070700032.0000\n",
            "Epoch 105/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 14674504704.0000 - val_loss: 11979317248.0000\n",
            "Epoch 106/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 14563757056.0000 - val_loss: 11889018880.0000\n",
            "Epoch 107/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 14454169600.0000 - val_loss: 11799798784.0000\n",
            "Epoch 108/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 14345718784.0000 - val_loss: 11711643648.0000\n",
            "Epoch 109/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 14238386176.0000 - val_loss: 11624537088.0000\n",
            "Epoch 110/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 14132143104.0000 - val_loss: 11538459648.0000\n",
            "Epoch 111/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - loss: 14026957824.0000 - val_loss: 11453385728.0000\n",
            "Epoch 112/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 13922800640.0000 - val_loss: 11369286656.0000\n",
            "Epoch 113/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 13819634688.0000 - val_loss: 11286126592.0000\n",
            "Epoch 114/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 13717422080.0000 - val_loss: 11203877888.0000\n",
            "Epoch 115/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 13616129024.0000 - val_loss: 11122504704.0000\n",
            "Epoch 116/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - loss: 13515714560.0000 - val_loss: 11041971200.0000\n",
            "Epoch 117/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 13416144896.0000 - val_loss: 10962235392.0000\n",
            "Epoch 118/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 13317377024.0000 - val_loss: 10883264512.0000\n",
            "Epoch 119/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 92ms/step - loss: 13219381248.0000 - val_loss: 10805022720.0000\n",
            "Epoch 120/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 150ms/step - loss: 13122115584.0000 - val_loss: 10727472128.0000\n",
            "Epoch 121/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 91ms/step - loss: 13025550336.0000 - val_loss: 10650577920.0000\n",
            "Epoch 122/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 12929648640.0000 - val_loss: 10574307328.0000\n",
            "Epoch 123/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 12834382848.0000 - val_loss: 10498625536.0000\n",
            "Epoch 124/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - loss: 12739722240.0000 - val_loss: 10423502848.0000\n",
            "Epoch 125/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - loss: 12645635072.0000 - val_loss: 10348910592.0000\n",
            "Epoch 126/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 12552098816.0000 - val_loss: 10274818048.0000\n",
            "Epoch 127/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 12459084800.0000 - val_loss: 10201200640.0000\n",
            "Epoch 128/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - loss: 12366569472.0000 - val_loss: 10128031744.0000\n",
            "Epoch 129/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - loss: 12274530304.0000 - val_loss: 10055289856.0000\n",
            "Epoch 130/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 12182940672.0000 - val_loss: 9982951424.0000\n",
            "Epoch 131/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 12091785216.0000 - val_loss: 9910995968.0000\n",
            "Epoch 132/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 12001039360.0000 - val_loss: 9839405056.0000\n",
            "Epoch 133/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 11910684672.0000 - val_loss: 9768157184.0000\n",
            "Epoch 134/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - loss: 11820701696.0000 - val_loss: 9697239040.0000\n",
            "Epoch 135/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 11731070976.0000 - val_loss: 9626629120.0000\n",
            "Epoch 136/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 11641773056.0000 - val_loss: 9556317184.0000\n",
            "Epoch 137/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 11552792576.0000 - val_loss: 9486284800.0000\n",
            "Epoch 138/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - loss: 11464108032.0000 - val_loss: 9416516608.0000\n",
            "Epoch 139/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 11375706112.0000 - val_loss: 9347003392.0000\n",
            "Epoch 140/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 11287569408.0000 - val_loss: 9277730816.0000\n",
            "Epoch 141/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 11199680512.0000 - val_loss: 9208687616.0000\n",
            "Epoch 142/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 116ms/step - loss: 11112023040.0000 - val_loss: 9139861504.0000\n",
            "Epoch 143/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 125ms/step - loss: 11024581632.0000 - val_loss: 9071243264.0000\n",
            "Epoch 144/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 10937344000.0000 - val_loss: 9002823680.0000\n",
            "Epoch 145/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 10850294784.0000 - val_loss: 8934590464.0000\n",
            "Epoch 146/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 10763417600.0000 - val_loss: 8866541568.0000\n",
            "Epoch 147/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 90ms/step - loss: 10676704256.0000 - val_loss: 8798664704.0000\n",
            "Epoch 148/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 10590144512.0000 - val_loss: 8730957824.0000\n",
            "Epoch 149/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 10503724032.0000 - val_loss: 8663412736.0000\n",
            "Epoch 150/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 10417433600.0000 - val_loss: 8596028416.0000\n",
            "Epoch 151/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - loss: 10331267072.0000 - val_loss: 8528799744.0000\n",
            "Epoch 152/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - loss: 10245219328.0000 - val_loss: 8461729280.0000\n",
            "Epoch 153/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 10159281152.0000 - val_loss: 8394812416.0000\n",
            "Epoch 154/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 10073452544.0000 - val_loss: 8328050176.0000\n",
            "Epoch 155/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 9987728384.0000 - val_loss: 8261446656.0000\n",
            "Epoch 156/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 9902111744.0000 - val_loss: 8195007488.0000\n",
            "Epoch 157/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 9816602624.0000 - val_loss: 8128736256.0000\n",
            "Epoch 158/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 9731207168.0000 - val_loss: 8062639104.0000\n",
            "Epoch 159/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 9645927424.0000 - val_loss: 7996727296.0000\n",
            "Epoch 160/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 9560774656.0000 - val_loss: 7931006464.0000\n",
            "Epoch 161/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 9475760128.0000 - val_loss: 7865493504.0000\n",
            "Epoch 162/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 9390895104.0000 - val_loss: 7800200192.0000\n",
            "Epoch 163/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 9306193920.0000 - val_loss: 7735139328.0000\n",
            "Epoch 164/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 9221677056.0000 - val_loss: 7670327296.0000\n",
            "Epoch 165/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 9137364992.0000 - val_loss: 7605783552.0000\n",
            "Epoch 166/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 9053276160.0000 - val_loss: 7541527552.0000\n",
            "Epoch 167/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 8969437184.0000 - val_loss: 7477572608.0000\n",
            "Epoch 168/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - loss: 8885872640.0000 - val_loss: 7413947904.0000\n",
            "Epoch 169/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 8802610176.0000 - val_loss: 7350671360.0000\n",
            "Epoch 170/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 8719676416.0000 - val_loss: 7287761920.0000\n",
            "Epoch 171/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 8637104128.0000 - val_loss: 7225246208.0000\n",
            "Epoch 172/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 8554923008.0000 - val_loss: 7163141632.0000\n",
            "Epoch 173/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 8473162240.0000 - val_loss: 7101475328.0000\n",
            "Epoch 174/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 8391852032.0000 - val_loss: 7040261632.0000\n",
            "Epoch 175/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 8311026176.0000 - val_loss: 6979526144.0000\n",
            "Epoch 176/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 8230708224.0000 - val_loss: 6919286784.0000\n",
            "Epoch 177/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - loss: 8150933504.0000 - val_loss: 6859563008.0000\n",
            "Epoch 178/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 8071727104.0000 - val_loss: 6800373248.0000\n",
            "Epoch 179/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 7993117184.0000 - val_loss: 6741731328.0000\n",
            "Epoch 180/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 7915126784.0000 - val_loss: 6683655168.0000\n",
            "Epoch 181/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 7837782016.0000 - val_loss: 6626156032.0000\n",
            "Epoch 182/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - loss: 7761103872.0000 - val_loss: 6569248256.0000\n",
            "Epoch 183/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - loss: 7685111296.0000 - val_loss: 6512942080.0000\n",
            "Epoch 184/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 7609823744.0000 - val_loss: 6457246720.0000\n",
            "Epoch 185/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 7535257088.0000 - val_loss: 6402170368.0000\n",
            "Epoch 186/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - loss: 7461427200.0000 - val_loss: 6347720704.0000\n",
            "Epoch 187/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 7388347392.0000 - val_loss: 6293900800.0000\n",
            "Epoch 188/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 7316026880.0000 - val_loss: 6240717824.0000\n",
            "Epoch 189/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 7244476416.0000 - val_loss: 6188176896.0000\n",
            "Epoch 190/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - loss: 7173703680.0000 - val_loss: 6136274944.0000\n",
            "Epoch 191/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - loss: 7103714304.0000 - val_loss: 6085018112.0000\n",
            "Epoch 192/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 7034514432.0000 - val_loss: 6034407936.0000\n",
            "Epoch 193/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 6966109696.0000 - val_loss: 5984442368.0000\n",
            "Epoch 194/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - loss: 6898497536.0000 - val_loss: 5935121920.0000\n",
            "Epoch 195/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 92ms/step - loss: 6831686144.0000 - val_loss: 5886449152.0000\n",
            "Epoch 196/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 6765671424.0000 - val_loss: 5838419968.0000\n",
            "Epoch 197/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 6700455936.0000 - val_loss: 5791036416.0000\n",
            "Epoch 198/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 6636039168.0000 - val_loss: 5744296960.0000\n",
            "Epoch 199/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - loss: 6572420096.0000 - val_loss: 5698202624.0000\n",
            "Epoch 200/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 6509597184.0000 - val_loss: 5652749312.0000\n",
            "Epoch 201/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 6447569408.0000 - val_loss: 5607940608.0000\n",
            "Epoch 202/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 6386334208.0000 - val_loss: 5563776000.0000\n",
            "Epoch 203/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 6325889536.0000 - val_loss: 5520255488.0000\n",
            "Epoch 204/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 6266234368.0000 - val_loss: 5477378048.0000\n",
            "Epoch 205/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 6207365120.0000 - val_loss: 5435148288.0000\n",
            "Epoch 206/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 6149280256.0000 - val_loss: 5393564672.0000\n",
            "Epoch 207/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 6091980288.0000 - val_loss: 5352630272.0000\n",
            "Epoch 208/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 6035460096.0000 - val_loss: 5312345600.0000\n",
            "Epoch 209/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 5979720192.0000 - val_loss: 5272716288.0000\n",
            "Epoch 210/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 5924756480.0000 - val_loss: 5233742848.0000\n",
            "Epoch 211/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 5870570496.0000 - val_loss: 5195427840.0000\n",
            "Epoch 212/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 5817158656.0000 - val_loss: 5157775872.0000\n",
            "Epoch 213/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 5764519424.0000 - val_loss: 5120788992.0000\n",
            "Epoch 214/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 5712653312.0000 - val_loss: 5084469760.0000\n",
            "Epoch 215/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 5661557760.0000 - val_loss: 5048822784.0000\n",
            "Epoch 216/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 5611233792.0000 - val_loss: 5013849088.0000\n",
            "Epoch 217/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 5561680896.0000 - val_loss: 4979555328.0000\n",
            "Epoch 218/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 5512892928.0000 - val_loss: 4945939968.0000\n",
            "Epoch 219/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 5464872960.0000 - val_loss: 4913005568.0000\n",
            "Epoch 220/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 5417620480.0000 - val_loss: 4880756736.0000\n",
            "Epoch 221/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 5371129344.0000 - val_loss: 4849191936.0000\n",
            "Epoch 222/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 5325401600.0000 - val_loss: 4818312192.0000\n",
            "Epoch 223/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 5280432128.0000 - val_loss: 4788116992.0000\n",
            "Epoch 224/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 5236221440.0000 - val_loss: 4758609408.0000\n",
            "Epoch 225/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 5192762880.0000 - val_loss: 4729782272.0000\n",
            "Epoch 226/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 5150057984.0000 - val_loss: 4701635584.0000\n",
            "Epoch 227/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 5108099072.0000 - val_loss: 4674168320.0000\n",
            "Epoch 228/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 5066885120.0000 - val_loss: 4647375360.0000\n",
            "Epoch 229/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 5026408448.0000 - val_loss: 4621250560.0000\n",
            "Epoch 230/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 4986666496.0000 - val_loss: 4595789312.0000\n",
            "Epoch 231/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 4947652608.0000 - val_loss: 4570985472.0000\n",
            "Epoch 232/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - loss: 4909360640.0000 - val_loss: 4546832896.0000\n",
            "Epoch 233/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - loss: 4871782400.0000 - val_loss: 4523322368.0000\n",
            "Epoch 234/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 4834913280.0000 - val_loss: 4500444160.0000\n",
            "Epoch 235/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 4798743040.0000 - val_loss: 4478189568.0000\n",
            "Epoch 236/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 4763265024.0000 - val_loss: 4456551424.0000\n",
            "Epoch 237/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - loss: 4728470528.0000 - val_loss: 4435514880.0000\n",
            "Epoch 238/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 4694351360.0000 - val_loss: 4415072256.0000\n",
            "Epoch 239/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 4660894720.0000 - val_loss: 4395208704.0000\n",
            "Epoch 240/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 4628093440.0000 - val_loss: 4375910912.0000\n",
            "Epoch 241/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - loss: 4595935232.0000 - val_loss: 4357171712.0000\n",
            "Epoch 242/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 4564410368.0000 - val_loss: 4338973696.0000\n",
            "Epoch 243/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 4533508608.0000 - val_loss: 4321304576.0000\n",
            "Epoch 244/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 4503216128.0000 - val_loss: 4304152064.0000\n",
            "Epoch 245/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 4473524736.0000 - val_loss: 4287501568.0000\n",
            "Epoch 246/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 4444418560.0000 - val_loss: 4271341568.0000\n",
            "Epoch 247/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 4415889408.0000 - val_loss: 4255657216.0000\n",
            "Epoch 248/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 4387924480.0000 - val_loss: 4240432384.0000\n",
            "Epoch 249/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 4360509440.0000 - val_loss: 4225656064.0000\n",
            "Epoch 250/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 4333632512.0000 - val_loss: 4211314432.0000\n",
            "Epoch 251/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 4307282944.0000 - val_loss: 4197396224.0000\n",
            "Epoch 252/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 90ms/step - loss: 4281445888.0000 - val_loss: 4183883008.0000\n",
            "Epoch 253/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 4256110592.0000 - val_loss: 4170767616.0000\n",
            "Epoch 254/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - loss: 4231266048.0000 - val_loss: 4158034176.0000\n",
            "Epoch 255/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 4206896640.0000 - val_loss: 4145669632.0000\n",
            "Epoch 256/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 4182992128.0000 - val_loss: 4133662464.0000\n",
            "Epoch 257/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 4159538432.0000 - val_loss: 4122000896.0000\n",
            "Epoch 258/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - loss: 4136526336.0000 - val_loss: 4110674688.0000\n",
            "Epoch 259/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 92ms/step - loss: 4113941248.0000 - val_loss: 4099670016.0000\n",
            "Epoch 260/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 4091772416.0000 - val_loss: 4088976896.0000\n",
            "Epoch 261/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 4070008064.0000 - val_loss: 4078584832.0000\n",
            "Epoch 262/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - loss: 4048635904.0000 - val_loss: 4068484096.0000\n",
            "Epoch 263/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 134ms/step - loss: 4027645952.0000 - val_loss: 4058662144.0000\n",
            "Epoch 264/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - loss: 4007026176.0000 - val_loss: 4049110272.0000\n",
            "Epoch 265/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 3986764288.0000 - val_loss: 4039819520.0000\n",
            "Epoch 266/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 3966850816.0000 - val_loss: 4030780928.0000\n",
            "Epoch 267/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 149ms/step - loss: 3947276032.0000 - val_loss: 4021985280.0000\n",
            "Epoch 268/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - loss: 3928026368.0000 - val_loss: 4013421824.0000\n",
            "Epoch 269/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 3909093376.0000 - val_loss: 4005085696.0000\n",
            "Epoch 270/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 3890468096.0000 - val_loss: 3996967424.0000\n",
            "Epoch 271/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - loss: 3872138752.0000 - val_loss: 3989061120.0000\n",
            "Epoch 272/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - loss: 3854096384.0000 - val_loss: 3981355520.0000\n",
            "Epoch 273/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 3836333312.0000 - val_loss: 3973845248.0000\n",
            "Epoch 274/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 3818835712.0000 - val_loss: 3966523648.0000\n",
            "Epoch 275/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 3801598464.0000 - val_loss: 3959382784.0000\n",
            "Epoch 276/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - loss: 3784612096.0000 - val_loss: 3952415232.0000\n",
            "Epoch 277/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - loss: 3767865088.0000 - val_loss: 3945617664.0000\n",
            "Epoch 278/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3751352832.0000 - val_loss: 3938981888.0000\n",
            "Epoch 279/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 3735065600.0000 - val_loss: 3932502016.0000\n",
            "Epoch 280/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - loss: 3718993408.0000 - val_loss: 3926173440.0000\n",
            "Epoch 281/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 3703130624.0000 - val_loss: 3919990784.0000\n",
            "Epoch 282/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3687469824.0000 - val_loss: 3913946368.0000\n",
            "Epoch 283/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3672002048.0000 - val_loss: 3908037632.0000\n",
            "Epoch 284/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3656721408.0000 - val_loss: 3902257920.0000\n",
            "Epoch 285/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - loss: 3641617664.0000 - val_loss: 3896600576.0000\n",
            "Epoch 286/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 3626687232.0000 - val_loss: 3891065088.0000\n",
            "Epoch 287/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3611922688.0000 - val_loss: 3885641472.0000\n",
            "Epoch 288/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 3597316096.0000 - val_loss: 3880331008.0000\n",
            "Epoch 289/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 3582863104.0000 - val_loss: 3875123200.0000\n",
            "Epoch 290/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - loss: 3568555008.0000 - val_loss: 3870016768.0000\n",
            "Epoch 291/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3554387456.0000 - val_loss: 3865008640.0000\n",
            "Epoch 292/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3540356352.0000 - val_loss: 3860091136.0000\n",
            "Epoch 293/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 3526449152.0000 - val_loss: 3855263232.0000\n",
            "Epoch 294/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - loss: 3512666624.0000 - val_loss: 3850519808.0000\n",
            "Epoch 295/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3499002368.0000 - val_loss: 3845855232.0000\n",
            "Epoch 296/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3485449216.0000 - val_loss: 3841267712.0000\n",
            "Epoch 297/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 3472003584.0000 - val_loss: 3836752640.0000\n",
            "Epoch 298/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 149ms/step - loss: 3458659840.0000 - val_loss: 3832305408.0000\n",
            "Epoch 299/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 3445412352.0000 - val_loss: 3827921408.0000\n",
            "Epoch 300/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3432256512.0000 - val_loss: 3823596032.0000\n",
            "Epoch 301/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 3419189760.0000 - val_loss: 3819326976.0000\n",
            "Epoch 302/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 3406203904.0000 - val_loss: 3815111168.0000\n",
            "Epoch 303/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - loss: 3393298432.0000 - val_loss: 3810942208.0000\n",
            "Epoch 304/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3380466432.0000 - val_loss: 3806816512.0000\n",
            "Epoch 305/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 3367703552.0000 - val_loss: 3802728704.0000\n",
            "Epoch 306/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 3355006208.0000 - val_loss: 3798678272.0000\n",
            "Epoch 307/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 3342370048.0000 - val_loss: 3794654976.0000\n",
            "Epoch 308/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - loss: 3329791744.0000 - val_loss: 3790658048.0000\n",
            "Epoch 309/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - loss: 3317265408.0000 - val_loss: 3786680832.0000\n",
            "Epoch 310/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 3304788480.0000 - val_loss: 3782719488.0000\n",
            "Epoch 311/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 3292355072.0000 - val_loss: 3778766080.0000\n",
            "Epoch 312/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 3279964160.0000 - val_loss: 3774819328.0000\n",
            "Epoch 313/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 3267608576.0000 - val_loss: 3770867712.0000\n",
            "Epoch 314/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 3255284992.0000 - val_loss: 3766910976.0000\n",
            "Epoch 315/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 3242989056.0000 - val_loss: 3762934784.0000\n",
            "Epoch 316/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - loss: 3230717696.0000 - val_loss: 3758939392.0000\n",
            "Epoch 317/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - loss: 3218464000.0000 - val_loss: 3754912512.0000\n",
            "Epoch 318/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 3206225152.0000 - val_loss: 3750848768.0000\n",
            "Epoch 319/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - loss: 3193996032.0000 - val_loss: 3746738688.0000\n",
            "Epoch 320/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 136ms/step - loss: 3181772032.0000 - val_loss: 3742572032.0000\n",
            "Epoch 321/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 3169545472.0000 - val_loss: 3738339584.0000\n",
            "Epoch 322/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3157314560.0000 - val_loss: 3734031104.0000\n",
            "Epoch 323/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 3145069824.0000 - val_loss: 3729636096.0000\n",
            "Epoch 324/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 129ms/step - loss: 3132809728.0000 - val_loss: 3725142528.0000\n",
            "Epoch 325/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - loss: 3120526080.0000 - val_loss: 3720538368.0000\n",
            "Epoch 326/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3108213504.0000 - val_loss: 3715812608.0000\n",
            "Epoch 327/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 3095866368.0000 - val_loss: 3710950144.0000\n",
            "Epoch 328/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 142ms/step - loss: 3083478272.0000 - val_loss: 3705937152.0000\n",
            "Epoch 329/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 3071043072.0000 - val_loss: 3700760832.0000\n",
            "Epoch 330/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3058553856.0000 - val_loss: 3695408128.0000\n",
            "Epoch 331/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3046009856.0000 - val_loss: 3689864192.0000\n",
            "Epoch 332/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - loss: 3033398272.0000 - val_loss: 3684118528.0000\n",
            "Epoch 333/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - loss: 3020721152.0000 - val_loss: 3678157824.0000\n",
            "Epoch 334/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 3007970048.0000 - val_loss: 3671971584.0000\n",
            "Epoch 335/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 2995144704.0000 - val_loss: 3665550336.0000\n",
            "Epoch 336/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 129ms/step - loss: 2982240256.0000 - val_loss: 3658887936.0000\n",
            "Epoch 337/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 2969259520.0000 - val_loss: 3651980800.0000\n",
            "Epoch 338/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 2956201728.0000 - val_loss: 3644827136.0000\n",
            "Epoch 339/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 2943069440.0000 - val_loss: 3637432320.0000\n",
            "Epoch 340/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 2929869824.0000 - val_loss: 3629802240.0000\n",
            "Epoch 341/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 2916607744.0000 - val_loss: 3621948928.0000\n",
            "Epoch 342/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 2903296512.0000 - val_loss: 3613888768.0000\n",
            "Epoch 343/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 2889944576.0000 - val_loss: 3605645824.0000\n",
            "Epoch 344/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 2876569088.0000 - val_loss: 3597244928.0000\n",
            "Epoch 345/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 2863185920.0000 - val_loss: 3588719360.0000\n",
            "Epoch 346/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 2849815040.0000 - val_loss: 3580103680.0000\n",
            "Epoch 347/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 2836477184.0000 - val_loss: 3571441408.0000\n",
            "Epoch 348/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - loss: 2823193088.0000 - val_loss: 3562769920.0000\n",
            "Epoch 349/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 2809987072.0000 - val_loss: 3554132224.0000\n",
            "Epoch 350/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2796879872.0000 - val_loss: 3545570560.0000\n",
            "Epoch 351/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 2783892480.0000 - val_loss: 3537125888.0000\n",
            "Epoch 352/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - loss: 2771043072.0000 - val_loss: 3528836352.0000\n",
            "Epoch 353/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - loss: 2758352128.0000 - val_loss: 3520735488.0000\n",
            "Epoch 354/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 2745829376.0000 - val_loss: 3512849152.0000\n",
            "Epoch 355/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 2733487360.0000 - val_loss: 3505200640.0000\n",
            "Epoch 356/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 2721331456.0000 - val_loss: 3497805056.0000\n",
            "Epoch 357/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 2709364736.0000 - val_loss: 3490678016.0000\n",
            "Epoch 358/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - loss: 2697586432.0000 - val_loss: 3483819264.0000\n",
            "Epoch 359/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 2685994496.0000 - val_loss: 3477229056.0000\n",
            "Epoch 360/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 2674581248.0000 - val_loss: 3470906880.0000\n",
            "Epoch 361/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2663339008.0000 - val_loss: 3464838144.0000\n",
            "Epoch 362/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - loss: 2652259072.0000 - val_loss: 3459015424.0000\n",
            "Epoch 363/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 2641331968.0000 - val_loss: 3453425408.0000\n",
            "Epoch 364/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 2630547456.0000 - val_loss: 3448050176.0000\n",
            "Epoch 365/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 124ms/step - loss: 2619894784.0000 - val_loss: 3442877440.0000\n",
            "Epoch 366/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 130ms/step - loss: 2609368320.0000 - val_loss: 3437889536.0000\n",
            "Epoch 367/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 2598955008.0000 - val_loss: 3433074432.0000\n",
            "Epoch 368/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2588647424.0000 - val_loss: 3428415232.0000\n",
            "Epoch 369/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 93ms/step - loss: 2578440704.0000 - val_loss: 3423894784.0000\n",
            "Epoch 370/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - loss: 2568326144.0000 - val_loss: 3419504640.0000\n",
            "Epoch 371/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 95ms/step - loss: 2558298880.0000 - val_loss: 3415228672.0000\n",
            "Epoch 372/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2548352512.0000 - val_loss: 3411055104.0000\n",
            "Epoch 373/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2538482688.0000 - val_loss: 3406975232.0000\n",
            "Epoch 374/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 2528685056.0000 - val_loss: 3402976000.0000\n",
            "Epoch 375/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - loss: 2518956800.0000 - val_loss: 3399048448.0000\n",
            "Epoch 376/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2509294336.0000 - val_loss: 3395184896.0000\n",
            "Epoch 377/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2499691776.0000 - val_loss: 3391376128.0000\n",
            "Epoch 378/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 2490150144.0000 - val_loss: 3387617792.0000\n",
            "Epoch 379/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 2480665088.0000 - val_loss: 3383899392.0000\n",
            "Epoch 380/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - loss: 2471233792.0000 - val_loss: 3380216576.0000\n",
            "Epoch 381/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2461854464.0000 - val_loss: 3376563456.0000\n",
            "Epoch 382/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2452527360.0000 - val_loss: 3372933632.0000\n",
            "Epoch 383/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - loss: 2443247104.0000 - val_loss: 3369321728.0000\n",
            "Epoch 384/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 93ms/step - loss: 2434016512.0000 - val_loss: 3365725696.0000\n",
            "Epoch 385/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2424828928.0000 - val_loss: 3362140928.0000\n",
            "Epoch 386/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2415687424.0000 - val_loss: 3358562048.0000\n",
            "Epoch 387/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 2406588928.0000 - val_loss: 3354985216.0000\n",
            "Epoch 388/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 2397532672.0000 - val_loss: 3351409664.0000\n",
            "Epoch 389/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 2388517376.0000 - val_loss: 3347829760.0000\n",
            "Epoch 390/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 2379543040.0000 - val_loss: 3344245504.0000\n",
            "Epoch 391/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2370609152.0000 - val_loss: 3340650240.0000\n",
            "Epoch 392/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - loss: 2361713664.0000 - val_loss: 3337044480.0000\n",
            "Epoch 393/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step - loss: 2352856576.0000 - val_loss: 3333425920.0000\n",
            "Epoch 394/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 2344037120.0000 - val_loss: 3329792512.0000\n",
            "Epoch 395/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 2335254784.0000 - val_loss: 3326140416.0000\n",
            "Epoch 396/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - loss: 2326509824.0000 - val_loss: 3322472448.0000\n",
            "Epoch 397/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 93ms/step - loss: 2317799936.0000 - val_loss: 3318784000.0000\n",
            "Epoch 398/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 2309128704.0000 - val_loss: 3315075328.0000\n",
            "Epoch 399/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 91ms/step - loss: 2300493056.0000 - val_loss: 3311344128.0000\n",
            "Epoch 400/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - loss: 2291894016.0000 - val_loss: 3307592960.0000\n",
            "Epoch 401/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 143ms/step - loss: 2283330304.0000 - val_loss: 3303819008.0000\n",
            "Epoch 402/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - loss: 2274802944.0000 - val_loss: 3300021248.0000\n",
            "Epoch 403/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 2266312448.0000 - val_loss: 3296201472.0000\n",
            "Epoch 404/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 2257857280.0000 - val_loss: 3292358912.0000\n",
            "Epoch 405/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - loss: 2249440512.0000 - val_loss: 3288493824.0000\n",
            "Epoch 406/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 2241060096.0000 - val_loss: 3284607744.0000\n",
            "Epoch 407/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - loss: 2232718080.0000 - val_loss: 3280699904.0000\n",
            "Epoch 408/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 2224413184.0000 - val_loss: 3276771072.0000\n",
            "Epoch 409/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2216146944.0000 - val_loss: 3272824576.0000\n",
            "Epoch 410/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - loss: 2207919872.0000 - val_loss: 3268857856.0000\n",
            "Epoch 411/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - loss: 2199732480.0000 - val_loss: 3264879104.0000\n",
            "Epoch 412/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 2191585536.0000 - val_loss: 3260882432.0000\n",
            "Epoch 413/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 2183480064.0000 - val_loss: 3256872704.0000\n",
            "Epoch 414/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - loss: 2175415040.0000 - val_loss: 3252853504.0000\n",
            "Epoch 415/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - loss: 2167393024.0000 - val_loss: 3248823808.0000\n",
            "Epoch 416/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 2159413504.0000 - val_loss: 3244787200.0000\n",
            "Epoch 417/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2151476736.0000 - val_loss: 3240746240.0000\n",
            "Epoch 418/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 2143583744.0000 - val_loss: 3236700160.0000\n",
            "Epoch 419/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - loss: 2135737600.0000 - val_loss: 3232651008.0000\n",
            "Epoch 420/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 98ms/step - loss: 2127934464.0000 - val_loss: 3228603904.0000\n",
            "Epoch 421/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 2120177664.0000 - val_loss: 3224559360.0000\n",
            "Epoch 422/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 2112466176.0000 - val_loss: 3220518656.0000\n",
            "Epoch 423/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - loss: 2104801664.0000 - val_loss: 3216485120.0000\n",
            "Epoch 424/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 2097184512.0000 - val_loss: 3212456448.0000\n",
            "Epoch 425/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 2089613824.0000 - val_loss: 3208440064.0000\n",
            "Epoch 426/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 2082089856.0000 - val_loss: 3204433152.0000\n",
            "Epoch 427/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - loss: 2074613376.0000 - val_loss: 3200436224.0000\n",
            "Epoch 428/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 95ms/step - loss: 2067184896.0000 - val_loss: 3196456704.0000\n",
            "Epoch 429/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 2059803008.0000 - val_loss: 3192486656.0000\n",
            "Epoch 430/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 2052468992.0000 - val_loss: 3188535808.0000\n",
            "Epoch 431/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - loss: 2045182592.0000 - val_loss: 3184598016.0000\n",
            "Epoch 432/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 2037942400.0000 - val_loss: 3180678144.0000\n",
            "Epoch 433/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 2030749952.0000 - val_loss: 3176774400.0000\n",
            "Epoch 434/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 2023604480.0000 - val_loss: 3172887296.0000\n",
            "Epoch 435/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 2016504960.0000 - val_loss: 3169018368.0000\n",
            "Epoch 436/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 94ms/step - loss: 2009452544.0000 - val_loss: 3165167360.0000\n",
            "Epoch 437/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 2002445568.0000 - val_loss: 3161331968.0000\n",
            "Epoch 438/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 1995485440.0000 - val_loss: 3157515520.0000\n",
            "Epoch 439/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 1988569984.0000 - val_loss: 3153716736.0000\n",
            "Epoch 440/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 1981699200.0000 - val_loss: 3149936128.0000\n",
            "Epoch 441/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 94ms/step - loss: 1974874752.0000 - val_loss: 3146174720.0000\n",
            "Epoch 442/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 1968095232.0000 - val_loss: 3142425600.0000\n",
            "Epoch 443/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 1961358720.0000 - val_loss: 3138693376.0000\n",
            "Epoch 444/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 1954666240.0000 - val_loss: 3134978304.0000\n",
            "Epoch 445/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 1948015616.0000 - val_loss: 3131278080.0000\n",
            "Epoch 446/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 1941410944.0000 - val_loss: 3127594240.0000\n",
            "Epoch 447/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1934847744.0000 - val_loss: 3123924224.0000\n",
            "Epoch 448/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 1928326400.0000 - val_loss: 3120267776.0000\n",
            "Epoch 449/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1921848704.0000 - val_loss: 3116628224.0000\n",
            "Epoch 450/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 1915411968.0000 - val_loss: 3112999936.0000\n",
            "Epoch 451/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 1909017088.0000 - val_loss: 3109383936.0000\n",
            "Epoch 452/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - loss: 1902663424.0000 - val_loss: 3105780224.0000\n",
            "Epoch 453/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - loss: 1896350208.0000 - val_loss: 3102189312.0000\n",
            "Epoch 454/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 1890077824.0000 - val_loss: 3098611200.0000\n",
            "Epoch 455/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 1883846400.0000 - val_loss: 3095044096.0000\n",
            "Epoch 456/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - loss: 1877655168.0000 - val_loss: 3091487232.0000\n",
            "Epoch 457/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 1871503872.0000 - val_loss: 3087941376.0000\n",
            "Epoch 458/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1865391232.0000 - val_loss: 3084406272.0000\n",
            "Epoch 459/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 95ms/step - loss: 1859317760.0000 - val_loss: 3080880384.0000\n",
            "Epoch 460/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1853282944.0000 - val_loss: 3077365760.0000\n",
            "Epoch 461/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - loss: 1847288064.0000 - val_loss: 3073858560.0000\n",
            "Epoch 462/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 129ms/step - loss: 1841330560.0000 - val_loss: 3070362368.0000\n",
            "Epoch 463/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1835410944.0000 - val_loss: 3066876928.0000\n",
            "Epoch 464/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 1829529216.0000 - val_loss: 3063398912.0000\n",
            "Epoch 465/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1823685376.0000 - val_loss: 3059928832.0000\n",
            "Epoch 466/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - loss: 1817878912.0000 - val_loss: 3056466944.0000\n",
            "Epoch 467/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - loss: 1812108544.0000 - val_loss: 3053014528.0000\n",
            "Epoch 468/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1806375296.0000 - val_loss: 3049572352.0000\n",
            "Epoch 469/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - loss: 1800677760.0000 - val_loss: 3046136832.0000\n",
            "Epoch 470/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - loss: 1795016960.0000 - val_loss: 3042710528.0000\n",
            "Epoch 471/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - loss: 1789392000.0000 - val_loss: 3039291136.0000\n",
            "Epoch 472/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 1783802624.0000 - val_loss: 3035881728.0000\n",
            "Epoch 473/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 1778247552.0000 - val_loss: 3032479488.0000\n",
            "Epoch 474/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - loss: 1772728704.0000 - val_loss: 3029085184.0000\n",
            "Epoch 475/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 1767243648.0000 - val_loss: 3025700352.0000\n",
            "Epoch 476/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 1761794048.0000 - val_loss: 3022321408.0000\n",
            "Epoch 477/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 1756376576.0000 - val_loss: 3018952960.0000\n",
            "Epoch 478/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - loss: 1750995072.0000 - val_loss: 3015592704.0000\n",
            "Epoch 479/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 1745647232.0000 - val_loss: 3012239872.0000\n",
            "Epoch 480/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1740332160.0000 - val_loss: 3008894720.0000\n",
            "Epoch 481/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 1735049344.0000 - val_loss: 3005557248.0000\n",
            "Epoch 482/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - loss: 1729799936.0000 - val_loss: 3002228736.0000\n",
            "Epoch 483/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - loss: 1724582016.0000 - val_loss: 2998907392.0000\n",
            "Epoch 484/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1719396608.0000 - val_loss: 2995594752.0000\n",
            "Epoch 485/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1714244224.0000 - val_loss: 2992289280.0000\n",
            "Epoch 486/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 129ms/step - loss: 1709121536.0000 - val_loss: 2988993280.0000\n",
            "Epoch 487/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1704032256.0000 - val_loss: 2985706240.0000\n",
            "Epoch 488/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 1698972032.0000 - val_loss: 2982425344.0000\n",
            "Epoch 489/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1693943552.0000 - val_loss: 2979152640.0000\n",
            "Epoch 490/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - loss: 1688945280.0000 - val_loss: 2975890176.0000\n",
            "Epoch 491/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 1683977984.0000 - val_loss: 2972635392.0000\n",
            "Epoch 492/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1679039488.0000 - val_loss: 2969387520.0000\n",
            "Epoch 493/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1674130944.0000 - val_loss: 2966148864.0000\n",
            "Epoch 494/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 1669251200.0000 - val_loss: 2962919168.0000\n",
            "Epoch 495/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 135ms/step - loss: 1664401152.0000 - val_loss: 2959696896.0000\n",
            "Epoch 496/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 1659579136.0000 - val_loss: 2956484864.0000\n",
            "Epoch 497/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1654785920.0000 - val_loss: 2953279744.0000\n",
            "Epoch 498/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 1650020864.0000 - val_loss: 2950083584.0000\n",
            "Epoch 499/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 1645283200.0000 - val_loss: 2946894080.0000\n",
            "Epoch 500/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - loss: 1640574080.0000 - val_loss: 2943714304.0000\n",
            "Epoch 501/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1635891840.0000 - val_loss: 2940544256.0000\n",
            "Epoch 502/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1631236096.0000 - val_loss: 2937380608.0000\n",
            "Epoch 503/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 1626607360.0000 - val_loss: 2934224128.0000\n",
            "Epoch 504/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 1622005120.0000 - val_loss: 2931079168.0000\n",
            "Epoch 505/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1617429504.0000 - val_loss: 2927942144.0000\n",
            "Epoch 506/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1612878976.0000 - val_loss: 2924813568.0000\n",
            "Epoch 507/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - loss: 1608355072.0000 - val_loss: 2921691136.0000\n",
            "Epoch 508/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - loss: 1603855488.0000 - val_loss: 2918577664.0000\n",
            "Epoch 509/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 100ms/step - loss: 1599380480.0000 - val_loss: 2915473664.0000\n",
            "Epoch 510/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1594932224.0000 - val_loss: 2912377600.0000\n",
            "Epoch 511/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - loss: 1590507008.0000 - val_loss: 2909291008.0000\n",
            "Epoch 512/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - loss: 1586106368.0000 - val_loss: 2906212096.0000\n",
            "Epoch 513/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 1581729536.0000 - val_loss: 2903140864.0000\n",
            "Epoch 514/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 1577376896.0000 - val_loss: 2900079104.0000\n",
            "Epoch 515/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 149ms/step - loss: 1573047936.0000 - val_loss: 2897026304.0000\n",
            "Epoch 516/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - loss: 1568740864.0000 - val_loss: 2893980928.0000\n",
            "Epoch 517/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1564458240.0000 - val_loss: 2890944000.0000\n",
            "Epoch 518/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1560197888.0000 - val_loss: 2887912192.0000\n",
            "Epoch 519/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 1555959168.0000 - val_loss: 2884891136.0000\n",
            "Epoch 520/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1551743616.0000 - val_loss: 2881878784.0000\n",
            "Epoch 521/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 1547549184.0000 - val_loss: 2878873344.0000\n",
            "Epoch 522/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - loss: 1543376384.0000 - val_loss: 2875875584.0000\n",
            "Epoch 523/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 1539225472.0000 - val_loss: 2872889088.0000\n",
            "Epoch 524/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1535095680.0000 - val_loss: 2869908480.0000\n",
            "Epoch 525/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 122ms/step - loss: 1530986624.0000 - val_loss: 2866935040.0000\n",
            "Epoch 526/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 134ms/step - loss: 1526897536.0000 - val_loss: 2863969536.0000\n",
            "Epoch 527/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - loss: 1522829440.0000 - val_loss: 2861011456.0000\n",
            "Epoch 528/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 1518779648.0000 - val_loss: 2858062848.0000\n",
            "Epoch 529/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - loss: 1514751872.0000 - val_loss: 2855120640.0000\n",
            "Epoch 530/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 1510742784.0000 - val_loss: 2852187136.0000\n",
            "Epoch 531/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 1506752256.0000 - val_loss: 2849260544.0000\n",
            "Epoch 532/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - loss: 1502781696.0000 - val_loss: 2846341632.0000\n",
            "Epoch 533/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 1498829952.0000 - val_loss: 2843429376.0000\n",
            "Epoch 534/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1494895744.0000 - val_loss: 2840525568.0000\n",
            "Epoch 535/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1490981248.0000 - val_loss: 2837628928.0000\n",
            "Epoch 536/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 159ms/step - loss: 1487084544.0000 - val_loss: 2834740224.0000\n",
            "Epoch 537/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 1483204864.0000 - val_loss: 2831857664.0000\n",
            "Epoch 538/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1479343488.0000 - val_loss: 2828981760.0000\n",
            "Epoch 539/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1475498880.0000 - val_loss: 2826114048.0000\n",
            "Epoch 540/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - loss: 1471670784.0000 - val_loss: 2823251456.0000\n",
            "Epoch 541/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - loss: 1467860480.0000 - val_loss: 2820396032.0000\n",
            "Epoch 542/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 1464065920.0000 - val_loss: 2817549824.0000\n",
            "Epoch 543/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - loss: 1460287104.0000 - val_loss: 2814708480.0000\n",
            "Epoch 544/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - loss: 1456525568.0000 - val_loss: 2811873280.0000\n",
            "Epoch 545/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1452778368.0000 - val_loss: 2809045504.0000\n",
            "Epoch 546/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 1449047296.0000 - val_loss: 2806223104.0000\n",
            "Epoch 547/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 1445332096.0000 - val_loss: 2803408384.0000\n",
            "Epoch 548/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 129ms/step - loss: 1441631488.0000 - val_loss: 2800599296.0000\n",
            "Epoch 549/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 1437945472.0000 - val_loss: 2797796096.0000\n",
            "Epoch 550/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1434273664.0000 - val_loss: 2794998272.0000\n",
            "Epoch 551/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 141ms/step - loss: 1430616192.0000 - val_loss: 2792208384.0000\n",
            "Epoch 552/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1426972800.0000 - val_loss: 2789423360.0000\n",
            "Epoch 553/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1423342464.0000 - val_loss: 2786643456.0000\n",
            "Epoch 554/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1419726080.0000 - val_loss: 2783870208.0000\n",
            "Epoch 555/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 114ms/step - loss: 1416122240.0000 - val_loss: 2781100032.0000\n",
            "Epoch 556/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1412532352.0000 - val_loss: 2778336256.0000\n",
            "Epoch 557/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1408954240.0000 - val_loss: 2775576576.0000\n",
            "Epoch 558/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1405389056.0000 - val_loss: 2772824576.0000\n",
            "Epoch 559/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 1401835264.0000 - val_loss: 2770074368.0000\n",
            "Epoch 560/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 1398292992.0000 - val_loss: 2767331328.0000\n",
            "Epoch 561/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 153ms/step - loss: 1394763008.0000 - val_loss: 2764591616.0000\n",
            "Epoch 562/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 1391243776.0000 - val_loss: 2761857536.0000\n",
            "Epoch 563/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1387735424.0000 - val_loss: 2759126784.0000\n",
            "Epoch 564/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1384238464.0000 - val_loss: 2756400384.0000\n",
            "Epoch 565/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - loss: 1380750464.0000 - val_loss: 2753678848.0000\n",
            "Epoch 566/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 161ms/step - loss: 1377273472.0000 - val_loss: 2750959360.0000\n",
            "Epoch 567/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 1373807232.0000 - val_loss: 2748244480.0000\n",
            "Epoch 568/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 1370348416.0000 - val_loss: 2745533184.0000\n",
            "Epoch 569/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1366900224.0000 - val_loss: 2742826240.0000\n",
            "Epoch 570/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 162ms/step - loss: 1363461376.0000 - val_loss: 2740121344.0000\n",
            "Epoch 571/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 1360031360.0000 - val_loss: 2737417984.0000\n",
            "Epoch 572/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 1356608512.0000 - val_loss: 2734720512.0000\n",
            "Epoch 573/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 160ms/step - loss: 1353195008.0000 - val_loss: 2732025088.0000\n",
            "Epoch 574/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 1349789568.0000 - val_loss: 2729334528.0000\n",
            "Epoch 575/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1346391296.0000 - val_loss: 2726644480.0000\n",
            "Epoch 576/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1343001088.0000 - val_loss: 2723956480.0000\n",
            "Epoch 577/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 131ms/step - loss: 1339618304.0000 - val_loss: 2721272064.0000\n",
            "Epoch 578/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 1336241792.0000 - val_loss: 2718590464.0000\n",
            "Epoch 579/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1332872704.0000 - val_loss: 2715910912.0000\n",
            "Epoch 580/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1329511424.0000 - val_loss: 2713233664.0000\n",
            "Epoch 581/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - loss: 1326155776.0000 - val_loss: 2710557952.0000\n",
            "Epoch 582/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 125ms/step - loss: 1322806656.0000 - val_loss: 2707886592.0000\n",
            "Epoch 583/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1319464064.0000 - val_loss: 2705216000.0000\n",
            "Epoch 584/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 1316128640.0000 - val_loss: 2702548736.0000\n",
            "Epoch 585/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - loss: 1312799488.0000 - val_loss: 2699885056.0000\n",
            "Epoch 586/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - loss: 1309475712.0000 - val_loss: 2697223424.0000\n",
            "Epoch 587/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1306159488.0000 - val_loss: 2694564608.0000\n",
            "Epoch 588/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1302849792.0000 - val_loss: 2691909632.0000\n",
            "Epoch 589/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - loss: 1299545856.0000 - val_loss: 2689258496.0000\n",
            "Epoch 590/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - loss: 1296249984.0000 - val_loss: 2686610688.0000\n",
            "Epoch 591/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 1292961920.0000 - val_loss: 2683970048.0000\n",
            "Epoch 592/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 1289680640.0000 - val_loss: 2681331456.0000\n",
            "Epoch 593/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1286406912.0000 - val_loss: 2678701056.0000\n",
            "Epoch 594/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - loss: 1283142144.0000 - val_loss: 2676074752.0000\n",
            "Epoch 595/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - loss: 1279885568.0000 - val_loss: 2673455616.0000\n",
            "Epoch 596/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - loss: 1276638592.0000 - val_loss: 2670843648.0000\n",
            "Epoch 597/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - loss: 1273400960.0000 - val_loss: 2668240896.0000\n",
            "Epoch 598/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - loss: 1270174464.0000 - val_loss: 2665646336.0000\n",
            "Epoch 599/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1266958336.0000 - val_loss: 2663061760.0000\n",
            "Epoch 600/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 1263752832.0000 - val_loss: 2660488192.0000\n",
            "Epoch 601/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - loss: 1260559872.0000 - val_loss: 2657924352.0000\n",
            "Epoch 602/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - loss: 1257380736.0000 - val_loss: 2655372800.0000\n",
            "Epoch 603/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1254214144.0000 - val_loss: 2652836864.0000\n",
            "Epoch 604/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1251061120.0000 - val_loss: 2650310656.0000\n",
            "Epoch 605/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 1247922048.0000 - val_loss: 2647801344.0000\n",
            "Epoch 606/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - loss: 1244799104.0000 - val_loss: 2645305600.0000\n",
            "Epoch 607/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - loss: 1241690368.0000 - val_loss: 2642826496.0000\n",
            "Epoch 608/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1238598784.0000 - val_loss: 2640363776.0000\n",
            "Epoch 609/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 1235521920.0000 - val_loss: 2637918976.0000\n",
            "Epoch 610/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - loss: 1232462080.0000 - val_loss: 2635490048.0000\n",
            "Epoch 611/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 96ms/step - loss: 1229418496.0000 - val_loss: 2633080064.0000\n",
            "Epoch 612/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 1226392576.0000 - val_loss: 2630688000.0000\n",
            "Epoch 613/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - loss: 1223380992.0000 - val_loss: 2628313856.0000\n",
            "Epoch 614/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 1220387456.0000 - val_loss: 2625957376.0000\n",
            "Epoch 615/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1217410560.0000 - val_loss: 2623620352.0000\n",
            "Epoch 616/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 1214450816.0000 - val_loss: 2621300736.0000\n",
            "Epoch 617/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - loss: 1211506560.0000 - val_loss: 2618999040.0000\n",
            "Epoch 618/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 1208578688.0000 - val_loss: 2616714752.0000\n",
            "Epoch 619/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 1205667456.0000 - val_loss: 2614450688.0000\n",
            "Epoch 620/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 1202771200.0000 - val_loss: 2612203776.0000\n",
            "Epoch 621/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 1199890560.0000 - val_loss: 2609974016.0000\n",
            "Epoch 622/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 1197025280.0000 - val_loss: 2607759872.0000\n",
            "Epoch 623/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 1194174464.0000 - val_loss: 2605564160.0000\n",
            "Epoch 624/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 1191337856.0000 - val_loss: 2603384064.0000\n",
            "Epoch 625/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - loss: 1188516224.0000 - val_loss: 2601219584.0000\n",
            "Epoch 626/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1185708288.0000 - val_loss: 2599074304.0000\n",
            "Epoch 627/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1182914688.0000 - val_loss: 2596940800.0000\n",
            "Epoch 628/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - loss: 1180134528.0000 - val_loss: 2594824960.0000\n",
            "Epoch 629/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - loss: 1177366912.0000 - val_loss: 2592723456.0000\n",
            "Epoch 630/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1174613120.0000 - val_loss: 2590637824.0000\n",
            "Epoch 631/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 1171872000.0000 - val_loss: 2588564992.0000\n",
            "Epoch 632/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - loss: 1169142912.0000 - val_loss: 2586509056.0000\n",
            "Epoch 633/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - loss: 1166427264.0000 - val_loss: 2584466176.0000\n",
            "Epoch 634/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1163723392.0000 - val_loss: 2582435584.0000\n",
            "Epoch 635/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1161031680.0000 - val_loss: 2580421120.0000\n",
            "Epoch 636/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 1158352000.0000 - val_loss: 2578420736.0000\n",
            "Epoch 637/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 98ms/step - loss: 1155684352.0000 - val_loss: 2576433664.0000\n",
            "Epoch 638/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step - loss: 1153028352.0000 - val_loss: 2574460928.0000\n",
            "Epoch 639/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: 1150383488.0000 - val_loss: 2572503040.0000\n",
            "Epoch 640/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 1147750784.0000 - val_loss: 2570556416.0000\n",
            "Epoch 641/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - loss: 1145129472.0000 - val_loss: 2568625152.0000\n",
            "Epoch 642/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1142519552.0000 - val_loss: 2566706688.0000\n",
            "Epoch 643/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1139920640.0000 - val_loss: 2564803328.0000\n",
            "Epoch 644/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - loss: 1137333760.0000 - val_loss: 2562912000.0000\n",
            "Epoch 645/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 1134756992.0000 - val_loss: 2561036288.0000\n",
            "Epoch 646/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 1132192128.0000 - val_loss: 2559171840.0000\n",
            "Epoch 647/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 110ms/step - loss: 1129637376.0000 - val_loss: 2557324288.0000\n",
            "Epoch 648/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - loss: 1127093632.0000 - val_loss: 2555489536.0000\n",
            "Epoch 649/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1124560512.0000 - val_loss: 2553667584.0000\n",
            "Epoch 650/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 1122038016.0000 - val_loss: 2551861248.0000\n",
            "Epoch 651/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - loss: 1119527680.0000 - val_loss: 2550066944.0000\n",
            "Epoch 652/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 135ms/step - loss: 1117026048.0000 - val_loss: 2548288256.0000\n",
            "Epoch 653/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - loss: 1114534784.0000 - val_loss: 2546523392.0000\n",
            "Epoch 654/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1112055552.0000 - val_loss: 2544774400.0000\n",
            "Epoch 655/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 146ms/step - loss: 1109585536.0000 - val_loss: 2543038208.0000\n",
            "Epoch 656/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 1107126528.0000 - val_loss: 2541316864.0000\n",
            "Epoch 657/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1104677376.0000 - val_loss: 2539607040.0000\n",
            "Epoch 658/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 1102237824.0000 - val_loss: 2537915136.0000\n",
            "Epoch 659/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - loss: 1099808256.0000 - val_loss: 2536235776.0000\n",
            "Epoch 660/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - loss: 1097388800.0000 - val_loss: 2534570240.0000\n",
            "Epoch 661/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1094980224.0000 - val_loss: 2532919552.0000\n",
            "Epoch 662/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - loss: 1092580096.0000 - val_loss: 2531284736.0000\n",
            "Epoch 663/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - loss: 1090190208.0000 - val_loss: 2529664768.0000\n",
            "Epoch 664/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 1087809920.0000 - val_loss: 2528057856.0000\n",
            "Epoch 665/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1085439104.0000 - val_loss: 2526464512.0000\n",
            "Epoch 666/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 149ms/step - loss: 1083078016.0000 - val_loss: 2524886016.0000\n",
            "Epoch 667/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 1080726272.0000 - val_loss: 2523322368.0000\n",
            "Epoch 668/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1078383616.0000 - val_loss: 2521773056.0000\n",
            "Epoch 669/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 1076049792.0000 - val_loss: 2520237568.0000\n",
            "Epoch 670/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - loss: 1073726656.0000 - val_loss: 2518716672.0000\n",
            "Epoch 671/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - loss: 1071411648.0000 - val_loss: 2517211392.0000\n",
            "Epoch 672/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 1069105472.0000 - val_loss: 2515718912.0000\n",
            "Epoch 673/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1066808384.0000 - val_loss: 2514240768.0000\n",
            "Epoch 674/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1064521216.0000 - val_loss: 2512777472.0000\n",
            "Epoch 675/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - loss: 1062240832.0000 - val_loss: 2511328256.0000\n",
            "Epoch 676/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 1059971136.0000 - val_loss: 2509892352.0000\n",
            "Epoch 677/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1057709248.0000 - val_loss: 2508471296.0000\n",
            "Epoch 678/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1055456192.0000 - val_loss: 2507063040.0000\n",
            "Epoch 679/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - loss: 1053211584.0000 - val_loss: 2505669120.0000\n",
            "Epoch 680/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 99ms/step - loss: 1050974848.0000 - val_loss: 2504290560.0000\n",
            "Epoch 681/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 1048746880.0000 - val_loss: 2502924544.0000\n",
            "Epoch 682/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1046527872.0000 - val_loss: 2501571840.0000\n",
            "Epoch 683/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - loss: 1044316416.0000 - val_loss: 2500231680.0000\n",
            "Epoch 684/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - loss: 1042112832.0000 - val_loss: 2498905088.0000\n",
            "Epoch 685/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 1039918080.0000 - val_loss: 2497593856.0000\n",
            "Epoch 686/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 1037731840.0000 - val_loss: 2496296448.0000\n",
            "Epoch 687/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 143ms/step - loss: 1035553152.0000 - val_loss: 2495010560.0000\n",
            "Epoch 688/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - loss: 1033382272.0000 - val_loss: 2493739008.0000\n",
            "Epoch 689/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1031219392.0000 - val_loss: 2492481024.0000\n",
            "Epoch 690/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 101ms/step - loss: 1029064320.0000 - val_loss: 2491233536.0000\n",
            "Epoch 691/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - loss: 1026917376.0000 - val_loss: 2490001920.0000\n",
            "Epoch 692/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 1024778304.0000 - val_loss: 2488781568.0000\n",
            "Epoch 693/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 1022646464.0000 - val_loss: 2487573760.0000\n",
            "Epoch 694/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1020522496.0000 - val_loss: 2486379264.0000\n",
            "Epoch 695/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1018405696.0000 - val_loss: 2485195008.0000\n",
            "Epoch 696/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 1016297536.0000 - val_loss: 2484026624.0000\n",
            "Epoch 697/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1014195712.0000 - val_loss: 2482869248.0000\n",
            "Epoch 698/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 1012102528.0000 - val_loss: 2481724160.0000\n",
            "Epoch 699/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 160ms/step - loss: 1010017088.0000 - val_loss: 2480591616.0000\n",
            "Epoch 700/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - loss: 1007937728.0000 - val_loss: 2479472128.0000\n",
            "Epoch 701/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 1005866688.0000 - val_loss: 2478362880.0000\n",
            "Epoch 702/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 1003802560.0000 - val_loss: 2477265152.0000\n",
            "Epoch 703/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 143ms/step - loss: 1001745216.0000 - val_loss: 2476181504.0000\n",
            "Epoch 704/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 123ms/step - loss: 999696896.0000 - val_loss: 2475108352.0000\n",
            "Epoch 705/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 997653952.0000 - val_loss: 2474045952.0000\n",
            "Epoch 706/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 96ms/step - loss: 995618944.0000 - val_loss: 2472996352.0000\n",
            "Epoch 707/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - loss: 993591424.0000 - val_loss: 2471957504.0000\n",
            "Epoch 708/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 97ms/step - loss: 991570816.0000 - val_loss: 2470930176.0000\n",
            "Epoch 709/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 989557120.0000 - val_loss: 2469914368.0000\n",
            "Epoch 710/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 98ms/step - loss: 987550400.0000 - val_loss: 2468908544.0000\n",
            "Epoch 711/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 122ms/step - loss: 985551808.0000 - val_loss: 2467916032.0000\n",
            "Epoch 712/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 150ms/step - loss: 983559616.0000 - val_loss: 2466932480.0000\n",
            "Epoch 713/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 981575232.0000 - val_loss: 2465960960.0000\n",
            "Epoch 714/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 97ms/step - loss: 979597248.0000 - val_loss: 2464998656.0000\n",
            "Epoch 715/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 977626752.0000 - val_loss: 2464049408.0000\n",
            "Epoch 716/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 975663040.0000 - val_loss: 2463109376.0000\n",
            "Epoch 717/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 973706240.0000 - val_loss: 2462179072.0000\n",
            "Epoch 718/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - loss: 971756480.0000 - val_loss: 2461260032.0000\n",
            "Epoch 719/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 969813952.0000 - val_loss: 2460351744.0000\n",
            "Epoch 720/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 99ms/step - loss: 967878400.0000 - val_loss: 2459452928.0000\n",
            "Epoch 721/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 150ms/step - loss: 965950016.0000 - val_loss: 2458564608.0000\n",
            "Epoch 722/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 99ms/step - loss: 964027968.0000 - val_loss: 2457687040.0000\n",
            "Epoch 723/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 962113216.0000 - val_loss: 2456817920.0000\n",
            "Epoch 724/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 960205568.0000 - val_loss: 2455959040.0000\n",
            "Epoch 725/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 120ms/step - loss: 958304768.0000 - val_loss: 2455111936.0000\n",
            "Epoch 726/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - loss: 956411008.0000 - val_loss: 2454272000.0000\n",
            "Epoch 727/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - loss: 954524224.0000 - val_loss: 2453442048.0000\n",
            "Epoch 728/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 100ms/step - loss: 952644480.0000 - val_loss: 2452622592.0000\n",
            "Epoch 729/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - loss: 950771008.0000 - val_loss: 2451812608.0000\n",
            "Epoch 730/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - loss: 948904832.0000 - val_loss: 2451011584.0000\n",
            "Epoch 731/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 947045376.0000 - val_loss: 2450220288.0000\n",
            "Epoch 732/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 945193280.0000 - val_loss: 2449437696.0000\n",
            "Epoch 733/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - loss: 943347456.0000 - val_loss: 2448662272.0000\n",
            "Epoch 734/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 941508544.0000 - val_loss: 2447899648.0000\n",
            "Epoch 735/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 939677376.0000 - val_loss: 2447143936.0000\n",
            "Epoch 736/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 127ms/step - loss: 937852800.0000 - val_loss: 2446397696.0000\n",
            "Epoch 737/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 936034688.0000 - val_loss: 2445658880.0000\n",
            "Epoch 738/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 934223872.0000 - val_loss: 2444930816.0000\n",
            "Epoch 739/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 113ms/step - loss: 932418816.0000 - val_loss: 2444210432.0000\n",
            "Epoch 740/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - loss: 930621696.0000 - val_loss: 2443498240.0000\n",
            "Epoch 741/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 102ms/step - loss: 928831104.0000 - val_loss: 2442794752.0000\n",
            "Epoch 742/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 927047616.0000 - val_loss: 2442100224.0000\n",
            "Epoch 743/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - loss: 925271168.0000 - val_loss: 2441412864.0000\n",
            "Epoch 744/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - loss: 923501120.0000 - val_loss: 2440733440.0000\n",
            "Epoch 745/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 921737792.0000 - val_loss: 2440064000.0000\n",
            "Epoch 746/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 919981376.0000 - val_loss: 2439399936.0000\n",
            "Epoch 747/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 918232320.0000 - val_loss: 2438745344.0000\n",
            "Epoch 748/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 916489408.0000 - val_loss: 2438097920.0000\n",
            "Epoch 749/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 914753664.0000 - val_loss: 2437460480.0000\n",
            "Epoch 750/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 118ms/step - loss: 913024128.0000 - val_loss: 2436828416.0000\n",
            "Epoch 751/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 911302464.0000 - val_loss: 2436204032.0000\n",
            "Epoch 752/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 909587072.0000 - val_loss: 2435589632.0000\n",
            "Epoch 753/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 907878272.0000 - val_loss: 2434981888.0000\n",
            "Epoch 754/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - loss: 906176512.0000 - val_loss: 2434381568.0000\n",
            "Epoch 755/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 904481408.0000 - val_loss: 2433788416.0000\n",
            "Epoch 756/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 902793344.0000 - val_loss: 2433201920.0000\n",
            "Epoch 757/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 901111872.0000 - val_loss: 2432622592.0000\n",
            "Epoch 758/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - loss: 899437312.0000 - val_loss: 2432051456.0000\n",
            "Epoch 759/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 897768896.0000 - val_loss: 2431487744.0000\n",
            "Epoch 760/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 896107520.0000 - val_loss: 2430929664.0000\n",
            "Epoch 761/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 894452544.0000 - val_loss: 2430379264.0000\n",
            "Epoch 762/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - loss: 892804800.0000 - val_loss: 2429834496.0000\n",
            "Epoch 763/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - loss: 891163520.0000 - val_loss: 2429297664.0000\n",
            "Epoch 764/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 889529536.0000 - val_loss: 2428765440.0000\n",
            "Epoch 765/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 887900864.0000 - val_loss: 2428242432.0000\n",
            "Epoch 766/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 124ms/step - loss: 886279616.0000 - val_loss: 2427724800.0000\n",
            "Epoch 767/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - loss: 884665152.0000 - val_loss: 2427215104.0000\n",
            "Epoch 768/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 883056768.0000 - val_loss: 2426710016.0000\n",
            "Epoch 769/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 881455872.0000 - val_loss: 2426212352.0000\n",
            "Epoch 770/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 879860992.0000 - val_loss: 2425720576.0000\n",
            "Epoch 771/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 878272256.0000 - val_loss: 2425235968.0000\n",
            "Epoch 772/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - loss: 876691264.0000 - val_loss: 2424756736.0000\n",
            "Epoch 773/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 875116160.0000 - val_loss: 2424283392.0000\n",
            "Epoch 774/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 873547584.0000 - val_loss: 2423815936.0000\n",
            "Epoch 775/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 871985856.0000 - val_loss: 2423354880.0000\n",
            "Epoch 776/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - loss: 870430272.0000 - val_loss: 2422899968.0000\n",
            "Epoch 777/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - loss: 868880576.0000 - val_loss: 2422450432.0000\n",
            "Epoch 778/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 867338368.0000 - val_loss: 2422008064.0000\n",
            "Epoch 779/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 132ms/step - loss: 865802304.0000 - val_loss: 2421568768.0000\n",
            "Epoch 780/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 864272704.0000 - val_loss: 2421136640.0000\n",
            "Epoch 781/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 862749952.0000 - val_loss: 2420710144.0000\n",
            "Epoch 782/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 861233216.0000 - val_loss: 2420287744.0000\n",
            "Epoch 783/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - loss: 859723328.0000 - val_loss: 2419870720.0000\n",
            "Epoch 784/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 858219072.0000 - val_loss: 2419460864.0000\n",
            "Epoch 785/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 856721088.0000 - val_loss: 2419055104.0000\n",
            "Epoch 786/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 152ms/step - loss: 855229824.0000 - val_loss: 2418654720.0000\n",
            "Epoch 787/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 853745152.0000 - val_loss: 2418259200.0000\n",
            "Epoch 788/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 852266752.0000 - val_loss: 2417869056.0000\n",
            "Epoch 789/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 142ms/step - loss: 850794496.0000 - val_loss: 2417485568.0000\n",
            "Epoch 790/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 118ms/step - loss: 849328960.0000 - val_loss: 2417106432.0000\n",
            "Epoch 791/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 847869376.0000 - val_loss: 2416730880.0000\n",
            "Epoch 792/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 846416640.0000 - val_loss: 2416361984.0000\n",
            "Epoch 793/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 148ms/step - loss: 844969152.0000 - val_loss: 2415996672.0000\n",
            "Epoch 794/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 843527680.0000 - val_loss: 2415637504.0000\n",
            "Epoch 795/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 108ms/step - loss: 842093376.0000 - val_loss: 2415280384.0000\n",
            "Epoch 796/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 154ms/step - loss: 840664896.0000 - val_loss: 2414928640.0000\n",
            "Epoch 797/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 111ms/step - loss: 839242240.0000 - val_loss: 2414582016.0000\n",
            "Epoch 798/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 837826752.0000 - val_loss: 2414241536.0000\n",
            "Epoch 799/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 836416128.0000 - val_loss: 2413902592.0000\n",
            "Epoch 800/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 162ms/step - loss: 835012736.0000 - val_loss: 2413569792.0000\n",
            "Epoch 801/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 833615360.0000 - val_loss: 2413242624.0000\n",
            "Epoch 802/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 832223360.0000 - val_loss: 2412918016.0000\n",
            "Epoch 803/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 830838272.0000 - val_loss: 2412598016.0000\n",
            "Epoch 804/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - loss: 829458368.0000 - val_loss: 2412282112.0000\n",
            "Epoch 805/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 828084800.0000 - val_loss: 2411972096.0000\n",
            "Epoch 806/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 826716672.0000 - val_loss: 2411662848.0000\n",
            "Epoch 807/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 825355968.0000 - val_loss: 2411360768.0000\n",
            "Epoch 808/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - loss: 824000512.0000 - val_loss: 2411063296.0000\n",
            "Epoch 809/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 145ms/step - loss: 822650816.0000 - val_loss: 2410767872.0000\n",
            "Epoch 810/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 821307136.0000 - val_loss: 2410476032.0000\n",
            "Epoch 811/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 819969664.0000 - val_loss: 2410190336.0000\n",
            "Epoch 812/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - loss: 818637504.0000 - val_loss: 2409906688.0000\n",
            "Epoch 813/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 817312064.0000 - val_loss: 2409627904.0000\n",
            "Epoch 814/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 815991872.0000 - val_loss: 2409351168.0000\n",
            "Epoch 815/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 127ms/step - loss: 814677696.0000 - val_loss: 2409080832.0000\n",
            "Epoch 816/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 813369856.0000 - val_loss: 2408812800.0000\n",
            "Epoch 817/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 812066944.0000 - val_loss: 2408548352.0000\n",
            "Epoch 818/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 810770368.0000 - val_loss: 2408286720.0000\n",
            "Epoch 819/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - loss: 809479040.0000 - val_loss: 2408029440.0000\n",
            "Epoch 820/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - loss: 808194624.0000 - val_loss: 2407775232.0000\n",
            "Epoch 821/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 806914688.0000 - val_loss: 2407526144.0000\n",
            "Epoch 822/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 805641600.0000 - val_loss: 2407278848.0000\n",
            "Epoch 823/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 149ms/step - loss: 804373952.0000 - val_loss: 2407036928.0000\n",
            "Epoch 824/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 803111808.0000 - val_loss: 2406797056.0000\n",
            "Epoch 825/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 801855360.0000 - val_loss: 2406560768.0000\n",
            "Epoch 826/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 158ms/step - loss: 800604544.0000 - val_loss: 2406327808.0000\n",
            "Epoch 827/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 110ms/step - loss: 799359360.0000 - val_loss: 2406099200.0000\n",
            "Epoch 828/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - loss: 798119744.0000 - val_loss: 2405872128.0000\n",
            "Epoch 829/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - loss: 796885760.0000 - val_loss: 2405650944.0000\n",
            "Epoch 830/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 795657792.0000 - val_loss: 2405431296.0000\n",
            "Epoch 831/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 794434816.0000 - val_loss: 2405215488.0000\n",
            "Epoch 832/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 133ms/step - loss: 793218176.0000 - val_loss: 2405001216.0000\n",
            "Epoch 833/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - loss: 792006208.0000 - val_loss: 2404792064.0000\n",
            "Epoch 834/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 790800256.0000 - val_loss: 2404586496.0000\n",
            "Epoch 835/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 789599552.0000 - val_loss: 2404382976.0000\n",
            "Epoch 836/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 788404096.0000 - val_loss: 2404183040.0000\n",
            "Epoch 837/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - loss: 787214528.0000 - val_loss: 2403984640.0000\n",
            "Epoch 838/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 119ms/step - loss: 786030080.0000 - val_loss: 2403792896.0000\n",
            "Epoch 839/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - loss: 784851584.0000 - val_loss: 2403600384.0000\n",
            "Epoch 840/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 161ms/step - loss: 783678656.0000 - val_loss: 2403413504.0000\n",
            "Epoch 841/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 115ms/step - loss: 782510976.0000 - val_loss: 2403228928.0000\n",
            "Epoch 842/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 110ms/step - loss: 781347840.0000 - val_loss: 2403046400.0000\n",
            "Epoch 843/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 780191168.0000 - val_loss: 2402867200.0000\n",
            "Epoch 844/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 165ms/step - loss: 779038912.0000 - val_loss: 2402692352.0000\n",
            "Epoch 845/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 777892736.0000 - val_loss: 2402519808.0000\n",
            "Epoch 846/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 109ms/step - loss: 776751424.0000 - val_loss: 2402350848.0000\n",
            "Epoch 847/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 167ms/step - loss: 775616128.0000 - val_loss: 2402184704.0000\n",
            "Epoch 848/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 774485632.0000 - val_loss: 2402019584.0000\n",
            "Epoch 849/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 773360256.0000 - val_loss: 2401858816.0000\n",
            "Epoch 850/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 148ms/step - loss: 772240640.0000 - val_loss: 2401700864.0000\n",
            "Epoch 851/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 771125632.0000 - val_loss: 2401545984.0000\n",
            "Epoch 852/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 770016064.0000 - val_loss: 2401392128.0000\n",
            "Epoch 853/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 108ms/step - loss: 768912064.0000 - val_loss: 2401244416.0000\n",
            "Epoch 854/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - loss: 767812800.0000 - val_loss: 2401097216.0000\n",
            "Epoch 855/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 131ms/step - loss: 766719040.0000 - val_loss: 2400953344.0000\n",
            "Epoch 856/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - loss: 765630208.0000 - val_loss: 2400813056.0000\n",
            "Epoch 857/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 137ms/step - loss: 764547136.0000 - val_loss: 2400674560.0000\n",
            "Epoch 858/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - loss: 763468352.0000 - val_loss: 2400538112.0000\n",
            "Epoch 859/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - loss: 762395136.0000 - val_loss: 2400406272.0000\n",
            "Epoch 860/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - loss: 761326656.0000 - val_loss: 2400276736.0000\n",
            "Epoch 861/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 760264000.0000 - val_loss: 2400149760.0000\n",
            "Epoch 862/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 759206336.0000 - val_loss: 2400026112.0000\n",
            "Epoch 863/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 758152256.0000 - val_loss: 2399904768.0000\n",
            "Epoch 864/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - loss: 757105152.0000 - val_loss: 2399787776.0000\n",
            "Epoch 865/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - loss: 756062528.0000 - val_loss: 2399672064.0000\n",
            "Epoch 866/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 755024448.0000 - val_loss: 2399558912.0000\n",
            "Epoch 867/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 753991616.0000 - val_loss: 2399447808.0000\n",
            "Epoch 868/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - loss: 752963968.0000 - val_loss: 2399341568.0000\n",
            "Epoch 869/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - loss: 751940864.0000 - val_loss: 2399236352.0000\n",
            "Epoch 870/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 750923008.0000 - val_loss: 2399133696.0000\n",
            "Epoch 871/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 749910272.0000 - val_loss: 2399034368.0000\n",
            "Epoch 872/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - loss: 748901376.0000 - val_loss: 2398938880.0000\n",
            "Epoch 873/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 121ms/step - loss: 747898688.0000 - val_loss: 2398845696.0000\n",
            "Epoch 874/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 746900224.0000 - val_loss: 2398753792.0000\n",
            "Epoch 875/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 745906752.0000 - val_loss: 2398665984.0000\n",
            "Epoch 876/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 744918592.0000 - val_loss: 2398580224.0000\n",
            "Epoch 877/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 743934528.0000 - val_loss: 2398496768.0000\n",
            "Epoch 878/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 742956224.0000 - val_loss: 2398417408.0000\n",
            "Epoch 879/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 101ms/step - loss: 741981632.0000 - val_loss: 2398339584.0000\n",
            "Epoch 880/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 160ms/step - loss: 741012544.0000 - val_loss: 2398263808.0000\n",
            "Epoch 881/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 104ms/step - loss: 740048064.0000 - val_loss: 2398193408.0000\n",
            "Epoch 882/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 739087872.0000 - val_loss: 2398122496.0000\n",
            "Epoch 883/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - loss: 738133824.0000 - val_loss: 2398054656.0000\n",
            "Epoch 884/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 103ms/step - loss: 737183488.0000 - val_loss: 2397993216.0000\n",
            "Epoch 885/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 100ms/step - loss: 736237760.0000 - val_loss: 2397930496.0000\n",
            "Epoch 886/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 735296896.0000 - val_loss: 2397872896.0000\n",
            "Epoch 887/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - loss: 734360768.0000 - val_loss: 2397817600.0000\n",
            "Epoch 888/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 143ms/step - loss: 733429440.0000 - val_loss: 2397764864.0000\n",
            "Epoch 889/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - loss: 732502272.0000 - val_loss: 2397714432.0000\n",
            "Epoch 890/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 731580096.0000 - val_loss: 2397667584.0000\n",
            "Epoch 891/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - loss: 730663488.0000 - val_loss: 2397621760.0000\n",
            "Epoch 892/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 729749760.0000 - val_loss: 2397579520.0000\n",
            "Epoch 893/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 728841216.0000 - val_loss: 2397541120.0000\n",
            "Epoch 894/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 727937472.0000 - val_loss: 2397504000.0000\n",
            "Epoch 895/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - loss: 727037888.0000 - val_loss: 2397469696.0000\n",
            "Epoch 896/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 123ms/step - loss: 726143616.0000 - val_loss: 2397437440.0000\n",
            "Epoch 897/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 725253504.0000 - val_loss: 2397409024.0000\n",
            "Epoch 898/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - loss: 724368000.0000 - val_loss: 2397383936.0000\n",
            "Epoch 899/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 122ms/step - loss: 723487040.0000 - val_loss: 2397361408.0000\n",
            "Epoch 900/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 722610176.0000 - val_loss: 2397340160.0000\n",
            "Epoch 901/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 721737664.0000 - val_loss: 2397321728.0000\n",
            "Epoch 902/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - loss: 720869696.0000 - val_loss: 2397307392.0000\n",
            "Epoch 903/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 720006272.0000 - val_loss: 2397295872.0000\n",
            "Epoch 904/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 719147648.0000 - val_loss: 2397285888.0000\n",
            "Epoch 905/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 718292800.0000 - val_loss: 2397279232.0000\n",
            "Epoch 906/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 102ms/step - loss: 717442368.0000 - val_loss: 2397274368.0000\n",
            "Epoch 907/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 716596160.0000 - val_loss: 2397272832.0000\n",
            "Epoch 908/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - loss: 715754752.0000 - val_loss: 2397275136.0000\n",
            "Epoch 909/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 714917888.0000 - val_loss: 2397280768.0000\n",
            "Epoch 910/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 714084416.0000 - val_loss: 2397287424.0000\n",
            "Epoch 911/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 129ms/step - loss: 713256192.0000 - val_loss: 2397297152.0000\n",
            "Epoch 912/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 148ms/step - loss: 712430976.0000 - val_loss: 2397309952.0000\n",
            "Epoch 913/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 711610624.0000 - val_loss: 2397325824.0000\n",
            "Epoch 914/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 710795264.0000 - val_loss: 2397344768.0000\n",
            "Epoch 915/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 709983296.0000 - val_loss: 2397366016.0000\n",
            "Epoch 916/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - loss: 709175744.0000 - val_loss: 2397390080.0000\n",
            "Epoch 917/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 117ms/step - loss: 708372480.0000 - val_loss: 2397417984.0000\n",
            "Epoch 918/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 707573376.0000 - val_loss: 2397446912.0000\n",
            "Epoch 919/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 103ms/step - loss: 706778496.0000 - val_loss: 2397479680.0000\n",
            "Epoch 920/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 150ms/step - loss: 705987840.0000 - val_loss: 2397515520.0000\n",
            "Epoch 921/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 124ms/step - loss: 705200896.0000 - val_loss: 2397553920.0000\n",
            "Epoch 922/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 704418432.0000 - val_loss: 2397596416.0000\n",
            "Epoch 923/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 703639232.0000 - val_loss: 2397641216.0000\n",
            "Epoch 924/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 113ms/step - loss: 702864896.0000 - val_loss: 2397688320.0000\n",
            "Epoch 925/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 702095360.0000 - val_loss: 2397738752.0000\n",
            "Epoch 926/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 102ms/step - loss: 701327552.0000 - val_loss: 2397792512.0000\n",
            "Epoch 927/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 700565248.0000 - val_loss: 2397849088.0000\n",
            "Epoch 928/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 699806720.0000 - val_loss: 2397908736.0000\n",
            "Epoch 929/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 699052224.0000 - val_loss: 2397971456.0000\n",
            "Epoch 930/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 153ms/step - loss: 698302208.0000 - val_loss: 2398037248.0000\n",
            "Epoch 931/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 697554880.0000 - val_loss: 2398104064.0000\n",
            "Epoch 932/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 104ms/step - loss: 696811968.0000 - val_loss: 2398176512.0000\n",
            "Epoch 933/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 166ms/step - loss: 696073984.0000 - val_loss: 2398251264.0000\n",
            "Epoch 934/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 105ms/step - loss: 695338816.0000 - val_loss: 2398329344.0000\n",
            "Epoch 935/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 694608640.0000 - val_loss: 2398411008.0000\n",
            "Epoch 936/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 162ms/step - loss: 693881600.0000 - val_loss: 2398495488.0000\n",
            "Epoch 937/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - loss: 693158016.0000 - val_loss: 2398583296.0000\n",
            "Epoch 938/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 692438848.0000 - val_loss: 2398674176.0000\n",
            "Epoch 939/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - loss: 691723264.0000 - val_loss: 2398768896.0000\n",
            "Epoch 940/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - loss: 691011520.0000 - val_loss: 2398866176.0000\n",
            "Epoch 941/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 103ms/step - loss: 690302912.0000 - val_loss: 2398968064.0000\n",
            "Epoch 942/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - loss: 689599104.0000 - val_loss: 2399071232.0000\n",
            "Epoch 943/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - loss: 688898624.0000 - val_loss: 2399178752.0000\n",
            "Epoch 944/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 105ms/step - loss: 688201216.0000 - val_loss: 2399290368.0000\n",
            "Epoch 945/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 687509184.0000 - val_loss: 2399404288.0000\n",
            "Epoch 946/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - loss: 686819520.0000 - val_loss: 2399522304.0000\n",
            "Epoch 947/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - loss: 686134080.0000 - val_loss: 2399641600.0000\n",
            "Epoch 948/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 685452416.0000 - val_loss: 2399766784.0000\n",
            "Epoch 949/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 684774208.0000 - val_loss: 2399895040.0000\n",
            "Epoch 950/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 152ms/step - loss: 684099008.0000 - val_loss: 2400025856.0000\n",
            "Epoch 951/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step - loss: 683428032.0000 - val_loss: 2400159744.0000\n",
            "Epoch 952/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 105ms/step - loss: 682761024.0000 - val_loss: 2400296960.0000\n",
            "Epoch 953/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 125ms/step - loss: 682097024.0000 - val_loss: 2400439808.0000\n",
            "Epoch 954/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 106ms/step - loss: 681436416.0000 - val_loss: 2400584960.0000\n",
            "Epoch 955/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 106ms/step - loss: 680779840.0000 - val_loss: 2400733184.0000\n",
            "Epoch 956/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 107ms/step - loss: 680126592.0000 - val_loss: 2400885248.0000\n",
            "Epoch 957/20000\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 149ms/step - loss: 679477248.0000 - val_loss: 2401041664.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79ee4e17f550>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "trainr_dist = getdist(mydat_rep_list, mydat_trainr_list)\n",
        "val_dist = getdist(mydat_rep_list, mydat_val_list)\n",
        "m.fit(x=trainr_dist, y=mydat_trainr_y, epochs = 20000, validation_data = (val_dist, mydat_val_y), callbacks = [callback], verbose = 1, batch_size = 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SngA3vulY9Wo",
        "outputId": "42daf71c-c247-4143-eb7a-7a9cbb17c238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 50ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 44ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 48ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 85ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 81ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 103ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
            "2401.042066000889\n",
            "2203.3560128355307\n",
            "24.64134020818681\n",
            "0.4640266859541295\n",
            "0.2435930788436163\n",
            "-0.02593947007518611\n"
          ]
        }
      ],
      "source": [
        "neval = 2048\n",
        "nfull = len(mydat_y)\n",
        "niters = nfull//neval +1\n",
        "predslist = []\n",
        "for i in range(niters):\n",
        "  start = int(neval*i)\n",
        "  stop = int(np.min([neval*(i+1), nfull]))\n",
        "  currentlist = [mydat_cat[start:stop], mydat_num[start:stop]]\n",
        "  currentdist = getdist(mydat_rep_list, currentlist)\n",
        "  predslist.append(m.predict(x=currentdist))\n",
        "\n",
        "mypreds_full = np.concatenate(predslist)\n",
        "mypreds_val = m.predict(x=val_dist)\n",
        "mypreds_trainr = m.predict(x=trainr_dist)\n",
        "\n",
        "print(np.mean((np.array(mydat_val_y)-np.reshape(mypreds_val, newshape = (len(mypreds_val))))**2)/1e+6)\n",
        "print(np.mean((np.array(mydat_y)-np.reshape(mypreds_full, newshape = (len(mypreds_full))))**2)/1e+6)\n",
        "print(np.mean(np.abs(np.array(mydat_y)-np.reshape(mypreds_full, newshape = (len(mypreds_full)))))/1e+3)\n",
        "print(np.sqrt(np.mean((np.array(mydat_y)-np.reshape(mypreds_full, newshape = (len(mypreds_full))))**2))/np.abs(mydat_y).mean())\n",
        "print(np.mean(np.abs(np.array(mydat_y)-np.reshape(mypreds_full, newshape = (len(mypreds_full)))))/np.abs(mydat_y).mean())\n",
        "print(np.mean(np.array(mydat_y)-np.reshape(mypreds_full, newshape = (len(mypreds_full))))/mydat_y.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXbQwBeMz89I"
      },
      "source": [
        "# NTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-vzD482_8Dg"
      },
      "source": [
        "## Plain NTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihTWRLhOLJcR"
      },
      "source": [
        "This code is used for both plain NTK and regularized NTK. To obtain the regularized NTK, set diag_reg = 1e-4. Additionally, you will need to manually change the activation functions (e.g., stax.relu, stax.erf, or stax.gelu) and adjust the architecture from a 1-layer to a 2-layer network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7Y8LL0-GUfj"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.example_libraries import optimizers\n",
        "from jax import jit, grad, vmap\n",
        "import jax.numpy as jnp\n",
        "import functools\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import predict, stax\n",
        "import numpy as np\n",
        "from random import seed, sample\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from scipy import stats\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def unflatten(x):\n",
        "  return np.reshape(x, (len(x), 1))\n",
        "\n",
        "mydat_train_y_ntk = jnp.array(unflatten(mydat_train_y))\n",
        "mydat_train_ntk = jnp.array(mydat_train)\n",
        "mydat_val_y_ntk = jnp.array(unflatten(mydat_val_y))\n",
        "mydat_val_ntk = jnp.array(mydat_val)\n",
        "mydat_y_ntk = jnp.array(unflatten(mydat_y))\n",
        "mydat_array_ntk = jnp.array(mydat_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m5cwOhFJGbt",
        "outputId": "df603e7e-15b8-4b3a-ba1e-cb33c676c5be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1\n",
            "0.2\n",
            "0.3\n",
            "0.4\n",
            "0.5\n",
            "0.6\n",
            "0.7\n",
            "0.8\n",
            "0.9\n",
            "1.0\n",
            "1.1\n",
            "1.2\n",
            "1.3\n",
            "1.4\n",
            "1.5\n",
            "1.6\n",
            "1.7\n",
            "1.8\n",
            "1.9\n",
            "2.0\n",
            "2.1\n",
            "2.2\n",
            "2.3\n",
            "2.4\n",
            "2.5\n",
            "2.6\n",
            "2.7\n",
            "2.8\n",
            "2.9\n",
            "3.0\n",
            "3.1\n",
            "3.2\n",
            "3.3\n",
            "3.4\n",
            "3.5\n",
            "3.6\n",
            "3.7\n",
            "3.8\n",
            "3.9\n",
            "4.0\n",
            "4.1\n",
            "4.2\n",
            "4.3\n",
            "4.4\n",
            "4.5\n",
            "4.6\n",
            "4.7\n",
            "4.8\n",
            "4.9\n",
            "5.0\n",
            "5.1\n",
            "5.2\n",
            "5.3\n",
            "5.4\n",
            "5.5\n",
            "5.6\n",
            "5.7\n",
            "5.8\n",
            "5.9\n",
            "6.0\n",
            "6.1\n",
            "6.2\n",
            "6.3\n",
            "6.4\n",
            "6.5\n",
            "6.6\n",
            "6.7\n",
            "6.8\n",
            "6.9\n",
            "7.0\n",
            "7.1\n",
            "7.2\n",
            "7.3\n",
            "7.4\n",
            "7.5\n",
            "7.6\n",
            "7.7\n",
            "7.8\n",
            "7.9\n",
            "8.0\n",
            "8.1\n",
            "8.2\n",
            "8.3\n",
            "8.4\n",
            "8.5\n",
            "8.6\n",
            "8.7\n",
            "8.8\n",
            "8.9\n",
            "9.0\n",
            "9.1\n",
            "9.2\n",
            "9.3\n",
            "9.4\n",
            "9.5\n",
            "9.6\n",
            "9.7\n",
            "9.8\n",
            "9.9\n",
            "10.0\n",
            "10.1\n",
            "10.2\n",
            "10.3\n",
            "10.4\n",
            "10.5\n",
            "10.6\n",
            "10.7\n",
            "10.8\n",
            "10.9\n",
            "11.0\n",
            "11.1\n",
            "11.2\n",
            "11.3\n",
            "11.4\n",
            "11.5\n",
            "11.6\n",
            "11.7\n",
            "11.8\n",
            "11.9\n",
            "12.0\n",
            "12.1\n",
            "12.2\n",
            "12.3\n",
            "12.4\n",
            "12.5\n",
            "12.6\n",
            "12.7\n",
            "12.8\n",
            "12.9\n",
            "13.0\n",
            "13.1\n",
            "13.2\n",
            "13.3\n",
            "13.4\n",
            "13.5\n",
            "13.6\n",
            "13.7\n",
            "13.8\n",
            "13.9\n",
            "14.0\n",
            "14.1\n",
            "14.2\n",
            "14.3\n",
            "14.4\n",
            "14.5\n",
            "14.6\n",
            "14.7\n",
            "14.8\n",
            "14.9\n",
            "15.0\n"
          ]
        }
      ],
      "source": [
        "K = 150\n",
        "mae_val = np.zeros(K)\n",
        "mse_val = np.zeros(K)\n",
        "mae_test = np.zeros(K)\n",
        "mse_test = np.zeros(K)\n",
        "mse_train = np.zeros(K)\n",
        "perr = np.zeros(K)\n",
        "b_std = 0.1\n",
        "diag_reg = 0\n",
        "myactivation = stax.Relu #stax.Erf, stax.Gelu\n",
        "\n",
        "for i in range(K):\n",
        "  random.seed(10)\n",
        "\n",
        "  W_std = (i+1)/10\n",
        "  print(W_std)\n",
        "\n",
        "  init_fn, apply_fn, kernel_fn = stax.serial(\n",
        "    stax.Dense(128, W_std=W_std, b_std=b_std), myactivation(),\n",
        "    #stax.Dense(128, W_std=W_std, b_std=b_std), myactivation(),\n",
        "    stax.Dense(1, W_std=W_std, b_std=b_std)\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  kernel_fn_jit = jit(kernel_fn, static_argnames='get')\n",
        "  get = 'ntk'\n",
        "\n",
        "  #Ktrain_train = np.array(kernel_fn_jit(mydat_train, mydat_train, get))\n",
        "  #Ktrain_train_reg = Ktrain_train + 0*np.identity(len(mydat_train_y))\n",
        "  #Ktrain_full = np.array(kernel_fn_jit(mydat_array, mydat_train, get))\n",
        "  #Ktrain_val = np.array(kernel_fn_jit(mydat_val, mydat_train, get))\n",
        "  #alpha = np.linalg.solve(Ktrain_train_reg, mydat_train_y)\n",
        "  #mypreds_train_ntk = np.matmul(Ktrain_train, alpha)\n",
        "  #mypreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "  #mypreds_full_ntk = np.matmul(Ktrain_full, alpha)\n",
        "  predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn_jit, mydat_train_ntk, mydat_train_y_ntk, diag_reg = diag_reg)\n",
        "  mypreds_train_ntk = predict_fn(x_test=mydat_train_ntk, get=get, compute_cov=False).flatten()\n",
        "  mypreds_val_ntk = predict_fn(x_test=mydat_val_ntk, get=get, compute_cov=False).flatten()\n",
        "  mypreds_full_ntk = predict_fn(x_test=mydat_array_ntk, get=get, compute_cov=False).flatten()\n",
        "  errs_ntk_train = mypreds_train_ntk - mydat_train_y\n",
        "  errs_ntk_val =mypreds_val_ntk-mydat_val_y\n",
        "  errs_ntk_test = mypreds_full_ntk-mydat_y\n",
        "  mae_val[i] = np.mean(np.absolute(errs_ntk_val))\n",
        "  mse_val[i] = np.mean(np.square(errs_ntk_val))\n",
        "  mae_test[i] = np.mean(np.absolute(errs_ntk_test))\n",
        "  mse_test[i] = np.mean(np.square(errs_ntk_test))\n",
        "  perr[i] = np.mean(errs_ntk_test)/mydat_y.mean()\n",
        "  mse_train[i] = np.mean(np.square(errs_ntk_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apGEHpFBiN69",
        "outputId": "4ab6a272-71d2-4bb5-e11a-5d20bf7d5b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.4\n",
            "737050185.3168143\n",
            "744876402.6437538\n",
            "17029.118320987844\n",
            "26.980048617911322\n",
            "16.8342116409053\n",
            "0.7860746694324796\n",
            "13.69757080078125\n"
          ]
        }
      ],
      "source": [
        "random.seed(10)\n",
        "#W_std = (sum(np.isnan(mse_test)) + np.argmin(mse_test[~np.isnan(mse_test)]))/10+0.1\n",
        "W_std = (sum(np.isnan(mse_val)) + np.argmin(mse_val[~np.isnan(mse_val)]))/10+0.1\n",
        "\n",
        "print(W_std)\n",
        "b_std = 0.1\n",
        "myactivation = stax.Relu #stax.Erf, stax.Gelu\n",
        "\n",
        "init_fn, apply_fn, kernel_fn = stax.serial(\n",
        "    stax.Dense(128, W_std=W_std, b_std=b_std), myactivation(),\n",
        "    stax.Dense(1, W_std=W_std, b_std=b_std)\n",
        ")\n",
        "\n",
        "diag_reg = 0\n",
        "start = time.time()\n",
        "kernel_fn_jit = jit(kernel_fn, static_argnames='get')\n",
        "Ktrain_train = np.array(kernel_fn_jit(mydat_train, mydat_train, 'ntk'))\n",
        "Ktrain_train_reg = Ktrain_train + diag_reg*np.identity(len(mydat_train_y))\n",
        "Ktrain_full = np.array(kernel_fn_jit(mydat_array, mydat_train, 'ntk'))\n",
        "Ktrain_val = np.array(kernel_fn_jit(mydat_val, mydat_train, 'ntk'))\n",
        "alpha = np.linalg.solve(Ktrain_train_reg, mydat_train_y)\n",
        "mypreds_train_ntk = np.matmul(Ktrain_train, alpha).flatten()\n",
        "mypreds_val_ntk = np.matmul(Ktrain_val, alpha).flatten()\n",
        "mypreds_full_ntk = np.matmul(Ktrain_full, alpha).flatten()\n",
        "#get = 'ntk'\n",
        "#predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn_jit, mydat_train_ntk, mydat_train_y_ntk, diag_reg = diag_reg)\n",
        "#mypreds_train_ntk = predict_fn(x_test=mydat_train_ntk, get=get, compute_cov=False).flatten()\n",
        "#mypreds_val_ntk = predict_fn(x_test=mydat_val_ntk, get=get, compute_cov=False).flatten()\n",
        "#mypreds_full_ntk = predict_fn(x_test=mydat_array_ntk, get=get, compute_cov=False).flatten()\n",
        "end = time.time()\n",
        "\n",
        "print(np.mean(np.square(mypreds_val_ntk-mydat_val_y)))\n",
        "errs_ntk = mydat_y - mypreds_full_ntk\n",
        "print(np.mean(np.square(errs_ntk)))\n",
        "print(np.mean(np.absolute(errs_ntk)))\n",
        "print(np.sqrt(np.mean(np.square(errs_ntk)))/mave*100)\n",
        "print(np.mean(np.absolute(errs_ntk))/mave*100)\n",
        "print(np.mean(errs_ntk)/mydat_y.mean()*100)\n",
        "print(end-start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9lgBtOyK8SM"
      },
      "source": [
        "## Further tuning for bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWlag5XaeDHm"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.example_libraries import optimizers\n",
        "from jax import jit, grad, vmap\n",
        "import jax.numpy as jnp\n",
        "import functools\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import predict, stax\n",
        "import numpy as np\n",
        "from random import seed, sample\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from scipy import stats\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def unflatten(x):\n",
        "  return np.reshape(x, (len(x), 1))\n",
        "\n",
        "mydat_train_y_ntk = jnp.array(unflatten(mydat_train_y))\n",
        "mydat_train_ntk = jnp.array(mydat_train)\n",
        "mydat_val_y_ntk = jnp.array(unflatten(mydat_val_y))\n",
        "mydat_val_ntk = jnp.array(mydat_val)\n",
        "mydat_y_ntk = jnp.array(unflatten(mydat_y))\n",
        "mydat_array_ntk = jnp.array(mydat_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjJ4v5IGdf3_",
        "outputId": "6b0ea14e-9bf0-4acc-f229-1699bab6727c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/VA\n",
            "/content/drive/MyDrive/NTK/Revision/data\n",
            "3.6\n",
            "0.0\n",
            "0.1\n",
            "0.2\n",
            "0.3\n",
            "0.4\n",
            "0.5\n",
            "0.6\n",
            "0.7\n",
            "0.8\n",
            "0.9\n",
            "1.0\n",
            "1.1\n",
            "1.2\n",
            "1.3\n",
            "1.4\n",
            "1.5\n",
            "1.6\n",
            "1.7\n",
            "1.8\n",
            "1.9\n",
            "2.0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/VA/\n",
        "ntk_mat = np.loadtxt('ntk_mat_gelu_srs_reg.csv', delimiter = ',')\n",
        "mse_val = ntk_mat[1]\n",
        "W_std = (sum(np.isnan(mse_val)) + np.argmin(mse_val[~np.isnan(mse_val)]))/10+0.1\n",
        "print(W_std)\n",
        "\n",
        "myactivation = stax.Gelu\n",
        "L = 21\n",
        "mae_val = np.zeros(L)\n",
        "mse_val = np.zeros(L)\n",
        "mae_test = np.zeros(L)\n",
        "mse_test = np.zeros(L)\n",
        "mse_train = np.zeros(L)\n",
        "perr = np.zeros(L)\n",
        "for i in range(L):\n",
        "  random.seed(10)\n",
        "  b_std = i/10\n",
        "  print(b_std)\n",
        "\n",
        "  init_fn, apply_fn, kernel_fn = stax.serial(\n",
        "    stax.Dense(128, W_std=W_std, b_std=b_std), myactivation(),\n",
        "    #stax.Dense(128, W_std=W_std, b_std=b_std), myactivation(),\n",
        "    stax.Dense(1, W_std=W_std, b_std=b_std)\n",
        "  )\n",
        "\n",
        "  kernel_fn_jit = jit(kernel_fn, static_argnames='get')\n",
        "  get = 'ntk'\n",
        "\n",
        "  #Ktrain_train = np.array(kernel_fn(mydat_train, mydat_train, 'ntk'))\n",
        "  #Ktrain_train_reg = Ktrain_train + 0.001*np.identity(len(mydat_train_y))\n",
        "  #Ktrain_full = np.array(kernel_fn(mydat_array, mydat_train, 'ntk'))\n",
        "  #Ktrain_val = np.array(kernel_fn(mydat_val, mydat_train, 'ntk'))\n",
        "  #alpha = np.linalg.solve(Ktrain_train_reg, mydat_train_y)\n",
        "  #mypreds_train_ntk = np.matmul(Ktrain_train, alpha)\n",
        "  #mypreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "  #mypreds_full_ntk = np.matmul(Ktrain_full, alpha)\n",
        "  predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn_jit, mydat_train_ntk, mydat_train_y_ntk, diag_reg = 1e-4)\n",
        "  mypreds_train_ntk = predict_fn(x_test=mydat_train_ntk, get=get, compute_cov=False).flatten()\n",
        "  mypreds_val_ntk = predict_fn(x_test=mydat_val_ntk, get=get, compute_cov=False).flatten()\n",
        "  mypreds_full_ntk = predict_fn(x_test=mydat_array_ntk, get=get, compute_cov=False).flatten()\n",
        "  errs_ntk_train = mypreds_train_ntk - mydat_train_y\n",
        "  errs_ntk_val =mypreds_val_ntk-mydat_val_y\n",
        "  errs_ntk_test = mypreds_full_ntk-mydat_y\n",
        "  mae_val[i] = np.mean(np.absolute(errs_ntk_val))\n",
        "  mse_val[i] = np.mean(np.square(errs_ntk_val))\n",
        "  mae_test[i] = np.mean(np.absolute(errs_ntk_test))\n",
        "  mse_test[i] = np.mean(np.square(errs_ntk_test))\n",
        "  perr[i] = np.mean(errs_ntk_test)/mydat_y.mean()\n",
        "  mse_train[i] = np.mean(np.square(errs_ntk_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "IyPexY1ef7-K",
        "outputId": "9d000e6d-e317-453f-9b3f-fb786ac334fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8.58757039 8.58667464 8.58434233 8.58164278 8.58054911 8.57921577\n",
            " 8.57779339 8.57876925 8.57948335 8.5803784  8.5829333  8.58364232\n",
            " 8.58676898 8.59011298 8.59217626 8.5951496  8.5975815  8.60113417\n",
            " 8.60288513 8.60625396 8.60907675]\n",
            "0.3\n",
            "0.6\n",
            "378262592.0\n",
            "381630240.0\n",
            "386874752.0\n",
            "-2.2260847872543565\n",
            "-1.3556097866009087\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x79ef0c2acc10>]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "L=21\n",
        "plt.figure(figsize = (3, 2))\n",
        "xvals = np.array(range(L))/(L-1)*2\n",
        "W_val = xvals[(sum(np.isnan(mse_val)) + np.argmin(mse_val[~np.isnan(mse_val)]))]\n",
        "W_test = xvals[(sum(np.isnan(mse_test)) + np.argmin(mse_test[~np.isnan(mse_test)]))]\n",
        "\n",
        "plt.plot(xvals,np.log10(mse_val))\n",
        "plt.plot(xvals, np.log10(mse_test))\n",
        "plt.ylim([8.55, 9.05])\n",
        "#plt.plot([1.5, 1.5], [8.5, 9])\n",
        "plt.ylabel('log(MSE)')\n",
        "plt.xlabel('sigma_b')\n",
        "#plt.legend(['val', 'test'])\n",
        "print(np.log10(mse_test))\n",
        "print(W_val)\n",
        "print(W_test)\n",
        "mse_test_min = np.min(mse_test[~np.isnan(mse_test)])\n",
        "mse_test_val = mse_test[~np.isnan(mse_test)][np.argmin(mse_val[~np.isnan(mse_val)])]\n",
        "print(mse_test_min)\n",
        "print(mse_test_val)\n",
        "print(mse_test[0])\n",
        "print(100*(mse_test_min/mse_test[0]-1))\n",
        "print(100*(mse_test_val/mse_test[0]-1))\n",
        "plt.plot(W_val, np.log10(np.min(mse_val[~np.isnan(mse_val)])), marker = \"*\", ls = None, color = \"#1f77b4\")\n",
        "plt.plot(W_test, np.log10(np.min(mse_test[~np.isnan(mse_test)])), marker = \"*\", ls = None, color = \"#ff7f0e\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7I3K4eT6b86"
      },
      "source": [
        "## Coordinate descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "ovVrX7Qx60In",
        "outputId": "8d486c68-1f5f-4c8d-c5bf-8c12becb40f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_std\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-22db5270289c>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mmytrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mKtrain_train_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKtrain_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiag_reg\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmytrace\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mKtrain_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_fn_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydat_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mmypreds_val_ntk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n\u001b[0m\u001b[1;32m    333\u001b[0m         fun, jit_info, *args, **kwargs)\n\u001b[1;32m    334\u001b[0m     \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_most_recent_pjit_call_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mout_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpjit_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mpxla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeviceAssignmentMismatchError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mfails\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2737\u001b[0m     top_trace = (top_trace if not axis_main or axis_main.level < top_trace.level\n\u001b[1;32m   2738\u001b[0m                  else axis_main.with_cur_sublevel())\n\u001b[0;32m-> 2739\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpop_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    937\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcall_impl_with_key_reuse_checks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1728\u001b[0m   has_explicit_sharding = _pjit_explicit_sharding(\n\u001b[1;32m   1729\u001b[0m       in_shardings, out_shardings, None, None)\n\u001b[0;32m-> 1730\u001b[0;31m   return xc._xla.pjit(\n\u001b[0m\u001b[1;32m   1731\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_impl_cache_miss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonated_argnums\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m       \u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcall_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                     donated_invars, name, keep_unused, inline):\n\u001b[1;32m   1711\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_impl_cache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m     out_flat, compiled = _pjit_call_impl_python(\n\u001b[0m\u001b[1;32m   1713\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_shardings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_shardings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[0mout_shardings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_shardings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_layouts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_layouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1640\u001b[0m       \u001b[0mlowering_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoweringParameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m       \u001b[0mpgle_profiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpgle_profiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m   ).compile(compile_options)\n\u001b[0m\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m   \u001b[0m_most_recent_pjit_call_executable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweak_key_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiler_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMeshExecutable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompiler_options\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m       executable = UnloadedMeshExecutable.from_hlo(\n\u001b[0m\u001b[1;32m   2296\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m           compiler_options=compiler_options)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36mfrom_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2805\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2807\u001b[0;31m     xla_executable = _cached_compilation(\n\u001b[0m\u001b[1;32m   2808\u001b[0m         \u001b[0mhlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspmd_lowering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m         \u001b[0mtuple_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_spmd_lowering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_prop_to_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values, pgle_profiler)\u001b[0m\n\u001b[1;32m   2619\u001b[0m       \u001b[0;34m\"Finished XLA compilation of {fun_name} in {elapsed_time:.9f} sec\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m       fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2621\u001b[0;31m     xla_executable = compiler.compile_or_get_cached(\n\u001b[0m\u001b[1;32m   2622\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m         pgle_profiler)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[0m\n\u001b[1;32m    397\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mlog_persistent_cache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     return _compile_and_write_cache(\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    625\u001b[0m ) -> xc.LoadedExecutable:\n\u001b[1;32m    626\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m   executable = backend_compile(\n\u001b[0m\u001b[1;32m    628\u001b[0m       \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0;31m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;31m# to take in `host_callbacks`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilt_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m def compile_or_get_cached(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "mydat_array = mydat.values\n",
        "def makenpfloat(vec):\n",
        "  return np.array(vec, dtype = np.float32)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.example_libraries import optimizers\n",
        "from jax import jit, grad, vmap\n",
        "import jax.numpy as jnp\n",
        "import functools\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import predict, stax\n",
        "import numpy as np\n",
        "from random import seed, sample\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from scipy import stats\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def unflatten(x):\n",
        "  return np.reshape(x, (len(x), 1))\n",
        "\n",
        "activations = [stax.Relu]\n",
        "K = 150\n",
        "B = 21\n",
        "R = 81\n",
        "W_vec = np.array(range(K))/10+0.1\n",
        "B_vec = np.array(range(B))/10\n",
        "R_vec = np.concatenate(([0], 10**((np.array(range(R))-(R-1))/10)))\n",
        "seeds = [10]\n",
        "nreps = len(seeds)\n",
        "ntk_mat_cd = np.zeros((nreps*len(activations), 10))\n",
        "for L in range(len(activations)):\n",
        "  kernel_fn_list = [[] for _ in range(150)]\n",
        "  activation = activations[L]\n",
        "  for i in range(K):\n",
        "    W_std = (i+1)/10\n",
        "    for j in range(B):\n",
        "      b_std = j/10\n",
        "      init_fn, apply_fn, kernel_fn = stax.serial(\n",
        "        stax.Dense(128, W_std=W_std, b_std=b_std), activation(),\n",
        "        stax.Dense(1, W_std=W_std, b_std=b_std)\n",
        "      )\n",
        "      kernel_fn_jit = jit(kernel_fn, static_argnames='get')\n",
        "      kernel_fn_list[i].append(kernel_fn_jit)\n",
        "  for M in range(nreps):\n",
        "\n",
        "    current_seed = seeds[M]\n",
        "\n",
        "    mydat_train, mydat_val, mydat_train_emb, mydat_val_emb, mydat_train_y, mydat_val_y  = train_test_split(\n",
        "        mydat_array, mydat_emb, mydat_y, test_size=340/190000,train_size=680/190000,\n",
        "        stratify = mydat_emb['productType'], random_state =current_seed)\n",
        "\n",
        "    mydat_train = makenpfloat(mydat_train)\n",
        "    mydat_val = makenpfloat(mydat_val)\n",
        "    mydat_train_y = makenpfloat(mydat_train_y).flatten()\n",
        "    mydat_val_y = makenpfloat(mydat_val_y).flatten()\n",
        "\n",
        "    mse_val_w = np.zeros(K)\n",
        "    mse_val_b = np.zeros(B)\n",
        "    mse_val_r = np.zeros(R+1)\n",
        "    b_std = 0.1\n",
        "    b_ind = int(10*b_std)\n",
        "    W_std = 3\n",
        "    diag_reg = 1e-4\n",
        "    tol = 1e-4\n",
        "    rel_improvement = 1\n",
        "    error_old = 1e+10\n",
        "    while rel_improvement > tol:\n",
        "      print(\"W_std\")\n",
        "      for i in range(K):\n",
        "        print(i)\n",
        "        random.seed(10)\n",
        "        currentW_std = (i+1)/10\n",
        "        W_ind = int(10*(currentW_std -0.1))\n",
        "        kernel_fn_jit = kernel_fn_list[i][b_ind]\n",
        "        get = 'ntk'\n",
        "        Ktrain_train = np.array(kernel_fn_jit(mydat_train, None, get))\n",
        "        mytrace = np.mean(np.trace(Ktrain_train))\n",
        "        Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(mydat_train_y))\n",
        "        Ktrain_val = np.array(kernel_fn_jit(mydat_val, mydat_train, get))\n",
        "        alpha = np.linalg.solve(Ktrain_train_reg, mydat_train_y)\n",
        "        mypreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "        errs_val_ntk =mypreds_val_ntk-mydat_val_y\n",
        "        mse_val_w[i] = np.mean(np.square(errs_val_ntk))\n",
        "      W_ind = np.argmin(mse_val_w)\n",
        "      W_std = W_ind/10+0.1\n",
        "      if error_old == 1e+10:\n",
        "        error_old = np.min(mse_val_w)\n",
        "      kernel_fn_jit = kernel_fn_list[W_ind][b_ind]\n",
        "      Ktrain_train = np.array(kernel_fn_jit(mydat_train, None, get))\n",
        "      Ktrain_val = np.array(kernel_fn_jit(mydat_val, mydat_train, get))\n",
        "      mytrace = np.mean(np.trace(Ktrain_train))\n",
        "\n",
        "\n",
        "      print(\"Diag\")\n",
        "      for i in range(R+1):\n",
        "        diag_reg = R_vec[i]\n",
        "        print(i)\n",
        "        Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(mydat_train_y))\n",
        "        alpha = np.linalg.solve(Ktrain_train_reg, mydat_train_y)\n",
        "        mypreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "        errs_val_ntk = mypreds_val_ntk - mydat_val_y\n",
        "        mse_val_r[i] = np.mean(np.square(errs_val_ntk))\n",
        "      diag_reg = R_vec[np.argmin(mse_val_r)]\n",
        "      print(\"b_std\")\n",
        "      for i in range(B):\n",
        "        print(i)\n",
        "        kernel_fn_jit = kernel_fn_list[W_ind][i]\n",
        "        Ktrain_train = np.array(kernel_fn_jit(mydat_train, None, get))\n",
        "        mytrace = np.mean(np.trace(Ktrain_train))\n",
        "        Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(mydat_train_y))\n",
        "        Ktrain_val = np.array(kernel_fn_jit(mydat_val, mydat_train, get))\n",
        "        alpha = np.linalg.solve(Ktrain_train_reg, mydat_train_y)\n",
        "        mypreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "        errs_val_ntk = mypreds_val_ntk - mydat_val_y\n",
        "        mse_val_b[i] = np.mean(np.square(errs_val_ntk))\n",
        "      error_new = np.min(mse_val_b)\n",
        "      b_ind = np.argmin(mse_val_b)\n",
        "      rel_improvement = np.log(error_old)-np.log(error_new)\n",
        "      error_old = error_new\n",
        "    W_std = W_ind/10 + 0.1\n",
        "    b_std = b_ind/10\n",
        "    kernel_fn_jit = kernel_fn_list[W_ind][b_ind]\n",
        "    start = time.time()\n",
        "    Ktrain_train = np.array(kernel_fn_jit(mydat_train, mydat_train, get))\n",
        "    mytrace = np.mean(np.trace(Ktrain_train))\n",
        "    Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(mydat_train_y))\n",
        "    Ktrain_full = np.array(kernel_fn_jit(mydat_array, mydat_train, 'ntk'))\n",
        "    Ktrain_val = np.array(kernel_fn_jit(mydat_val, mydat_train, get))\n",
        "    alpha = np.linalg.solve(Ktrain_train_reg, mydat_train_y)\n",
        "    mypreds_full_ntk = np.matmul(Ktrain_full, alpha)\n",
        "    mypreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "    end = time.time()\n",
        "    time_eval = start-end\n",
        "    errs_full_ntk =mypreds_full_ntk-mydat_y\n",
        "    errs_val_ntk = mypreds_val_ntk - mydat_val_y\n",
        "    currentind = 5*L+M\n",
        "    ntk_mat_cd[currentind, 0] = np.mean(np.square(errs_full_ntk))/1e+6\n",
        "    ntk_mat_cd[currentind, 1] = np.mean(np.absolute(errs_full_ntk))/1e+3\n",
        "    ntk_mat_cd[currentind, 2] = np.sqrt(ntk_mat_cd[currentind, 0])/(mave/1e+3)\n",
        "    ntk_mat_cd[currentind, 3] = ntk_mat_cd[currentind, 1]/(mave/1e+3)\n",
        "    ntk_mat_cd[currentind, 4] = np.mean(errs_full_ntk)/mave\n",
        "    ntk_mat_cd[currentind, 5] = time_eval\n",
        "    ntk_mat_cd[currentind, 6] = W_std\n",
        "    ntk_mat_cd[currentind, 7] = diag_reg\n",
        "    ntk_mat_cd[currentind, 8] = b_std\n",
        "    ntk_mat_cd[currentind, 9] = np.mean(np.square(errs_val_ntk))/1e+6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jITVb88lkHiI"
      },
      "source": [
        "## Separate networks NTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZenBAoJWjsQG"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.example_libraries import optimizers\n",
        "from jax import jit, grad, vmap\n",
        "import jax.numpy as jnp\n",
        "import functools\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import predict, stax\n",
        "import numpy as np\n",
        "from random import seed, sample\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from scipy import stats\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import random\n",
        "#import kernel\n",
        "#import eigenpro\n",
        "import jax\n",
        "from jax.lib import xla_bridge\n",
        "import jax.profiler\n",
        "from datetime import datetime\n",
        "\n",
        "torch.cuda.is_available()\n",
        "#print(jax.devices())\n",
        "#print(xla_bridge.get_backend().platform)\n",
        "#!nvidia-smi\n",
        "K=150\n",
        "b_std = 0.1\n",
        "diag_reg = 0\n",
        "activation = stax.Gelu\n",
        "kernelfn_vec = []\n",
        "for j in range(K):\n",
        "  W_std = (j+1)/10\n",
        "  init_fn, apply_fn, kernel_fn = stax.serial(\n",
        "     stax.Dense(128, W_std=W_std, b_std=b_std), activation(),\n",
        "     stax.Dense(1, W_std=W_std, b_std=b_std)\n",
        "  )\n",
        "  kernel_fn_jit = jit(kernel_fn, static_argnames='get')\n",
        "  kernelfn_vec.append(kernel_fn_jit)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "accRxLFqXGyc",
        "outputId": "3f1b9c5f-0073-47e6-b760-55a49875e9cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-601569c8adb4>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mKtrain_train_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKtrain_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiag_reg\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentdat_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mKtrain_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_fn_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentdat_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentdat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ntk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mKtrain_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_fn_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentdat_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentdat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ntk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentdat_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n\u001b[0m\u001b[1;32m    333\u001b[0m         fun, jit_info, *args, **kwargs)\n\u001b[1;32m    334\u001b[0m     \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_most_recent_pjit_call_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_python_pjit_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m   \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_flat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_infer_params\u001b[0;34m(fun, ji, args, kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m       fun, ji, signature, avals, pjit_mesh, resource_env)\n\u001b[1;32m    735\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpjit_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     p, args_flat = _infer_params_impl(\n\u001b[0m\u001b[1;32m    737\u001b[0m         fun, ji, pjit_mesh, resource_env, args, kwargs, in_avals=avals)\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs_tracked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_infer_params_impl\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0mattr_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_attr_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m   jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr(\n\u001b[0m\u001b[1;32m    634\u001b[0m       \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       \u001b[0mHashableFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    350\u001b[0m       \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_stores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mexplain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_cache_misses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnew_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_create_pjit_jaxpr\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mattrs_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m       jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n\u001b[0m\u001b[1;32m   1278\u001b[0m           fun, in_type, debug_info=pe_debug)\n\u001b[1;32m   1279\u001b[0m       \u001b[0;31m# assert attr_data is sentinel or attr_data matches attrs_tracked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_dynamic\u001b[0;34m(fun, in_avals, debug_info, keep_inputs)\u001b[0m\n\u001b[1;32m   2353\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDynamicJaxprTrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaxpr_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m     jaxpr, out_avals, consts, attrs_tracked = trace_to_subjaxpr_dynamic(\n\u001b[0m\u001b[1;32m   2356\u001b[0m       fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)\n\u001b[1;32m   2357\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals, keep_inputs, debug_info)\u001b[0m\n\u001b[1;32m   2376\u001b[0m     \u001b[0min_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_type_to_tracers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m     \u001b[0min_tracers_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2378\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2379\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/utils/utils.py\u001b[0m in \u001b[0;36mh\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/utils/utils.py\u001b[0m in \u001b[0;36mgetter_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m                                                           len(args)])\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m       \u001b[0mfn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcanonicalized_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0;34m@\u001b[0m\u001b[0mnt_tree_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/requirements.py\u001b[0m in \u001b[0;36mkernel_fn_any\u001b[0;34m(x1_or_kernel, x2, get, pattern, mask_constant, diagonal_batch, diagonal_spatial, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                               **kwargs)\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m     return kernel_fn_x1(x1_or_kernel, x2, get,\n\u001b[0m\u001b[1;32m   1047\u001b[0m                         \u001b[0mpattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                         \u001b[0mdiagonal_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiagonal_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/requirements.py\u001b[0m in \u001b[0;36mkernel_fn_x1\u001b[0;34m(x1, x2, get, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m     \u001b[0mout_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/utils/utils.py\u001b[0m in \u001b[0;36mh\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/requirements.py\u001b[0m in \u001b[0;36mnew_kernel_fn\u001b[0;34m(k, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m               \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mkernel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0m_set_req\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_kernel_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozendict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrozendict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatic_reqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/combinators.py\u001b[0m in \u001b[0;36mkernel_fn\u001b[0;34m(k, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# inside kernel functions here and parallel below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkernel_fns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/utils/utils.py\u001b[0m in \u001b[0;36mh\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/utils/utils.py\u001b[0m in \u001b[0;36mgetter_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m                                                           len(args)])\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m       \u001b[0mfn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcanonicalized_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0;34m@\u001b[0m\u001b[0mnt_tree_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/requirements.py\u001b[0m in \u001b[0;36mkernel_fn_any\u001b[0;34m(x1_or_kernel, x2, get, pattern, mask_constant, diagonal_batch, diagonal_spatial, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_or_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKernel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       return kernel_fn_kernel(x1_or_kernel,\n\u001b[0m\u001b[1;32m   1041\u001b[0m                               \u001b[0mpattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                               \u001b[0mdiagonal_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiagonal_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/requirements.py\u001b[0m in \u001b[0;36mkernel_fn_kernel\u001b[0;34m(kernel, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mkernel_fn_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m     \u001b[0mout_kernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_kernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mkernel_fn_x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/requirements.py\u001b[0m in \u001b[0;36m_set_shapes\u001b[0;34m(init_fn, apply_fn, in_kernel, out_kernel, **kwargs)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m   \u001b[0mshape1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_propagate_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munmask_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m   \u001b[0mshape2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_propagate_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munmask_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapply_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m   \u001b[0mset_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/requirements.py\u001b[0m in \u001b[0;36m_propagate_shape\u001b[0;34m(init_fn, apply_fn, shaped, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m   \u001b[0makey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m     \u001b[0mshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_and_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0makey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;31m# Some layers do not implement an `apply_fn` and in this case we keep the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36meval_shape\u001b[0;34m(fun, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2821\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2822\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2823\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36meval_shape\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0meval_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m     \u001b[0mout_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_unspecified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_shardings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;31m# TODO(yashkatariya): Add `Layout` to SDS.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_infer_params\u001b[0;34m(fun, ji, args, kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m       fun, ji, signature, avals, pjit_mesh, resource_env)\n\u001b[1;32m    735\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpjit_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     p, args_flat = _infer_params_impl(\n\u001b[0m\u001b[1;32m    737\u001b[0m         fun, ji, pjit_mesh, resource_env, args, kwargs, in_avals=avals)\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs_tracked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_infer_params_impl\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0mattr_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_attr_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m   jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr(\n\u001b[0m\u001b[1;32m    634\u001b[0m       \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       \u001b[0mHashableFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    350\u001b[0m       \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_stores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mexplain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_cache_misses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnew_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_create_pjit_jaxpr\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mattrs_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m       jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n\u001b[0m\u001b[1;32m   1278\u001b[0m           fun, in_type, debug_info=pe_debug)\n\u001b[1;32m   1279\u001b[0m       \u001b[0;31m# assert attr_data is sentinel or attr_data matches attrs_tracked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_dynamic\u001b[0;34m(fun, in_avals, debug_info, keep_inputs)\u001b[0m\n\u001b[1;32m   2353\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDynamicJaxprTrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaxpr_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m     jaxpr, out_avals, consts, attrs_tracked = trace_to_subjaxpr_dynamic(\n\u001b[0m\u001b[1;32m   2356\u001b[0m       fun, main, in_avals, keep_inputs=keep_inputs, debug_info=debug_info)\n\u001b[1;32m   2357\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals, keep_inputs, debug_info)\u001b[0m\n\u001b[1;32m   2376\u001b[0m     \u001b[0min_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_input_type_to_tracers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2377\u001b[0m     \u001b[0min_tracers_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2378\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2379\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2380\u001b[0m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;31m# Some transformations yield from inside context managers, so we have to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/requirements.py\u001b[0m in \u001b[0;36minit_and_apply\u001b[0;34m(rng, x)\u001b[0m\n\u001b[1;32m    840\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0minit_and_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m   \u001b[0makey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/utils/utils.py\u001b[0m in \u001b[0;36mh\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/requirements.py\u001b[0m in \u001b[0;36mfn_no_mask\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfn_no_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/requirements.py\u001b[0m in \u001b[0;36mapply_fn_with_masking\u001b[0;34m(params, inputs, mask_constant, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m             is_leaf=is_leaf)\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         outputs_mask = mask_fn(mask,\n\u001b[1;32m    221\u001b[0m                                \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/stax/linear.py\u001b[0m in \u001b[0;36mapply_fn\u001b[0;34m(params, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mb_std\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mparameterization\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'standard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprod\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_forward_operator_to_aval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"_{name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;31m# Note: don't use isinstance here, because we don't want to raise for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n\u001b[0m\u001b[1;32m    333\u001b[0m         fun, jit_info, *args, **kwargs)\n\u001b[1;32m    334\u001b[0m     \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_most_recent_pjit_call_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_flat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs_tracked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mcheck_arg\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_jaxtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     raise TypeError(f\"Argument '{arg}' of type {type(arg)} is not a valid \"\n\u001b[1;32m    285\u001b[0m                     \"JAX type.\")\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mvalid_jaxtype\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalid_jaxtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m     \u001b[0mconcrete_aval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1491\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def rmprod(dat):\n",
        "  return np.array(dat.loc[:, dat.columns != 'productType'].values)\n",
        "\n",
        "mse_val_ntk_mat = np.zeros((19, K))\n",
        "mse_train_ntk_mat = np.zeros((19, K))\n",
        "mse_remainder_ntk_mat = np.zeros((19, K))\n",
        "mae_val_ntk_mat = np.zeros((19, K))\n",
        "mae_remainder_ntk_mat = np.zeros((19, K))\n",
        "percenterr_ntk_mat = np.zeros((19, K))\n",
        "dat_train_list = []\n",
        "dat_val_list = []\n",
        "dat_full_list = []\n",
        "dat_train_y_list = []\n",
        "dat_val_y_list = []\n",
        "dat_full_y_list = []\n",
        "\n",
        "for i in range(19):\n",
        "  traininds = (mydat_train_emb['productType'] == i)\n",
        "  valinds = (mydat_val_emb['productType'] == i)\n",
        "  shift = 10000*i\n",
        "  dat_train_list.append(rmprod(mydat_train_emb[traininds]))\n",
        "  dat_val_list.append(rmprod(mydat_val_emb[valinds]))\n",
        "  dat_full_list.append(rmprod(mydat_emb[shift+0:shift+10000]))\n",
        "  dat_train_y_list.append(mydat_train_y[traininds].flatten())\n",
        "  dat_val_y_list.append(mydat_val_y[valinds].flatten())\n",
        "  dat_full_y_list.append(mydat_y[shift+0:shift+10000].flatten())\n",
        "\n",
        "for j in range(K):\n",
        "  print(j)\n",
        "  kernel_fn_jit = kernelfn_vec[j]\n",
        "\n",
        "  for i in range(19):\n",
        "    currentdat_train = dat_train_list[i]\n",
        "    currentdat_val = dat_val_list[i]\n",
        "    currentdat_full = dat_full_list[i]\n",
        "    currentdat_train_y = dat_train_y_list[i]\n",
        "    currentdat_val_y = dat_val_y_list[i]\n",
        "    currentdat_y = dat_full_y_list[i]\n",
        "    Ktrain_train = np.array(kernel_fn_jit(currentdat_train, None, 'ntk'))\n",
        "    #Ktrain_train_reg = Ktrain_train + diag_reg* np.mean(np.trace(Ktrain_train))*np.identity(len(currentdat_train_y))\n",
        "    Ktrain_train_reg = Ktrain_train + diag_reg* np.mean(np.trace(Ktrain_train))*np.identity(len(currentdat_train_y))\n",
        "    Ktrain_full = np.array(kernel_fn_jit(currentdat_full, currentdat_train, 'ntk'))\n",
        "    Ktrain_val = np.array(kernel_fn_jit(currentdat_val, currentdat_train, 'ntk'))\n",
        "    alpha = np.linalg.solve(Ktrain_train_reg, currentdat_train_y)\n",
        "\n",
        "    currentpreds_train_ntk = np.matmul(Ktrain_train, alpha)\n",
        "    currentpreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "    currentpreds_full_ntk = np.matmul(Ktrain_full, alpha)\n",
        "\n",
        "    currenterrs_train_ntk = currentpreds_train_ntk - currentdat_train_y\n",
        "    currenterrs_full_ntk = currentpreds_full_ntk - currentdat_y\n",
        "    currenterrs_val_ntk = currentpreds_val_ntk - currentdat_val_y\n",
        "    mse_val_ntk_mat[i, j] = np.mean(np.square(currenterrs_val_ntk))\n",
        "    mse_train_ntk_mat[i, j] = np.mean(np.square(currenterrs_train_ntk))\n",
        "    mae_val_ntk_mat[i, j] = np.mean(np.absolute(currenterrs_val_ntk))\n",
        "    mse_remainder_ntk_mat[i, j] = np.mean(np.square(currenterrs_full_ntk))\n",
        "    mae_remainder_ntk_mat[i, j] = np.mean(np.absolute(currenterrs_full_ntk))\n",
        "    percenterr_ntk_mat[i, j] = np.mean(currenterrs_full_ntk)/currentdat_y.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT03xq6tMyyz"
      },
      "source": [
        "### Coordinate descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "BE9EMLVKM7vZ",
        "outputId": "bd95ef2f-1198-4b17-c95e-5d4f0361a492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "W_std\n",
            "0\n",
            "0.1\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-07a29b732db6>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m           \u001b[0mmytrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m           \u001b[0mKtrain_train_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKtrain_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiag_reg\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmytrace\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentdat_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m           \u001b[0mKtrain_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_fn_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrentdat_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentdat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m           \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentdat_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m           \u001b[0mcurrentpreds_val_ntk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n\u001b[0m\u001b[1;32m    333\u001b[0m         fun, jit_info, *args, **kwargs)\n\u001b[1;32m    334\u001b[0m     \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_most_recent_pjit_call_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mout_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpjit_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mpxla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeviceAssignmentMismatchError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mfails\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2737\u001b[0m     top_trace = (top_trace if not axis_main or axis_main.level < top_trace.level\n\u001b[1;32m   2738\u001b[0m                  else axis_main.with_cur_sublevel())\n\u001b[0;32m-> 2739\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpop_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    937\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcall_impl_with_key_reuse_checks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1728\u001b[0m   has_explicit_sharding = _pjit_explicit_sharding(\n\u001b[1;32m   1729\u001b[0m       in_shardings, out_shardings, None, None)\n\u001b[0;32m-> 1730\u001b[0;31m   return xc._xla.pjit(\n\u001b[0m\u001b[1;32m   1731\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_impl_cache_miss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonated_argnums\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m       \u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcall_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                     donated_invars, name, keep_unused, inline):\n\u001b[1;32m   1711\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_impl_cache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m     out_flat, compiled = _pjit_call_impl_python(\n\u001b[0m\u001b[1;32m   1713\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_shardings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_shardings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[0mout_shardings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_shardings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_layouts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_layouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1640\u001b[0m       \u001b[0mlowering_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoweringParameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m       \u001b[0mpgle_profiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpgle_profiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m   ).compile(compile_options)\n\u001b[0m\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m   \u001b[0m_most_recent_pjit_call_executable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweak_key_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiler_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMeshExecutable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompiler_options\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m       executable = UnloadedMeshExecutable.from_hlo(\n\u001b[0m\u001b[1;32m   2296\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m           compiler_options=compiler_options)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36mfrom_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2805\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2807\u001b[0;31m     xla_executable = _cached_compilation(\n\u001b[0m\u001b[1;32m   2808\u001b[0m         \u001b[0mhlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspmd_lowering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m         \u001b[0mtuple_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_spmd_lowering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_prop_to_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values, pgle_profiler)\u001b[0m\n\u001b[1;32m   2619\u001b[0m       \u001b[0;34m\"Finished XLA compilation of {fun_name} in {elapsed_time:.9f} sec\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m       fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2621\u001b[0;31m     xla_executable = compiler.compile_or_get_cached(\n\u001b[0m\u001b[1;32m   2622\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m         pgle_profiler)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[0m\n\u001b[1;32m    397\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mlog_persistent_cache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     return _compile_and_write_cache(\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    625\u001b[0m ) -> xc.LoadedExecutable:\n\u001b[1;32m    626\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m   executable = backend_compile(\n\u001b[0m\u001b[1;32m    628\u001b[0m       \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0;31m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;31m# to take in `host_callbacks`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilt_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m def compile_or_get_cached(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "mydat_array = mydat.values\n",
        "def makenpfloat(vec):\n",
        "  return np.array(vec, dtype = np.float32)\n",
        "def rmprod(dat):\n",
        "  return np.array(dat.loc[:, dat.columns != 'productType'].values)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.example_libraries import optimizers\n",
        "from jax import jit, grad, vmap\n",
        "import jax.numpy as jnp\n",
        "import functools\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import predict, stax\n",
        "import numpy as np\n",
        "from random import seed, sample\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from scipy import stats\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def unflatten(x):\n",
        "  return np.reshape(x, (len(x), 1))\n",
        "\n",
        "activations = [stax.Relu, stax.Erf, stax.Gelu]\n",
        "K = 150\n",
        "B = 21\n",
        "R = 81\n",
        "W_vec = np.array(range(K))/10+0.1\n",
        "B_vec = np.array(range(B))/10\n",
        "R_vec = np.concatenate(([0], 10**((np.array(range(R))-(R-1))/10)))\n",
        "seeds = [10, 20, 30, 40, 50]\n",
        "nreps = 5\n",
        "ntk_mat_cd_prod = np.zeros((nreps*len(activations), 7))\n",
        "ntk_mat_cd_prod_sep = np.zeros((nreps*len(activations), 4, 19))\n",
        "\n",
        "\n",
        "for L in range(len(activations)):\n",
        "#for L in range(1):\n",
        "#  if ntk_mat_b_prod[5*L+4, 0] != 0:\n",
        "#    continue\n",
        "  kernel_fn_list = [[] for _ in range(150)]\n",
        "  activation = activations[L]\n",
        "  for i in range(K):\n",
        "    W_std = (i+1)/10\n",
        "    for j in range(B):\n",
        "      b_std = j/10\n",
        "      _, _, kernel_fn = stax.serial(\n",
        "        stax.Dense(128, W_std=W_std, b_std=b_std), activation(),\n",
        "        stax.Dense(1, W_std=W_std, b_std=b_std)\n",
        "      )\n",
        "      kernel_fn_jit = jit(kernel_fn, static_argnames='get')\n",
        "      kernel_fn_list[i].append(kernel_fn_jit)\n",
        "  for M in range(nreps):\n",
        "  # for M in range(1):\n",
        "    currentind = 5*L+M\n",
        "    print(currentind*100)\n",
        "#    if ntk_mat_b_prod[currentind, 0] !=0:\n",
        "#      continue\n",
        "\n",
        "    current_seed = seeds[M]\n",
        "\n",
        "    mydat_train, mydat_val, mydat_train_emb, mydat_val_emb, mydat_train_y, mydat_val_y  = train_test_split(\n",
        "        mydat_array, mydat_emb, mydat_y, test_size=340/190000,train_size=680/190000,\n",
        "        stratify = mydat_emb['productType'], random_state =current_seed)\n",
        "\n",
        "    mydat_train = makenpfloat(mydat_train)\n",
        "    mydat_val = makenpfloat(mydat_val)\n",
        "    mydat_train_y = makenpfloat(mydat_train_y).flatten()\n",
        "    mydat_val_y = makenpfloat(mydat_val_y).flatten()\n",
        "    mypreds_full_ntk = np.zeros(len(mydat_y))\n",
        "    mypreds_val_ntk = np.zeros(len(mydat_val_y))\n",
        "\n",
        "\n",
        "\n",
        "    mse_val_w = np.zeros(K)\n",
        "    mse_val_b = np.zeros(B)\n",
        "    mse_val_r = np.zeros(R+1)\n",
        "    b_vec = np.array([0.1]*19)\n",
        "    b_vec_ind = [1]*19\n",
        "    b1_vec = np.array([0.1]*19)\n",
        "    b1_vec_ind = np.array([1]*19)\n",
        "    w_vec = np.array([3]*19)\n",
        "    w_vec_ind = [31]*19\n",
        "    w1_vec = [0]*19\n",
        "    w1_vec_ind = [0]*19\n",
        "    r_vec = np.array([0]*19)\n",
        "    r1_vec = np.array([1e-4]*19)\n",
        "    r_vec_ind = [0]*19\n",
        "    tol = 1e-4\n",
        "    dat_train_list = []\n",
        "    dat_val_list = []\n",
        "    dat_full_list = []\n",
        "    dat_train_y_list = []\n",
        "    dat_val_y_list = []\n",
        "    dat_full_y_list = []\n",
        "    valinds_list = []\n",
        "\n",
        "\n",
        "    for k in range(19):\n",
        "      print(10*k)\n",
        "      traininds = (mydat_train_emb['productType'] == k)\n",
        "      valinds = (mydat_val_emb['productType'] == k)\n",
        "      valinds_list.append(valinds)\n",
        "      shift = 10000*k\n",
        "      dat_train_list.append(rmprod(mydat_train_emb[traininds]))\n",
        "      dat_val_list.append(rmprod(mydat_val_emb[valinds]))\n",
        "      dat_full_list.append(rmprod(mydat_emb[shift+0:shift+10000]))\n",
        "      dat_train_y_list.append(mydat_train_y[traininds].flatten())\n",
        "      dat_val_y_list.append(mydat_val_y[valinds].flatten())\n",
        "      dat_full_y_list.append(mydat_y[shift+0:shift+10000].flatten())\n",
        "      diag_reg = 0\n",
        "      b_ind = 1\n",
        "      b_std = 0.1\n",
        "      rel_improvement = 1\n",
        "      niter = 0\n",
        "\n",
        "      error_old = 1e+10\n",
        "\n",
        "      currentdat_train = dat_train_list[k]\n",
        "      currentdat_val = dat_val_list[k]\n",
        "      currentdat_train_y = dat_train_y_list[k]\n",
        "      currentdat_val_y = dat_val_y_list[k]\n",
        "      while rel_improvement > tol:\n",
        "        print(niter)\n",
        "        print(\"W_std\")\n",
        "        print(diag_reg)\n",
        "        print(b_std)\n",
        "        for i in range(K):\n",
        "          print(i)\n",
        "          random.seed(10)\n",
        "          kernel_fn_jit = kernel_fn_list[i][b_ind]\n",
        "          get = 'ntk'\n",
        "          Ktrain_train = np.array(kernel_fn_jit(currentdat_train, None, get))\n",
        "          mytrace = np.mean(np.trace(Ktrain_train))\n",
        "          Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(currentdat_train_y))\n",
        "          Ktrain_val = np.array(kernel_fn_jit(currentdat_val, currentdat_train, get))\n",
        "          alpha = np.linalg.solve(Ktrain_train_reg, currentdat_train_y)\n",
        "          currentpreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "          errs_val_ntk =currentpreds_val_ntk-currentdat_val_y\n",
        "          mse_val_w[i] = np.mean(np.square(errs_val_ntk))\n",
        "        W_ind = np.argmin(mse_val_w)\n",
        "        w_vec_ind[k] = W_ind\n",
        "        W_std = W_ind/10+0.1\n",
        "        w_vec[k] = W_std\n",
        "        if rel_improvement == 1:\n",
        "          w1_vec[k] = W_std\n",
        "          w1_vec_ind[k] = W_ind\n",
        "\n",
        "\n",
        "        if error_old == 1e+10:\n",
        "          error_old = np.min(mse_val_w)\n",
        "\n",
        "        kernel_fn_jit = kernel_fn_list[W_ind][b_ind]\n",
        "        Ktrain_train = np.array(kernel_fn_jit(currentdat_train, None, get))\n",
        "        Ktrain_val = np.array(kernel_fn_jit(currentdat_val, currentdat_train, get))\n",
        "        mytrace = np.mean(np.trace(Ktrain_train))\n",
        "        print(\"Diag\")\n",
        "        print(W_std)\n",
        "        print(0.1*b_ind)\n",
        "        for i in range(R+1):\n",
        "          diag_reg = R_vec[i]\n",
        "          print(i)\n",
        "          Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(currentdat_train_y))\n",
        "          alpha = np.linalg.solve(Ktrain_train_reg, currentdat_train_y)\n",
        "          currentpreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "          errs_val_ntk = currentpreds_val_ntk - currentdat_val_y\n",
        "          mse_val_r[i] = np.mean(np.square(errs_val_ntk))\n",
        "        diag_reg = R_vec[np.argmin(mse_val_r)]\n",
        "        r_vec[k] = diag_reg\n",
        "        r_vec_ind[k] =np.argmin(mse_val_r)\n",
        "        if rel_improvement == 1:\n",
        "          r1_vec[k] = diag_reg\n",
        "\n",
        "        print(\"b_std\")\n",
        "        print(W_std)\n",
        "        print(diag_reg)\n",
        "        for i in range(B):\n",
        "          print(i)\n",
        "          kernel_fn_jit = kernel_fn_list[W_ind][i]\n",
        "          Ktrain_train = np.array(kernel_fn_jit(currentdat_train, None, get))\n",
        "          mytrace = np.mean(np.trace(Ktrain_train))\n",
        "          Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(currentdat_train_y))\n",
        "          Ktrain_val = np.array(kernel_fn_jit(currentdat_val, currentdat_train, get))\n",
        "          alpha = np.linalg.solve(Ktrain_train_reg, currentdat_train_y)\n",
        "          currentpreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "          errs_val_ntk = currentpreds_val_ntk - currentdat_val_y\n",
        "          mse_val_b[i] = np.mean(np.square(errs_val_ntk))\n",
        "        b_ind = np.argmin(mse_val_b)\n",
        "        b_vec_ind[k] = b_ind\n",
        "        b_std = 0.1*b_ind\n",
        "        b_vec[k] = b_std\n",
        "\n",
        "        if rel_improvement == 1:\n",
        "          b1_vec[k] = b_ind/10\n",
        "          b1_vec_ind[k] = b_ind\n",
        "\n",
        "        error_new = np.min(mse_val_b)\n",
        "        rel_improvement = np.log(error_old)-np.log(error_new)\n",
        "        error_old = error_new\n",
        "        niter +=1\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Coordinate descent\")\n",
        "    print(w_vec_ind)\n",
        "    print(b_vec_ind)\n",
        "    print(R_vec[r_vec_ind])\n",
        "    currentind = 5*L+M\n",
        "    start = time.time()\n",
        "    for k in range(19):\n",
        "      W_ind = w_vec_ind[k]\n",
        "      W_std = W_ind/10 + 0.1\n",
        "      b_ind = b_vec_ind[k]\n",
        "      b_std = b_ind/10\n",
        "      diag_reg = R_vec[r_vec_ind[k]]\n",
        "      init_fn, apply_fn, kernel_fn = stax.serial(\n",
        "        stax.Dense(128, W_std=W_std, b_std=b_std), activation(),\n",
        "        stax.Dense(1, W_std=W_std, b_std=b_std)\n",
        "      )\n",
        "      currentdat_train = dat_train_list[k]\n",
        "      currentdat_full = dat_full_list[k]\n",
        "      currentdat_train_y = dat_train_y_list[k]\n",
        "      currentdat_val_y = dat_val_y_list[k]\n",
        "      currentdat_val = dat_val_list[k]\n",
        "      currentdat_y = dat_full_y_list[k]\n",
        "      kernel_fn_jit = jit(kernel_fn, static_argnames='get')\n",
        "\n",
        "      Ktrain_train = np.array(kernel_fn_jit(currentdat_train, None, get))\n",
        "      mytrace = np.mean(np.trace(Ktrain_train))\n",
        "      Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(currentdat_train_y))\n",
        "      Ktrain_full = np.array(kernel_fn_jit(currentdat_full, currentdat_train, 'ntk'))\n",
        "      Ktrain_val = np.array(kernel_fn_jit(currentdat_val, currentdat_train, get))\n",
        "      alpha = np.linalg.solve(Ktrain_train_reg, currentdat_train_y)\n",
        "      shift = k*10000\n",
        "      currentpreds_ntk = np.matmul(Ktrain_full, alpha)\n",
        "      mypreds_full_ntk[shift+0:shift+10000] = currentpreds_ntk\n",
        "      currentpreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "      currentvalinds = valinds_list[k]\n",
        "      mypreds_val_ntk[currentvalinds] = currentpreds_val_ntk\n",
        "    end = time.time()\n",
        "    time_eval = end-start\n",
        "    errs_full_ntk =mypreds_full_ntk-mydat_y\n",
        "    errs_val_ntk =mypreds_val_ntk-mydat_val_y\n",
        "    currentind = 5*L+M\n",
        "\n",
        "    ntk_mat_cd_prod[currentind, 0] = np.mean(np.square(errs_val_ntk))/1e+6\n",
        "    ntk_mat_cd_prod[currentind, 1] = np.mean(np.square(errs_full_ntk))/1e+6\n",
        "    ntk_mat_cd_prod[currentind, 2] = np.mean(np.absolute(errs_full_ntk))/1e+3\n",
        "    ntk_mat_cd_prod[currentind, 3] = np.sqrt(ntk_mat_cd_prod[currentind, 0])/(mave/1e+3)\n",
        "    ntk_mat_cd_prod[currentind, 4] = ntk_mat_cd_prod[currentind, 1]/(mave/1e+3)\n",
        "    ntk_mat_cd_prod[currentind, 5] = np.mean(errs_full_ntk)/mave\n",
        "    ntk_mat_cd_prod[currentind, 6] = time_eval\n",
        "\n",
        "\n",
        "    #np.savetxt('NTK_mat2_CD_prod.csv', ntk_mat_cd_prod)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9QH4pPoxYO_"
      },
      "source": [
        "#Kernel regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZORORskxajn"
      },
      "outputs": [],
      "source": [
        "from statsmodels.nonparametric.kernel_regression import KernelReg\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.example_libraries import optimizers\n",
        "from jax import jit, grad, vmap\n",
        "from jax.lax import fori_loop\n",
        "import jax.numpy as jnp\n",
        "import functools\n",
        "from jax.numpy.linalg import solve\n",
        "import numpy as np\n",
        "from random import seed, sample\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from scipy import stats\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def unflatten(x):\n",
        "  return np.reshape(x, (len(x), 1))\n",
        "\n",
        "mydat_train_y_ntk = jnp.array(unflatten(mydat_train_y))\n",
        "mydat_train_ntk = jnp.array(mydat_train)\n",
        "mydat_val_y_ntk = jnp.array(unflatten(mydat_val_y))\n",
        "mydat_val_ntk = jnp.array(mydat_val)\n",
        "mydat_y_ntk = jnp.array(unflatten(mydat_y))\n",
        "mydat_array_ntk = jnp.array(mydat_array)\n",
        "\n",
        "\n",
        "def gauss_const(h):\n",
        "    \"\"\"\n",
        "    Returns the normalization constant for a gaussian\n",
        "    \"\"\"\n",
        "    return 1/(h*np.sqrt(np.pi*2))\n",
        "\n",
        "def makegauss_exp(ker_x, xi, h):\n",
        "    const = gauss_const(h)\n",
        "    den = h*h\n",
        "    diffs = jnp.sum(jnp.square((xi - ker_x)), axis=xi.ndim-1)\n",
        "    return const*jnp.exp(-0.5*diffs/den)\n",
        "\n",
        "makegauss_exp_jit = jit(makegauss_exp, static_argnames = 'h')\n",
        "\n",
        "def makegauss_exp_mat(ker_x, xi, h):\n",
        "    gauss_mat = np.zeros((len(ker_x), len(xi)))\n",
        "    for i in range(len(ker_x)):\n",
        "        gauss_mat[i] = (makegauss_exp_jit(jnp.array(ker_x[i]), xi, h))\n",
        "    return jnp.array(gauss_mat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "txvH78H56c3c",
        "outputId": "27b5f4f0-6df1-4e9f-e7d1-9cab07e34467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1\n",
            "0.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-b17b217a1f33>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mKtrain_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakegauss_exp_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_train_ntk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydat_train_ntk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mKtrain_train_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKtrain_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiag_reg\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mKtrain_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmakegauss_exp_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_train_ntk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydat_array_ntk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mKtrain_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmakegauss_exp_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_val_ntk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydat_train_ntk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydat_train_y_ntk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-3f10947b5b40>\u001b[0m in \u001b[0;36mmakegauss_exp_mat\u001b[0;34m(ker_x, xi, h)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mker_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mgauss_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmakegauss_exp_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mker_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgauss_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin, device)\u001b[0m\n\u001b[1;32m   3457\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3458\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected input type for array: {type(object)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3459\u001b[0;31m   out_array: Array = lax_internal._convert_element_type(\n\u001b[0m\u001b[1;32m   3460\u001b[0m       out, dtype, weak_type=weak_type, sharding=sharding)\n\u001b[1;32m   3461\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type, sharding)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moperand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m     return convert_element_type_p.bind(\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweak_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         sharding=sharding)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type_bind\u001b[0;34m(operand, new_dtype, weak_type, sharding)\u001b[0m\n\u001b[1;32m   2557\u001b[0m \u001b[0mconvert_element_type_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrimitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'convert_element_type'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_convert_element_type_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m   operand = core.Primitive.bind(convert_element_type_p, operand,\n\u001b[0m\u001b[1;32m   2560\u001b[0m                                 \u001b[0mnew_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweak_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweak_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m                                 sharding=sharding)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    427\u001b[0m     assert (not config.enable_checks.value or\n\u001b[1;32m    428\u001b[0m             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpop_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    937\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcall_impl_with_key_reuse_checks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_jit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_thread_local_state_disable_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_jit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_thread_local_state_disable_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "K = 50\n",
        "mae_val = np.zeros(K)\n",
        "mse_val = np.zeros(K)\n",
        "mae_test = np.zeros(K)\n",
        "mse_test = np.zeros(K)\n",
        "mse_train = np.zeros(K)\n",
        "perr = np.zeros(K)\n",
        "diag_reg = 1e-4\n",
        "\n",
        "\n",
        "for i in range(K):\n",
        "  random.seed(10)\n",
        "\n",
        "  h = (i+1)/10\n",
        "  print(h)\n",
        "\n",
        "\n",
        "  Ktrain_train = makegauss_exp_mat(mydat_train_ntk, mydat_train_ntk, h)\n",
        "  Ktrain_train_reg = Ktrain_train + jnp.trace(Ktrain_train)*diag_reg*jnp.identity(len(mydat_train_y))\n",
        "  Ktrain_full = jnp.transpose(jnp.array(makegauss_exp_mat(mydat_train_ntk, mydat_array_ntk, h)))\n",
        "  Ktrain_val = jnp.array(makegauss_exp_mat(mydat_val_ntk, mydat_train_ntk, h))\n",
        "  alpha = solve(Ktrain_train_reg, mydat_train_y_ntk)\n",
        "  mypreds_train_ntk = jnp.matmul(Ktrain_train, alpha)\n",
        "  mypreds_val_ntk = jnp.matmul(Ktrain_val, alpha)\n",
        "  mypreds_full_ntk = jnp.matmul(Ktrain_full, alpha)\n",
        "  errs_ntk_train = mypreds_train_ntk - mydat_train_y_ntk\n",
        "  errs_ntk_val =mypreds_val_ntk-mydat_val_y_ntk\n",
        "  errs_ntk_test = mypreds_full_ntk-mydat_y_ntk\n",
        "  mae_val[i] = np.mean(np.absolute(errs_ntk_val))\n",
        "  mse_val[i] = np.mean(np.square(errs_ntk_val))\n",
        "  mae_test[i] = np.mean(np.absolute(errs_ntk_test))\n",
        "  mse_test[i] = np.mean(np.square(errs_ntk_test))\n",
        "  perr[i] = np.mean(errs_ntk_test)/mydat_y.mean()\n",
        "  mse_train[i] = np.mean(np.square(errs_ntk_train))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVgC9IYWaU4U"
      },
      "source": [
        "# Greeks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_5XTT-9chfK"
      },
      "outputs": [],
      "source": [
        "def getdelta1(mydat_greeks, mydat_vals):\n",
        "    greeks = mydat_greeks[['Delta1']]\n",
        "    greeks = greeks.join(mydat_vals[['base:1_U', 'base:1_D']])\n",
        "    return greeks\n",
        "\n",
        "greeks_train = getdelta1(mydat_greeks_train, mydat_vals_train)\n",
        "greeks_val = getdelta1(mydat_greeks_val, mydat_vals_val)\n",
        "greeks = getdelta1(datgreeks2, datvals2)\n",
        "nmodels = len(greeks_train.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epRARr42eJZ3"
      },
      "source": [
        "##Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "collapsed": true,
        "id": "_apI9d-PeJZ4",
        "outputId": "cf43bcba-fb9b-4b75-867f-faa488f61765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-43ad0517bc71>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                                               \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrentmaxdepth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                               \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrentminsamplesleaf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                                               random_state=0).fit(mydat_train, current_train_y)\n\u001b[0m\u001b[1;32m     27\u001b[0m           \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m           \u001b[0mval_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_val_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    611\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \"\"\"\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         super()._fit(\n\u001b[0m\u001b[1;32m   1321\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    441\u001b[0m             )\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "nest = [500, 1000, 2000]\n",
        "maxdepth = [1, 3, 5]\n",
        "minsamplesleaf = [1, 5, 10]\n",
        "learningrate = [0.01, 0.1, 0.2]\n",
        "boost_mat = np.zeros((nmodels, 81, 5))\n",
        "\n",
        "\n",
        "for m in range(nmodels):\n",
        "  current_train_y =np.array(greeks_train.iloc[:, m])\n",
        "  current_val_y =np.array(greeks_val.iloc[:, m])\n",
        "  ncount = 0\n",
        "  for i in range(3):\n",
        "    currentnest = nest[i]\n",
        "    for j in range(3):\n",
        "      currentmaxdepth = maxdepth[j]\n",
        "      for k in range(3):\n",
        "        currentminsamplesleaf = minsamplesleaf[k]\n",
        "        for l in range(3):\n",
        "          print(ncount)\n",
        "          currentlearningrate = learningrate[l]\n",
        "          booster = GradientBoostingRegressor(n_estimators=currentnest,\n",
        "                                              learning_rate = currentlearningrate,\n",
        "                                              max_depth = currentmaxdepth,\n",
        "                                              min_samples_leaf=currentminsamplesleaf,\n",
        "                                              random_state=0).fit(mydat_train, current_train_y)\n",
        "          val_preds = booster.predict(mydat_val)\n",
        "          val_error = np.mean(np.square(val_preds.flatten() - current_val_y.flatten()))\n",
        "          boost_mat[m, ncount] = [currentnest, currentmaxdepth, currentminsamplesleaf, currentlearningrate, val_error]\n",
        "\n",
        "\n",
        "\n",
        "          ncount = ncount+1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "7HkGdWzleJZ5",
        "outputId": "781e9474-7a06-4ea3-e099-f4e5250636fe"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidParameterError",
          "evalue": "The 'max_depth' parameter of GradientBoostingRegressor must be an int in the range [1, inf) or None. Got 0 instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-3b42bdc8dbfd>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mmyminsamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboost_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mmylearning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboost_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmynest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmylearning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmymaxdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyminsamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mmypreds_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpartial_fit_and_fitted\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m                 \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             with config_context(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0maccepted\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \"\"\"\n\u001b[0;32m--> 638\u001b[0;31m         validate_parameter_constraints(\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameter_constraints\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 )\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise InvalidParameterError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;34mf\"The {param_name!r} parameter of {caller_name} must be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;34mf\" {constraints_str}. Got {param_val!r} instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidParameterError\u001b[0m: The 'max_depth' parameter of GradientBoostingRegressor must be an int in the range [1, inf) or None. Got 0 instead."
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mypreds_full_mat = np.zeros((nmodels, len(mydat_y)))\n",
        "mypreds_val_mat = np.zeros((nmodels, len(mydat_val_y)))\n",
        "\n",
        "\n",
        "for m in range(nmodels):\n",
        "  current_train_y =np.array(greeks_train.iloc[:, m])\n",
        "  current_val_y =np.array(greeks_val.iloc[:, m])\n",
        "  current_y = np.array(greeks.iloc[:, m])\n",
        "  mave = np.mean(np.abs(current_y))\n",
        "  minind = np.argmin(boost_mat[m, :, 4])\n",
        "  mynest = int(boost_mat[m, minind, 0])\n",
        "  mymaxdepth = int(boost_mat[m, minind, 1])\n",
        "  myminsamples = int(boost_mat[m, minind, 2])\n",
        "  mylearning = boost_mat[m, minind, 3]\n",
        "  booster = GradientBoostingRegressor(n_estimators=mynest, learning_rate = mylearning, max_depth = mymaxdepth, min_samples_leaf=myminsamples, random_state=0).fit(mydat_train, current_train_y)\n",
        "\n",
        "  mypreds_full = booster.predict(mydat_array)\n",
        "  mypreds_full_mat[m] = mypreds_full\n",
        "  mypreds_val = booster.predict(mydat_val)\n",
        "  mypreds_val_mat[m] = mypreds_val\n",
        "  mypreds = booster.predict(mydat_train)\n",
        "  errs = mypreds_full.flatten() - current_y.flatten()\n",
        "  errs_val = mypreds_val.flatten()-current_val_y.flatten()\n",
        "\n",
        "  print(np.mean(np.square(errs_val))/1000000)\n",
        "  print(np.mean(np.square(errs))/1000000)\n",
        "  print(np.mean(np.absolute(errs))/1000)\n",
        "  print(np.sqrt(np.mean(np.square(errs)))/mave*100)\n",
        "  print(np.mean(np.absolute(errs))/mave*100)\n",
        "  print(np.mean(errs)/mave)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ8wbjve3JxQ",
        "outputId": "5660f899-e6c1-48f4-81cd-e246b318fa75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "284726.6115197664\n",
            "326667.53008593566\n",
            "410.8486807788169\n",
            "24.545866414416402\n",
            "17.644415601051993\n",
            "-1.2525933655401502\n"
          ]
        }
      ],
      "source": [
        "#boost_mat.tofile('boost_mat_delta1.csv', sep = ',')\n",
        "delta1_preds_val = (mypreds_val_mat[1] - mypreds_val_mat[2])/0.02\n",
        "delta1_preds_full = (mypreds_full_mat[1] - mypreds_full_mat[2])/0.02\n",
        "mave = np.mean(np.abs(datgreeks2['Delta1']))\n",
        "errs_preds_val = delta1_preds_val - greeks_val['Delta1']\n",
        "errs_preds_full = delta1_preds_full - datgreeks2['Delta1']\n",
        "print(np.mean(np.square(errs_preds_val))/1000000)\n",
        "print(np.mean(np.square(errs_preds_full))/1000000)\n",
        "print(np.mean(np.abs(errs_preds_full))/1000)\n",
        "print(np.sqrt(np.mean(np.square(errs_preds_full)))/mave)\n",
        "print(np.mean(np.abs(errs_preds_full))/mave)\n",
        "print(np.mean(errs_preds_full)/np.mean(datgreeks2['Delta1']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT1r9x-7b1fp"
      },
      "source": [
        "## Finite NN (Best NN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "s_CTkEYtIg_C",
        "outputId": "b586ac49-5e72-4fd3-edd3-f512daab9298"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-85c95fd2810c>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;31m#m1.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mm1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmydat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmydat_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_val_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36menumerate_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    647\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m             \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distributed_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 for step in range(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 709\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    746\u001b[0m             self._flat_output_types)\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3478\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3479\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3480\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from keras.layers import Embedding\n",
        "from tensorflow.keras import backend as K\n",
        "nepochs = 20000\n",
        "mypreds_full_mat = np.zeros((nmodels, len(mydat_y)))\n",
        "mypreds_val_mat = np.zeros((nmodels, len(mydat_val_y)))\n",
        "for m in range(nmodels):\n",
        "  K.clear_session()\n",
        "  set_random_seed(1)\n",
        "\n",
        "\n",
        "  current_train_y =np.array(greeks_train.iloc[:, m])\n",
        "  current_val_y =np.array(greeks_val.iloc[:, m])\n",
        "  current_y = np.array(greeks.iloc[:, m])\n",
        "  k = 128\n",
        "  inputdata = Input((34, ))\n",
        "  mydense = Dense(k, activation = 'relu')(inputdata)\n",
        "  mydense = Dense(int(k/2), activation = 'relu')(mydense)\n",
        "  myval = Dense(1)(mydense)\n",
        "\n",
        "  inputs = inputdata\n",
        "  outputs = myval\n",
        "\n",
        "  m1 = KerasModel(inputs = inputs, outputs=outputs)\n",
        "  m1.compile(optimizer=Adam(), loss='mse')\n",
        "  #m1.summary()\n",
        "  callback = EarlyStopping(monitor='val_loss', patience=50)\n",
        "  m1.fit(x=mydat_train, y=current_train_y, epochs = nepochs, validation_data = (mydat_val, current_val_y), callbacks = [callback], verbose = 0, batch_size = n)\n",
        "\n",
        "\n",
        "  mave = np.mean(np.abs(current_y))\n",
        "  mypreds_train = m1.predict(x=mydat_train).flatten()\n",
        "  errs_train = mypreds_train - current_train_y\n",
        "  mypreds_val  = m1.predict(x=mydat_val).flatten()\n",
        "  errs_val = mypreds_val - current_val_y\n",
        "  mse_val_1 = np.mean(np.square(errs_val)/1000000)\n",
        "  mypreds_full  = m1.predict(x=mydat_array).flatten()\n",
        "  errs = mypreds_full - current_y\n",
        "\n",
        "  K.clear_session()\n",
        "  set_random_seed(1)\n",
        "\n",
        "\n",
        "\n",
        "  input_product_type = Input(shape=(1,))\n",
        "  output_product_type = Embedding(19, 5, name='product_type_embedding')(input_product_type)\n",
        "  output_product_type = Reshape(target_shape=(5,))(output_product_type)\n",
        "  input_mat = Input(shape = (15, ))\n",
        "  output_mat = input_mat\n",
        "  input_list = [input_mat, input_product_type]\n",
        "  outputs =[output_mat, output_product_type]\n",
        "  outputs = Concatenate()(outputs)\n",
        "  k = 128\n",
        "\n",
        "  outputs = Dense(k, activation = 'relu')(outputs)\n",
        "  outputs = Dense(int(k/2), activation = 'relu')(outputs)\n",
        "  outputs = Dense(1)(outputs)\n",
        "\n",
        "  m2 = KerasModel(inputs = input_list, outputs=outputs)\n",
        "  m2.compile(optimizer=Adam(), loss='mse')\n",
        "  #m2.summary()\n",
        "  callback = EarlyStopping(monitor='val_loss', patience=50)\n",
        "\n",
        "\n",
        "  def split_features(dat_emb):\n",
        "    product_type = np.array(dat_emb['productType']).astype('float')\n",
        "    dat_emb = np.array(dat_emb.loc[:, dat_emb.columns != 'productType'].values)\n",
        "    return [dat_emb, product_type]\n",
        "\n",
        "\n",
        "  m2.fit(x=split_features(mydat_train_emb), y=current_train_y, epochs = nepochs, validation_data = (split_features(mydat_val_emb), current_val_y), callbacks = [callback], verbose = 0, batch_size = n)\n",
        "\n",
        "  mypreds2_val = m2.predict(x=split_features(mydat_val_emb)).flatten()\n",
        "  mypreds2_train = m2.predict(x=split_features(mydat_train_emb)).flatten()\n",
        "  errs2_train = mypreds2_train - current_train_y\n",
        "  errs2_val = mypreds2_val - current_val_y\n",
        "  mse_val_2 = np.mean(np.square(errs2_val)/1000000)\n",
        "  mypreds2_full  = m2.predict(x=split_features(mydat_emb)).flatten()\n",
        "  errs2 = mypreds2_full - current_y\n",
        "\n",
        "  if mse_val_1 > mse_val_2:\n",
        "    errs_best = errs2\n",
        "    mypreds_full_mat[m] = mypreds2_full\n",
        "  else:\n",
        "    errs_best = errs\n",
        "    mypreds_full_mat[m] = mypreds_full\n",
        "\n",
        "  print(np.min([mse_val_1, mse_val_2]))\n",
        "  print(np.mean(np.square(errs_best))/1000000)\n",
        "  print(np.mean(np.abs(errs_best))/1000)\n",
        "  print(np.sqrt(np.mean(np.square(errs_best)))/mave)\n",
        "  print(np.mean(np.abs(errs_best))/mave)\n",
        "  print(np.mean(errs_best)/np.mean(current_y))\n",
        "\n",
        "delta1_preds_val = (mypreds_val_mat[1] - mypreds_val_mat[2])/0.02\n",
        "delta1_preds_full = (mypreds_full_mat[1] - mypreds_full_mat[2])/0.02\n",
        "mave = np.mean(np.abs(datgreeks2['Delta1']))\n",
        "errs_preds_val = delta1_preds_val - greeks_val['Delta1']\n",
        "errs_preds_full = delta1_preds_full - datgreeks2['Delta1']\n",
        "print(np.mean(np.square(errs_preds_val))/1000000)\n",
        "print(np.mean(np.square(errs_preds_full))/1000000)\n",
        "print(np.mean(np.abs(errs_preds_full))/1000)\n",
        "print(np.sqrt(np.mean(np.square(errs_preds_full)))/mave)\n",
        "print(np.mean(np.abs(errs_preds_full))/mave)\n",
        "print(np.mean(errs_preds_full)/np.mean(datgreeks2['Delta1']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTVOu-T_bxwF"
      },
      "source": [
        "## NTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kS9lAILwphMx"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.example_libraries import optimizers\n",
        "from jax import jit, grad, vmap\n",
        "import jax.numpy as jnp\n",
        "import functools\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import predict, stax\n",
        "import numpy as np\n",
        "from random import seed, sample\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from scipy import stats\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def unflatten(x):\n",
        "  return np.reshape(x, (len(x), 1))\n",
        "\n",
        "#greeks_train_ntk = jnp.array(greeks_train)\n",
        "mydat_train_ntk = jnp.array(mydat_train)\n",
        "#greeks_val_ntk = jnp.array(greeks_val)\n",
        "mydat_val_ntk = jnp.array(mydat_val)\n",
        "#greeks_ntk = jnp.array(greeks)\n",
        "mydat_array_ntk = jnp.array(mydat_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "dpTSPkuNqBTE",
        "outputId": "9f7eb254-a12e-4467-dcac-46046fdb32cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1\n",
            "0.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-885775df4b6c>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmypreds_train_ntk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmydat_train_ntk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_cov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mmypreds_val_ntk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmydat_val_ntk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_cov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmypreds_full_ntk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmydat_array_ntk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_cov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0merrs_ntk_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmypreds_train_ntk\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurrent_train_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0merrs_ntk_val\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mmypreds_val_ntk\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcurrent_val_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/utils/utils.py\u001b[0m in \u001b[0;36mh\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/utils/utils.py\u001b[0m in \u001b[0;36mgetter_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m                                                           len(args)])\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m       \u001b[0mfn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcanonicalized_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0;34m@\u001b[0m\u001b[0mnt_tree_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/predict.py\u001b[0m in \u001b[0;36mpredict_fn\u001b[0;34m(t, x_test, get, compute_cov, **kernel_fn_test_test_kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m     \u001b[0;31m# train-train, test-train, test-test.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m     k_dd, k_td, nngp_tt = get_kernels(get, x_test, compute_cov,\n\u001b[0m\u001b[1;32m    987\u001b[0m                                       **kernel_fn_test_test_kwargs)\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/neural_tangents/_src/predict.py\u001b[0m in \u001b[0;36mget_kernels\u001b[0;34m(get, x_test, compute_cov, **kernel_fn_test_test_kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mkwargs_tt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv_tt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m       \u001b[0mk_td\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_td\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcompute_cov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n\u001b[0m\u001b[1;32m    333\u001b[0m         fun, jit_info, *args, **kwargs)\n\u001b[1;32m    334\u001b[0m     \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_most_recent_pjit_call_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mout_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpjit_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mpxla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeviceAssignmentMismatchError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mfails\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2737\u001b[0m     top_trace = (top_trace if not axis_main or axis_main.level < top_trace.level\n\u001b[1;32m   2738\u001b[0m                  else axis_main.with_cur_sublevel())\n\u001b[0;32m-> 2739\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpop_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    937\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcall_impl_with_key_reuse_checks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1728\u001b[0m   has_explicit_sharding = _pjit_explicit_sharding(\n\u001b[1;32m   1729\u001b[0m       in_shardings, out_shardings, None, None)\n\u001b[0;32m-> 1730\u001b[0;31m   return xc._xla.pjit(\n\u001b[0m\u001b[1;32m   1731\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_impl_cache_miss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonated_argnums\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m       \u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcall_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                     donated_invars, name, keep_unused, inline):\n\u001b[1;32m   1711\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_impl_cache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m     out_flat, compiled = _pjit_call_impl_python(\n\u001b[0m\u001b[1;32m   1713\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_shardings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_shardings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[0mout_shardings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_shardings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_layouts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_layouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1664\u001b[0m                           (\"fingerprint\", fingerprint))\n\u001b[1;32m   1665\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsafe_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mFloatingPointError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_nans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_infs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m  \u001b[0;31m# compiled_fun can only raise in this case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1246\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_token_bufs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_token_bufs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharded_runtime_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxla_executable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_sharded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_bufs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_check_special\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "K = 150\n",
        "mae_val = np.zeros((nmodels, K))\n",
        "mse_val = np.zeros((nmodels, K))\n",
        "mae_test = np.zeros((nmodels, K))\n",
        "mse_test = np.zeros((nmodels, K))\n",
        "#pmae_test = np.zeros((nmodels, K))\n",
        "#prmse_test = np.zeros((nmodels, K))\n",
        "mse_train = np.zeros((nmodels, K))\n",
        "perr = np.zeros((nmodels, K))\n",
        "mypreds_full_mat = np.zeros((nmodels, len(mydat_y)))\n",
        "mypreds_val_mat = np.zeros((nmodels, len(mydat_val_y)))\n",
        "b_std = 0.1\n",
        "diag_reg = 1e-4\n",
        "myactivation = stax.Gelu\n",
        "\n",
        "for m in [0]:\n",
        "  current_train_y = jnp.array(unflatten(greeks_train.iloc[:, m]))\n",
        "  current_val_y = jnp.array(unflatten(greeks_val.iloc[:, m]))\n",
        "  current_y = jnp.array(unflatten(greeks.iloc[:, m]))\n",
        "  mave = np.mean(np.abs(current_y))\n",
        "  for i in range(K):\n",
        "    random.seed(10)\n",
        "\n",
        "    W_std = (i+1)/10\n",
        "    print(W_std)\n",
        "\n",
        "    init_fn, apply_fn, kernel_fn = stax.serial(\n",
        "      stax.Dense(128, W_std=W_std, b_std=b_std), myactivation(),\n",
        "      stax.Dense(1, W_std=W_std, b_std=b_std)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    kernel_fn_jit = jit(kernel_fn, static_argnames='get')\n",
        "    get = 'ntk'\n",
        "\n",
        "    predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn_jit, mydat_train_ntk, current_train_y, diag_reg = diag_reg)\n",
        "    mypreds_train_ntk = predict_fn(x_test=mydat_train_ntk, get=get, compute_cov=False).flatten()\n",
        "    mypreds_val_ntk = predict_fn(x_test=mydat_val_ntk, get=get, compute_cov=False).flatten()\n",
        "    mypreds_full_ntk = predict_fn(x_test=mydat_array_ntk, get=get, compute_cov=False).flatten()\n",
        "    errs_ntk_train = mypreds_train_ntk - current_train_y\n",
        "    errs_ntk_val =mypreds_val_ntk-current_val_y.flatten()\n",
        "    errs_ntk_test = mypreds_full_ntk-current_y.flatten()\n",
        "\n",
        "    mae_val[m, i] = np.mean(np.absolute(errs_ntk_val))\n",
        "    mse_val[m, i] = np.mean(np.square(errs_ntk_val))\n",
        "    mae_test[m, i] = np.mean(np.absolute(errs_ntk_test))\n",
        "    mse_test[m, i] = np.mean(np.square(errs_ntk_test))\n",
        "    #pmae_test[m, i] = np.mean(np.absolute(errs_ntk_test))/mave\n",
        "    #prmse_test[m, i] = np.sqrt(np.mean(np.square(errs_ntk_test)))/mave\n",
        "    perr[m, i] = np.mean(errs_ntk_test)/current_y.mean()\n",
        "    mse_train[m, i] = np.mean(np.square(errs_ntk_train))\n",
        "\n",
        "  random.seed(10)\n",
        "  current_mse_val = mse_val[m]\n",
        "  W_std = (sum(np.isnan(current_mse_val)) + np.argmin(current_mse_val[~np.isnan(current_mse_val)]))/10+0.1\n",
        "  b_std = 0.1\n",
        "\n",
        "  init_fn, apply_fn, kernel_fn = stax.serial(\n",
        "      stax.Dense(128, W_std=W_std, b_std=b_std), myactivation(),\n",
        "      stax.Dense(1, W_std=W_std, b_std=b_std)\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  start = time.time()\n",
        "  kernel_fn_jit = jit(kernel_fn, static_argnames='get')\n",
        "  get = 'ntk'\n",
        "  predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn_jit, mydat_train_ntk, current_train_y, diag_reg = diag_reg)\n",
        "  mypreds_train_ntk = predict_fn(x_test=mydat_train_ntk, get=get, compute_cov=False).flatten()\n",
        "  mypreds_val_ntk = predict_fn(x_test=mydat_val_ntk, get=get, compute_cov=False).flatten()\n",
        "  mypreds_full_ntk = predict_fn(x_test=mydat_array_ntk, get=get, compute_cov=False).flatten()\n",
        "  end = time.time()\n",
        "  mypreds_full_mat[:,m] = mypreds_full_ntk[m]\n",
        "  mypreds_val_mat[:,m] = mypreds_val_ntk[m]\n",
        "\n",
        "  print(np.mean(np.square(mypreds_val_ntk-current_val_y.flatten())))\n",
        "  errs_ntk = current_y.flatten() - mypreds_full_ntk\n",
        "  print(np.mean(np.square(errs_ntk)))\n",
        "  print(np.mean(np.absolute(errs_ntk)))\n",
        "  print(np.sqrt(np.mean(np.square(errs_ntk)))/mave*100)\n",
        "  print(np.mean(np.absolute(errs_ntk))/mave*100)\n",
        "  print(np.mean(errs_ntk)/current_y.mean()*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2I4J6D2cFvs"
      },
      "source": [
        "## NTK CD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "KGQYJl1EaTyA",
        "outputId": "d5a9bee0-4341-44d3-a82e-d60647ce3523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x79efb82012d0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_std\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-8b675215d177>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m           \u001b[0mmytrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m           \u001b[0mKtrain_train_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKtrain_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiag_reg\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmytrace\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m           \u001b[0mKtrain_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_fn_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m           \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_train_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m           \u001b[0mmypreds_val_ntk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKtrain_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mapi_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n\u001b[0m\u001b[1;32m    333\u001b[0m         fun, jit_info, *args, **kwargs)\n\u001b[1;32m    334\u001b[0m     \u001b[0mexecutable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_most_recent_pjit_call_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mout_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpjit_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mpxla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeviceAssignmentMismatchError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mfails\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2737\u001b[0m     top_trace = (top_trace if not axis_main or axis_main.level < top_trace.level\n\u001b[1;32m   2738\u001b[0m                  else axis_main.with_cur_sublevel())\n\u001b[0;32m-> 2739\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpop_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    937\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcall_impl_with_key_reuse_checks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 939\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1728\u001b[0m   has_explicit_sharding = _pjit_explicit_sharding(\n\u001b[1;32m   1729\u001b[0m       in_shardings, out_shardings, None, None)\n\u001b[0;32m-> 1730\u001b[0;31m   return xc._xla.pjit(\n\u001b[0m\u001b[1;32m   1731\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_impl_cache_miss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonated_argnums\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m       \u001b[0mtree_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcall_impl_cache_miss\u001b[0;34m(*args_, **kwargs_)\u001b[0m\n\u001b[1;32m   1710\u001b[0m                     donated_invars, name, keep_unused, inline):\n\u001b[1;32m   1711\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_impl_cache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1712\u001b[0;31m     out_flat, compiled = _pjit_call_impl_python(\n\u001b[0m\u001b[1;32m   1713\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_shardings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_shardings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1714\u001b[0m         \u001b[0mout_shardings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_shardings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_layouts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_layouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1640\u001b[0m       \u001b[0mlowering_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoweringParameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m       \u001b[0mpgle_profiler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpgle_profiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m   ).compile(compile_options)\n\u001b[0m\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m   \u001b[0m_most_recent_pjit_call_executable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweak_key_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiler_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMeshExecutable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompiler_options\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2295\u001b[0;31m       executable = UnloadedMeshExecutable.from_hlo(\n\u001b[0m\u001b[1;32m   2296\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2297\u001b[0m           compiler_options=compiler_options)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36mfrom_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2805\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2807\u001b[0;31m     xla_executable = _cached_compilation(\n\u001b[0m\u001b[1;32m   2808\u001b[0m         \u001b[0mhlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspmd_lowering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m         \u001b[0mtuple_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_spmd_lowering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_prop_to_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/interpreters/pxla.py\u001b[0m in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_keys, compiler_options_values, pgle_profiler)\u001b[0m\n\u001b[1;32m   2619\u001b[0m       \u001b[0;34m\"Finished XLA compilation of {fun_name} in {elapsed_time:.9f} sec\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m       fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2621\u001b[0;31m     xla_executable = compiler.compile_or_get_cached(\n\u001b[0m\u001b[1;32m   2622\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m         pgle_profiler)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[0m\n\u001b[1;32m    397\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0mlog_persistent_cache_miss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     return _compile_and_write_cache(\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    625\u001b[0m ) -> xc.LoadedExecutable:\n\u001b[1;32m    626\u001b[0m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m   executable = backend_compile(\n\u001b[0m\u001b[1;32m    628\u001b[0m       \u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomputation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    265\u001b[0m   \u001b[0;31m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;31m# to take in `host_callbacks`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilt_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m def compile_or_get_cached(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "mydat_array = mydat.values\n",
        "def makenpfloat(vec):\n",
        "  return np.array(vec, dtype = np.float32)\n",
        "\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "from jax.example_libraries import optimizers\n",
        "from jax import jit, grad, vmap\n",
        "import jax.numpy as jnp\n",
        "import functools\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import predict, stax\n",
        "import numpy as np\n",
        "from random import seed, sample\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from scipy import stats\n",
        "import time\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "def unflatten(x):\n",
        "  return np.reshape(x, (len(x), 1))\n",
        "\n",
        "activations = [stax.Relu, stax.Erf, stax.Gelu]\n",
        "K = 150\n",
        "B = 21\n",
        "R = 81\n",
        "W_vec = np.array(range(K))/10+0.1\n",
        "B_vec = np.array(range(B))/10\n",
        "R_vec = np.concatenate(([0], 10**((np.array(range(R))-(R-1))/10)))\n",
        "seeds = [10, 20, 30, 40, 50]\n",
        "nreps = len(seeds)\n",
        "ntk_mat_cd = np.zeros((nmodels, nreps*len(activations), 10))\n",
        "mypreds_full_mat = np.zeros((nmodels, nreps*len(activations), len(mydat_array)))\n",
        "mypreds_val_mat = np.zeros((nmodels, nreps*len(activations), len(mydat_val)))\n",
        "for L in range(len(activations)):\n",
        "  kernel_fn_list = [[] for _ in range(150)]\n",
        "  activation = activations[L]\n",
        "  for i in range(K):\n",
        "    W_std = (i+1)/10\n",
        "    for j in range(B):\n",
        "      b_std = j/10\n",
        "      init_fn, apply_fn, kernel_fn = stax.serial(\n",
        "        stax.Dense(128, W_std=W_std, b_std=b_std), activation(),\n",
        "        stax.Dense(1, W_std=W_std, b_std=b_std)\n",
        "      )\n",
        "      kernel_fn_jit = jit(kernel_fn, static_argnames='get')\n",
        "      kernel_fn_list[i].append(kernel_fn_jit)\n",
        "  for M in range(nreps):\n",
        "\n",
        "    current_seed = seeds[M]\n",
        "\n",
        "    mydat_train, mydat_val, mydat_train_emb, mydat_val_emb, mydat_train_y, mydat_val_y  = train_test_split(\n",
        "        mydat_array, mydat_emb, mydat_y, test_size=340/190000,train_size=680/190000,\n",
        "        stratify = mydat_emb['productType'], random_state =current_seed)\n",
        "\n",
        "    mydat_train = makenpfloat(mydat_train)\n",
        "    mydat_val = makenpfloat(mydat_val)\n",
        "    mydat_greeks_train = datgreeks2.loc[mydat_train_emb.index]\n",
        "    mydat_greeks_val = datgreeks2.loc[mydat_val_emb.index]\n",
        "    mydat_vals_train = datvals2.loc[mydat_train_emb.index]\n",
        "    mydat_vals_val = datvals2.loc[mydat_val_emb.index]\n",
        "    for m in [0, 1, 2]:\n",
        "      current_train_y = greeks_train.iloc[:, m]\n",
        "      current_val_y = greeks_val.iloc[:, m]\n",
        "      current_y = greeks.iloc[:, m]\n",
        "      mave = np.mean(np.abs(current_y))\n",
        "\n",
        "      mse_val_w = np.zeros(K)\n",
        "      mse_val_b = np.zeros(B)\n",
        "      mse_val_r = np.zeros(R+1)\n",
        "      b_std = 0.1\n",
        "      b_ind = int(10*b_std)\n",
        "      W_std = 3\n",
        "      diag_reg = 1e-4\n",
        "      tol = 1e-4\n",
        "      rel_improvement = 1\n",
        "      error_old = 1e+10\n",
        "      while rel_improvement > tol:\n",
        "        print(\"W_std\")\n",
        "        for i in range(K):\n",
        "          print(i)\n",
        "          random.seed(10)\n",
        "          currentW_std = (i+1)/10\n",
        "          W_ind = int(10*(currentW_std -0.1))\n",
        "          kernel_fn_jit = kernel_fn_list[i][b_ind]\n",
        "          get = 'ntk'\n",
        "          Ktrain_train = np.array(kernel_fn_jit(mydat_train, None, get))\n",
        "          mytrace = np.mean(np.trace(Ktrain_train))\n",
        "          Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(current_train_y))\n",
        "          Ktrain_val = np.array(kernel_fn_jit(mydat_val, mydat_train, get))\n",
        "          alpha = np.linalg.solve(Ktrain_train_reg, current_train_y)\n",
        "          mypreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "          errs_val_ntk =mypreds_val_ntk-current_val_y\n",
        "          mse_val_w[i] = np.mean(np.square(errs_val_ntk))\n",
        "        W_ind = np.argmin(mse_val_w)\n",
        "        W_std = W_ind/10+0.1\n",
        "        if error_old == 1e+10:\n",
        "          error_old = np.min(mse_val_w)\n",
        "        kernel_fn_jit = kernel_fn_list[W_ind][b_ind]\n",
        "        Ktrain_train = np.array(kernel_fn_jit(mydat_train, None, get))\n",
        "        Ktrain_val = np.array(kernel_fn_jit(mydat_val, mydat_train, get))\n",
        "        mytrace = np.mean(np.trace(Ktrain_train))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(\"Diag\")\n",
        "        for i in range(R+1):\n",
        "          diag_reg = R_vec[i]\n",
        "          print(i)\n",
        "          Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(current_train_y))\n",
        "          alpha = np.linalg.solve(Ktrain_train_reg, current_train_y)\n",
        "          mypreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "          errs_val_ntk = mypreds_val_ntk - current_val_y\n",
        "          mse_val_r[i] = np.mean(np.square(errs_val_ntk))\n",
        "        diag_reg = R_vec[np.argmin(mse_val_r)]\n",
        "        print(\"b_std\")\n",
        "        for i in range(B):\n",
        "          print(i)\n",
        "          kernel_fn_jit = kernel_fn_list[W_ind][i]\n",
        "          Ktrain_train = np.array(kernel_fn_jit(mydat_train, None, get))\n",
        "          mytrace = np.mean(np.trace(Ktrain_train))\n",
        "          Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(current_train_y))\n",
        "          Ktrain_val = np.array(kernel_fn_jit(mydat_val, mydat_train, get))\n",
        "          alpha = np.linalg.solve(Ktrain_train_reg, current_train_y)\n",
        "          mypreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "          errs_val_ntk = mypreds_val_ntk - current_val_y\n",
        "          mse_val_b[i] = np.mean(np.square(errs_val_ntk))\n",
        "        error_new = np.min(mse_val_b)\n",
        "        b_ind = np.argmin(mse_val_b)\n",
        "        rel_improvement = np.log(error_old)-np.log(error_new)\n",
        "        error_old = error_new\n",
        "    W_std = W_ind/10 + 0.1\n",
        "    b_std = b_ind/10\n",
        "    kernel_fn_jit = kernel_fn_list[W_ind][b_ind]\n",
        "    start = time.time()\n",
        "    Ktrain_train = np.array(kernel_fn_jit(mydat_train, mydat_train, get))\n",
        "    mytrace = np.mean(np.trace(Ktrain_train))\n",
        "    Ktrain_train_reg = Ktrain_train + diag_reg*mytrace*np.identity(len(current_train_y))\n",
        "    Ktrain_full = np.array(kernel_fn_jit(mydat_array, mydat_train, 'ntk'))\n",
        "    Ktrain_val = np.array(kernel_fn_jit(mydat_val, mydat_train, 'ntk'))\n",
        "    alpha = np.linalg.solve(Ktrain_train_reg, current_train_y)\n",
        "    mypreds_full_ntk = np.matmul(Ktrain_full, alpha)\n",
        "    mypreds_val_ntk = np.matmul(Ktrain_val, alpha)\n",
        "    end = time.time()\n",
        "    time_eval = start-end\n",
        "    errs_full_ntk =mypreds_full_ntk-current_y\n",
        "    errs_val_ntk = mypreds_val_ntk - current_val_y\n",
        "    currentind = nreps*L + M\n",
        "    mse_val_ntk = np.mean(np.square(errs_val_ntk))\n",
        "    mse_full_ntk = np.mean(np.square(errs_full_ntk))\n",
        "    mae_full_ntk = np.mean(np.abs(errs_full_ntk))\n",
        "\n",
        "    ntk_mat_cd[m, currentind, 0] = mse_full_ntk/1e+6\n",
        "    ntk_mat_cd[m, currentind, 1] = mae_full_ntk/1e+3\n",
        "    ntk_mat_cd[m, currentind, 2] = np.sqrt(mse_full_ntk)/(mave/1e+3)\n",
        "    ntk_mat_cd[m, currentind, 3] = mae_full_ntk/(mave/1e+3)\n",
        "    ntk_mat_cd[m, currentind, 4] = np.mean(errs_full_ntk)/current_y.mean()\n",
        "    ntk_mat_cd[m, currentind, 5] = time_eval\n",
        "    ntk_mat_cd[m, currentind, 6] = W_std\n",
        "    ntk_mat_cd[m, currentind, 7] = diag_reg\n",
        "    ntk_mat_cd[m, currentind, 8] = b_std\n",
        "    ntk_mat_cd[m, currentind, 9] = mse_val_ntk/1e+6\n",
        "    mypreds_full_mat[m, currentind, :] = mypreds_full_ntk\n",
        "    mypreds_val_mat[m, currentind, :] = mypreds_val_ntk\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxlom6dD7qkr",
        "outputId": "4b51b523-37e8-4960-dd23-5501cfc3ab6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "69452094.38379914\n",
            "89833967.71040913\n",
            "5171.485931756038\n",
            "407.0480445381255\n",
            "222.09599622402342\n",
            "217.2921123792474\n",
            "70036630.93689649\n",
            "94976884.31274377\n",
            "5169.805694645452\n",
            "418.5374665901622\n",
            "222.0238363187481\n",
            "219.07093385917625\n",
            "70515048.95992176\n",
            "95533022.77511007\n",
            "5172.865022515053\n",
            "419.76105398938375\n",
            "222.1552230188071\n",
            "219.5342420844305\n"
          ]
        }
      ],
      "source": [
        "delta1_preds_val = (mypreds_val_mat[1, :, :] - mypreds_val_mat[2, :, :])/0.02\n",
        "delta1_preds_full = (mypreds_full_mat[1, :, :] - mypreds_full_mat[2, :, :])/0.02\n",
        "mave = np.mean(np.abs(datgreeks2['Delta1']))\n",
        "errs_preds_val = np.zeros(delta1_preds_val.shape)\n",
        "errs_preds_full = np.zeros(delta1_preds_full.shape)\n",
        "for i in range(len(delta1_preds_val)):\n",
        "  errs_preds_val[i] = delta1_preds_val[i] - greeks_val['Delta1']\n",
        "  errs_preds_full[i] = delta1_preds_full[i] - datgreeks2['Delta1']\n",
        "  print(np.mean(np.square(errs_preds_val[i]))/1000000)\n",
        "  print(np.mean(np.square(errs_preds_full[i]))/1000000)\n",
        "  print(np.mean(np.abs(errs_preds_full[i]))/1000)\n",
        "  print(np.sqrt(np.mean(np.square(errs_preds_full[i])))/mave)\n",
        "  print(np.mean(np.abs(errs_preds_full[i]))/mave)\n",
        "  print(np.mean(errs_preds_full[i])/np.mean(datgreeks2['Delta1']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtgdtkYH_qqC"
      },
      "source": [
        "## Hejazi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnBxLK_g_w6U",
        "outputId": "368f6ec9-0196-4482-af9c-0bf44d66346d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "340 340 340\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mydat_trainr, mydat_rep, mydat_trainr_y, mydat_rep_y = train_test_split(mydat_train_emb, mydat_train_y, test_size = 0.5, train_size = 0.5, random_state = random_state)\n",
        "print(len(mydat_rep), len(mydat_trainr), len(mydat_val))\n",
        "\n",
        "mydat_greeks_trainr = datgreeks2.loc[mydat_trainr.index]\n",
        "mydat_greeks_rep = datgreeks2.loc[mydat_rep.index]\n",
        "delta_trainr = np.array(mydat_greeks_trainr['Delta1'])\n",
        "delta_rep = np.array(mydat_greeks_rep['Delta1'])\n",
        "delta_val = np.array(mydat_greeks_val['Delta1'])\n",
        "delta_full = np.array(greeks['Delta1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U0hyiD9BJlZ"
      },
      "outputs": [],
      "source": [
        "#first, we need a function which evaluates the \"distance vector\"\n",
        "#there are three components:\n",
        "\n",
        "#first, distance on each categorical variable is a Hamming distance\n",
        "def getcatdist(catvec2, catvec1):\n",
        "  return 1*np.array(catvec2 != catvec1)\n",
        "\n",
        "#second, positive distance on numerics: max(y2-y1, 0)\n",
        "#third, (absolute) negative distance on numerics: max(y1-y2, 0)\n",
        "\n",
        "def getmaxdist(numvec2, numvec1):\n",
        "  numvec2 = np.array(numvec2)\n",
        "  numvec1 = np.array(numvec1)\n",
        "  return 1*np.maximum(numvec2-numvec1, 0)\n",
        "\n",
        "def getmindist(numvec2, numvec1):\n",
        "  numvec2 = np.array(numvec2)\n",
        "  numvec1 = np.array(numvec1)\n",
        "  return 1*np.maximum(numvec1-numvec2, 0)\n",
        "\n",
        "#if we concatenate catdist, maxdist and mindist, we get a vector of length 30\n",
        "\n",
        "#this creates a list of n_rep matrices of shape (n_pred x 30)\n",
        "#this allows defining in Tensorflow the list of inputs [dat1, dat2, ..., datn_rep] where each dat is a dataset with n_pred rows\n",
        "def getdist (list_rep, list_pred):\n",
        "  datcat_rep = list_rep[0]\n",
        "  datnum_rep = list_rep[1]\n",
        "  datcat_pred = list_pred[0]\n",
        "  datnum_pred = list_pred[1]\n",
        "  distlist = []\n",
        "  for i in range(len(list_rep[0])):\n",
        "    currentcat = getcatdist(datcat_rep.iloc[i], datcat_pred)\n",
        "    currentmax = getmaxdist(datnum_rep.iloc[i], datnum_pred)\n",
        "    currentmin = getmindist(datnum_rep.iloc[i], datnum_pred)\n",
        "    currentdist = np.concatenate([currentcat, currentmax, currentmin], axis=1)\n",
        "    distlist.append(currentdist)\n",
        "  return distlist\n",
        "\n",
        "mydat_trainr_cat = mydat_trainr[['gender', 'productType']]\n",
        "mydat_trainr_num = mydat_trainr.loc[:, ~mydat_trainr.columns.isin(['gender', 'productType'])]\n",
        "mydat_trainr_list = [mydat_trainr_cat, mydat_trainr_num]\n",
        "mydat_rep_cat = mydat_rep[['gender', 'productType']]\n",
        "mydat_rep_num = mydat_rep.loc[:, ~mydat_rep.columns.isin(['gender', 'productType'])]\n",
        "mydat_rep_list = [mydat_rep_cat, mydat_rep_num]\n",
        "mydat_val_cat = mydat_val_emb[['gender', 'productType']]\n",
        "mydat_val_num = mydat_val_emb.loc[:, ~mydat_val_emb.columns.isin(['gender', 'productType'])]\n",
        "mydat_val_list = [mydat_val_cat, mydat_val_num]\n",
        "mydat_cat = mydat_emb[['gender', 'productType']]\n",
        "mydat_num = mydat_emb.loc[:, ~mydat_emb.columns.isin(['gender', 'productType'])]\n",
        "mydat_list = [mydat_cat, mydat_num]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "qmbM0_7GBWmA",
        "outputId": "6286c227-18a7-47c5-acc0-23145c6d6f69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-b8fa8a97b479>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtrainr_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_rep_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydat_trainr_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mval_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydat_rep_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydat_val_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainr_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelta_trainr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       return api.converted_call(\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_autograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Permanently allowed: %s: AutoGraph artifact'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m   \u001b[0;31m# If this is a partial, unwrap it and redo all the checks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mone_step_on_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;34m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             outputs = self.distribute_strategy.run(\n\u001b[0m\u001b[1;32m    122\u001b[0m                 \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1671\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1672\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1673\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3261\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3263\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3265\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   4059\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4060\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4061\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4063\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       return api.converted_call(\n\u001b[0m\u001b[1;32m     42\u001b[0m           \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_autograph_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Permanently allowed: %s: AutoGraph artifact'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m   \u001b[0;31m# If this is a partial, unwrap it and redo all the checks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mone_step_on_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mone_step_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;34m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The model does not have any trainable weights.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;31m# Return iterations for compat with tf.keras.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_variables_are_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/adam.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             self._momentums.append(\n\u001b[0;32m---> 97\u001b[0;31m                 self.add_variable_from_reference(\n\u001b[0m\u001b[1;32m     98\u001b[0m                     \u001b[0mreference_variable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"momentum\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/optimizer.py\u001b[0m in \u001b[0;36madd_variable_from_reference\u001b[0;34m(self, reference_variable, name, initializer)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mcolocate_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         ):\n\u001b[0;32m---> 36\u001b[0;31m             return super().add_variable_from_reference(\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mreference_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36madd_variable_from_reference\u001b[0;34m(self, reference_variable, name, initializer)\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             )\n\u001b[0;32m--> 218\u001b[0;31m         return self.add_variable(\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreference_variable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36madd_variable\u001b[0;34m(self, shape, initializer, dtype, aggregation, name)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             variable = backend.Variable(\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initializer, shape, dtype, trainable, autocast, aggregation, name)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         self._value = tf.Variable(\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_variable_call\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m       \u001b[0mvariable_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvariable_call\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvariable_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_call\u001b[0;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape, experimental_enable_variable_lifting, **kwargs)\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maggregation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m       \u001b[0maggregation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariableAggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m     return previous_getter(\n\u001b[0m\u001b[1;32m   1231\u001b[0m         \u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36mgetter\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcaptured_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcreator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   3974\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3975\u001b[0m       \u001b[0m_require_strategy_scope_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3976\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnext_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3978\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_creator_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36mgetter\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcaptured_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcreator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   3974\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3975\u001b[0m       \u001b[0m_require_strategy_scope_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3976\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnext_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3978\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_creator_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36mgetter\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcaptured_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mvariable_capturing_scope\u001b[0;34m(next_creator, **kwds)\u001b[0m\n\u001b[1;32m    682\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menable_variable_lifting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m       v = UnliftedInitializerVariable(\n\u001b[0m\u001b[1;32m    685\u001b[0m           \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_initializers_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvariable_call\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvariable_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, caching_device, name, dtype, constraint, add_initializers_to, synchronization, aggregation, shape, **unused_kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# of the handle which helps async execution performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         cond.cond(\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_is_initialized_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             not_assign_fn, assign_fn)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                 instructions)\n\u001b[0;32m--> 588\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/cond.py\u001b[0m in \u001b[0;36mcond\u001b[0;34m(pred, true_fn, false_fn, strict, name, fn1, fn2)\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;31m# Always enable control flow v2 if building a function, regardless of toggle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnableControlFlowV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcond_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cond\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36mcond_v2\u001b[0;34m(pred, true_fn, false_fn, name)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mverify_captures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_COND\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrue_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_graph\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     return _build_cond(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mtrue_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_build_cond\u001b[0;34m(pred, true_graph, false_graph, true_inputs, false_inputs, building_gradient, name)\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;31m# Add inputs to true_graph and false_graph to make them match. Note that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;31m# this modifies true_graph and false_graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m   cond_inputs = _make_inputs_match([true_graph, false_graph],\n\u001b[0m\u001b[1;32m    258\u001b[0m                                    [true_inputs, false_inputs])\n\u001b[1;32m    259\u001b[0m   \u001b[0;31m# We do not output intermediates of the gradient If op since this is just\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_make_inputs_match\u001b[0;34m(branch_graphs, branch_inputs)\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbranch_input_to_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_dummy_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m       \u001b[0minput_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/cond_v2.py\u001b[0m in \u001b[0;36m_create_dummy_input\u001b[0;34m(func_graph, template_tensor)\u001b[0m\n\u001b[1;32m    814\u001b[0m   \"\"\"\n\u001b[1;32m    815\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m     return array_ops.placeholder(\n\u001b[0m\u001b[1;32m    817\u001b[0m         template_tensor.dtype, shape=template_tensor.shape)\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   2992\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   2993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2994\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   7087\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7088\u001b[0m   \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7089\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m   7090\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n\u001b[1;32m   7091\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    774\u001b[0m   \u001b[0;31m# Requires that op_def has passed validation (using the C++\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m   \u001b[0;31m# ValidateOpDef() from ../framework/op_def_util.h).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mas_default\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtf_contextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0minner_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m       \u001b[0;34m\"\"\"Context manager for copying distribute.Strategy scope information.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_contextlib.py\u001b[0m in \u001b[0;36mcontextmanager\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \"\"\"\n\u001b[1;32m     38\u001b[0m   \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_contextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'contextmanager'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36mmake_decorator\u001b[0;34m(target, decorator_func, decorator_name, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdecorator_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mdecorator_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m   decorator = TFDecorator(decorator_name, target, decorator_doc,\n\u001b[0m\u001b[1;32m    137\u001b[0m                           decorator_argspec)\n\u001b[1;32m    138\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorator_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tf_decorator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, decorator_name, target, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    306\u001b[0m   \"\"\"\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   def __init__(self,\n\u001b[0m\u001b[1;32m    309\u001b[0m                \u001b[0mdecorator_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "K.clear_session()\n",
        "set_random_seed(1)\n",
        "inputlist = []\n",
        "denselist = []\n",
        "nrep = len(mydat_rep)\n",
        "for i in range(nrep):\n",
        "  inputlist.append(Input((30, ), name = 'input'+str(i)))\n",
        "  #for each policy in the representative portfolio, calculate distance vector of target policy to the rep policy\n",
        "  denselist.append(Dense(1, name = 'dense'+str(i))(inputlist[i]))\n",
        "  #map each distance vector via an affine function to a single neuron. affine functions are distinct\n",
        "weights = Concatenate()(denselist)\n",
        "weights = Softmax()(weights)\n",
        "#concatenate the n_rep neurons, and then apply softmax to obtain weights\n",
        "output = Dense(1, use_bias = False, name = 'weightedval', trainable=False)(weights)\n",
        "#by introducing a dense layer on these weights, these weights will each be multiplied by a number and summed\n",
        "#we force the dense layer to be nontrainable, and set the number which the weights will be multiplied by\n",
        "#to be the representative portfolio values, in order to achieve this effect\n",
        "m = KerasModel(inputs = inputlist, outputs = output)\n",
        "m.get_layer('weightedval').set_weights([np.reshape(np.array(delta_rep), (len(delta_rep), 1))])\n",
        "#this forces the weights to each multiply by the representative portfolio values\n",
        "m.compile(optimizer=Adam(), loss='mse')\n",
        "#m.summary()\n",
        "callback = EarlyStopping(monitor='val_loss', patience=50)\n",
        "trainr_dist = getdist(mydat_rep_list, mydat_trainr_list)\n",
        "val_dist = getdist(mydat_rep_list, mydat_val_list)\n",
        "m.fit(x=trainr_dist, y=delta_trainr, epochs = 20000, validation_data = (val_dist, delta_val), callbacks = [callback], verbose = 1, batch_size = 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBt7eIA5CUAx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d5a145e-1b3a-4a31-d404-913a8aaa6687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 43ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 61ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 55ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 47ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 87ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 51ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 46ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 41ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 54ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 59ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 52ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 48ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 47ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 38ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 42ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 49ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 124ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step\n",
            "307.82878129223883\n",
            "347.5195387944415\n",
            "347.5195387944415\n",
            "0.8005989118790109\n",
            "14924.665597931224\n",
            "312.06583284197694\n",
            "364.1904764495889\n",
            "364.1904764495889\n",
            "0.734944742161615\n",
            "14025.520254722458\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "neval = 2048\n",
        "nfull = len(mydat_y)\n",
        "niters = nfull//neval +1\n",
        "predslist = []\n",
        "for i in range(niters):\n",
        "  start = int(neval*i)\n",
        "  stop = int(np.min([neval*(i+1), nfull]))\n",
        "  currentlist = [mydat_cat[start:stop], mydat_num[start:stop]]\n",
        "  currentdist = getdist(mydat_rep_list, currentlist)\n",
        "  predslist.append(m.predict(x=currentdist))\n",
        "mypreds_full = np.concatenate(predslist)\n",
        "mypreds_val = m.predict(x=val_dist)\n",
        "mypreds_trainr = m.predict(x=trainr_dist)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "#plt.scatter(mydat_remainder_y, mypreds_remainder)\n",
        "plt.scatter(delta_full, mypreds_full)\n",
        "plt.scatter(delta_val, mypreds_val)\n",
        "plt.scatter(delta_trainr, mypreds_trainr)\n",
        "errs = mypreds_full.flatten() - delta_full\n",
        "errs_val = mypreds_val.flatten() - delta_val\n",
        "errs_trainr = mypreds_trainr.flatten() - delta_trainr\n",
        "mave_delta = np.mean(np.abs(delta_full))\n",
        "\n",
        "errs_zero = errs[delta_full != 0]\n",
        "errs_val_zero = errs_val[delta_val != 0]\n",
        "mave_delta_zero = np.mean(np.abs(delta_full[delta_full!=0]))\n",
        "\n",
        "mse_full = np.mean(np.square(errs))\n",
        "mae_full = np.mean(np.abs(errs))\n",
        "mse_val = np.mean(np.abs(errs_val))\n",
        "mse_trainr = np.mean(np.square(errs_trainr))\n",
        "\n",
        "mse_full_zero = np.mean(np.square(errs_zero))\n",
        "mae_full_zero = np.mean(np.abs(errs_zero))\n",
        "mse_val_zero = np.mean(np.square(errs_val_zero))\n",
        "\n",
        "print(mse_val/1e+6)\n",
        "print(mse_full/1e+6)\n",
        "print(mae_full/1e+6)\n",
        "print(np.sqrt(mse_full)/mave_delta)\n",
        "print(mae_full/mave_delta)\n",
        "\n",
        "print(mse_val_zero/1e+6)\n",
        "print(mse_full_zero/1e+6)\n",
        "print(mae_full_zero/1e+6)\n",
        "print(np.sqrt(mse_full_zero)/mave_delta_zero)\n",
        "print(mae_full_zero/mave_delta_zero)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "errs = mypreds_full.flatten() - delta_full\n",
        "errs_val = mypreds_val.flatten() - delta_val\n",
        "errs_trainr = mypreds_trainr.flatten() - delta_trainr\n",
        "mave_delta = np.mean(np.abs(delta_full))\n",
        "\n",
        "errs_zero = errs[delta_full != 0]\n",
        "errs_val_zero = errs_val[delta_val != 0]\n",
        "mave_delta_zero = np.mean(np.abs(delta_full[delta_full!=0]))\n",
        "\n",
        "mse_full = np.mean(np.square(errs))\n",
        "mae_full = np.mean(np.abs(errs))\n",
        "mse_val = np.mean(np.abs(errs_val))\n",
        "mse_trainr = np.mean(np.square(errs_trainr))\n",
        "\n",
        "mse_full_zero = np.mean(np.square(errs_zero))\n",
        "mae_full_zero = np.mean(np.abs(errs_zero))\n",
        "mse_val_zero = np.mean(np.square(errs_val_zero))\n",
        "\n",
        "\n",
        "print(mse_val/1e+6)\n",
        "print(mse_full/1e+6)\n",
        "print(mae_full/1e+6)\n",
        "print(np.sqrt(mse_full)/mave_delta)\n",
        "print(mae_full/mave_delta)\n",
        "\n",
        "print(mse_val_zero/1e+6)\n",
        "print(mse_full_zero/1e+6)\n",
        "print(mae_full_zero/1e+6)\n",
        "print(np.sqrt(mse_full_zero)/mave_delta_zero)\n",
        "print(mae_full_zero/mave_delta_zero)\n",
        "print(np.min(delta_rep))\n",
        "print(np.max(delta_rep))\n",
        "print(np.min(delta_full))\n",
        "print(np.max(delta_full))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KayNbPwjGD9m",
        "outputId": "13cf7bf8-b05a-42af-d25e-3fa6ca4c4fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.010973296849433486\n",
            "347.5195387944415\n",
            "0.010847399546469395\n",
            "0.8005989118790109\n",
            "0.46585527651142244\n",
            "312.06583284197694\n",
            "364.1904764495889\n",
            "0.010941282158558007\n",
            "0.734944742161615\n",
            "0.4213651494226546\n",
            "-190559.835\n",
            "57562.14925\n",
            "-435069.7555\n",
            "216124.5184\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}